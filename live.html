<!-- live.html (vollständig, mit Vorbereitungs-Phase + harter Sprachbindung + Parent-Export) -->
<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <title>Live Interview</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#0b0c10; --fg:#e5e7eb; --muted:#9ca3af; --ok:#10b981; --warn:#f59e0b; --err:#ef4444; --card:#111217; --line:#1f2330; }
    html,body{margin:0;padding:0;background:var(--bg);color:var(--fg);font:14px/1.45 system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif}
    .wrap{display:grid;grid-template-columns:1fr 320px;gap:16px;padding:14px;align-items:start}
    .card{background:var(--card);border:1px solid var(--line);border-radius:12px;padding:12px}
    .row{display:flex;gap:10px;align-items:center}
    .bar{height:6px;background:#0f172a;border-radius:999px;overflow:hidden}
    .bar > i{display:block;height:100%;width:0;background:#4f46e5;transition:width 50ms linear}
    .badge{display:inline-block;padding:2px 6px;border-radius:8px;background:#0f172a;border:1px solid #24314f;color:#cbd5e1;font-size:12px}
    .muted{color:var(--muted)}
    .btn{appearance:none;border:1px solid #2b3244;background:#141827;color:#f8fafc;border-radius:8px;padding:8px 10px;cursor:pointer}
    .btn:disabled{opacity:.5;cursor:not-allowed}
    video{width:100%;max-height:320px;background:#000;border-radius:8px}
    audio{width:100%}
    .grid2{display:grid;grid-template-columns:1fr 1fr;gap:10px}
    .log{font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px;color:#9ca3af;white-space:pre-wrap;max-height:220px;overflow:auto}
    .ghost{opacity:.65}
  </style>
</head>
<body>
  <div class="wrap">
    <section class="card">
      <div class="row" style="justify-content:space-between">
        <div>
          <div id="status" class="badge">Init…</div>
          <div id="audioBadge" class="badge" style="margin-left:6px">• (stumm?)</div>
        </div>
        <div class="row">
          <button id="btnStop" class="btn">Interview abbrechen</button>
        </div>
      </div>

      <div class="grid2" style="margin-top:12px">
        <div>
          <div class="muted" style="margin-bottom:6px">Kamera-Vorschau</div>
          <video id="localVideo" autoplay playsinline muted></video>
          <div class="muted" style="margin:8px 0 4px">Mic-Level</div>
          <div class="bar"><i id="micLevel"></i></div>
        </div>
        <div>
          <div class="muted" style="margin-bottom:6px">Assistent-Audio</div>
          <audio id="remoteAudio" controls></audio>
          <div class="muted" style="margin:8px 0 4px">System</div>
          <div class="bar"><i id="sysLevel"></i></div>
          <div class="muted" style="margin-top:10px">Hinweis: Bei blockiertem Autoplay ggf. in den Player klicken.</div>
        </div>
      </div>
    </section>

    <aside class="card">
      <div style="font-weight:600;margin-bottom:8px">Recorder / Upload</div>
      <div class="row">
        <div class="badge ghost" id="recState">idle</div>
        <div class="badge ghost" id="muxState" style="margin-left:6px">no upload</div>
      </div>
      <div class="muted" style="margin:8px 0 4px">Fortschritt</div>
      <div class="bar"><i id="recProgress"></i></div>
      <div class="muted" style="margin:10px 0 4px">Logs</div>
      <div id="log" class="log"></div>
    </aside>
  </div>

  <script>
  // -------------------- Query --------------------
  const Q = new URLSearchParams(location.search);
  const UID = (Q.get('uid')||'').trim();
  const COMPANY_ID = (Q.get('companyId')||'').trim();
  const LANG = (Q.get('lang')||'de').toLowerCase();
  const VOICE = (Q.get('voice')||'verse').toLowerCase();

  // -------------------- UI Els -------------------
  const $status = document.getElementById('status');
  const $badge  = document.getElementById('audioBadge');
  const $mic    = document.getElementById('micLevel');
  const $sys    = document.getElementById('sysLevel');
  const $log    = document.getElementById('log');
  const $localV = document.getElementById('localVideo');
  const $remoteA= document.getElementById('remoteAudio');
  const $btnStop= document.getElementById('btnStop');
  const $recState = document.getElementById('recState');
  const $muxState = document.getElementById('muxState');
  const $recProg  = document.getElementById('recProgress');

  function log(...a){
    console.log(...a);
    const s = a.map(x => typeof x==='object' ? JSON.stringify(x) : String(x)).join(' ');
    $log.textContent += s + "\n";
    $log.scrollTop = $log.scrollHeight;
  }
  function setStatus(kind, text){
    $status.textContent = text || kind || '';
    $status.style.background = kind==='err' ? '#3b0d0d' : (kind==='ok' ? '#0d3b2a' : (kind==='warn' ? '#3b2f0d' : '#0f172a'));
    $status.style.borderColor = kind==='err' ? '#7f1d1d' : (kind==='ok' ? '#14532d' : (kind==='warn' ? '#7f6a1d' : '#24314f'));
  }

  // -------------------- Endpunkt -----------------
  const TOKEN_URL = 'https://www.clarity-nvl.com/_functions/realtimeToken';

  // -------------------- Global State -------------
  let pc, DC;
  let localStream, mixedStream, mediaRecorder;
  let remoteStream;
  let audioCtx, remoteSource;
  let destroyed = false;

  // Parent-Handshake / Flags
  let PARENT_UPLOAD = false;   // ctx.uploadMode === 'parent'
  let RECORD_ENABLED = true;   // clarity.live.record.enabled
  let CTX = { uploadMode:'internal', reportLang:'de', roleProfileText:'', docsText:'' };
  let ENDED_SENT = false;

  // ===== DC-Queue + Wait =====
  let dcIsOpen = false;
  let dcQueue = [];
  function dcSend(obj){
    if (destroyed) return;
    try{
      if (dcIsOpen && DC && DC.readyState === 'open') {
        DC.send(JSON.stringify(obj));
      } else {
        dcQueue.push(obj);
      }
    }catch(e){ console.warn('[RTC] dcSend fail', e); }
  }
  function waitDCOpen(timeoutMs = 6000){
    if (dcIsOpen && DC?.readyState === 'open') return Promise.resolve();
    return new Promise(resolve => {
      const t0 = Date.now();
      const iv = setInterval(() => {
        if (dcIsOpen && DC?.readyState === 'open') {
          clearInterval(iv); resolve();
        } else if (Date.now() - t0 > timeoutMs) {
          clearInterval(iv); resolve();
        }
      }, 50);
    });
  }

  // -------------------- Token --------------------
  async function getRealtimeToken(payload){
    log('[Live] Fetching token…', TOKEN_URL, JSON.stringify(payload));
    const r = await fetch(TOKEN_URL,{ method:'POST', headers:{'Content-Type':'application/json'}, body:JSON.stringify(payload) });
    if (!r.ok){
      const t = await r.text().catch(()=>r.statusText);
      throw new Error('token_http_'+r.status+': '+t);
    }
    const data = await r.json().catch(e=>{ throw new Error('token_json_parse: '+e); });
    log('[Live] Token response:', r.status, data);
    if (!data?.ok) throw new Error('token_error: '+(data?.error||'unknown')+' '+(data?.message||''));    
    const sec = extractClientSecret(data);
    if (!sec) throw new Error('client_secret_missing');
    log('[Live] effective', { lang:data.reportLang||payload.lang, voice:data.voice, model:data.model });
    return { secret: sec, ctx: data };
  }
  function extractClientSecret(obj){
    if (!obj || typeof obj !== 'object') return null;
    if (typeof obj.client_secret === 'string') return obj.client_secret;
    if (obj.client_secret && typeof obj.client_secret.value === 'string') return obj.client_secret.value;
    if (obj.data && typeof obj.data.client_secret === 'string') return obj.data.client_secret;
    if (obj.token && typeof obj.token === 'string') return obj.token;
    if (obj.clientSecret && typeof obj.clientSecret === 'string') return obj.clientSecret;
    return null;
  }

  // -------------------- Realtime WebRTC ----------
  async function connectRealtime(clientSecret){
    pc = new RTCPeerConnection({ iceServers: [{ urls: ['stun:stun.l.google.com:19302'] }] });

    pc.onconnectionstatechange = () => {
      log('[RTC] connected');
      if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected'){
        setStatus('err','Verbindung unterbrochen');
      }
    };

    pc.ontrack = async (ev) => {
      log('[RTC] ontrack kind=', ev.track.kind, 'streams=', ev.streams.length);
      if (!remoteStream) remoteStream = new MediaStream();
      remoteStream.addTrack(ev.track);
      $remoteA.srcObject = remoteStream;

      // WebAudio Analyser (System-Level)
      try{
        audioCtx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();
        await audioCtx.resume().catch(()=>{});
        remoteSource = audioCtx.createMediaStreamSource(remoteStream);
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 512;
        const buf = new Uint8Array(analyser.frequencyBinCount);
        remoteSource.connect(analyser);
        (function tick(){
          if (destroyed) return;
          analyser.getByteTimeDomainData(buf);
          let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
          $sys.style.width = Math.min(100, dev*2) + '%';
          requestAnimationFrame(tick);
        })();
      }catch(_){}

      // Playback Safety
      try {
        $remoteA.muted = false; $remoteA.volume = 1.0;
        $remoteA.setAttribute('autoplay',''); $remoteA.setAttribute('playsinline','');
        await $remoteA.play();
        log('[RTC] remote audio playing.');
      } catch(e) {
        log('[RTC] play() blocked', e);
        showAudioGate();
      }

      // Badge „Audio in“
      try {
        const levelAnalyser = audioCtx.createAnalyser();
        levelAnalyser.fftSize = 1024;
        remoteSource.connect(levelAnalyser);
        const tbuf = new Uint8Array(levelAnalyser.frequencyBinCount);
        let lastNZ = 0;
        (function ping(){
          if (destroyed) return;
          levelAnalyser.getByteTimeDomainData(tbuf);
          let dev = 0;
          for (let i=0;i<tbuf.length;i++) dev = Math.max(dev, Math.abs(tbuf[i]-128));
          if (dev>2) lastNZ = performance.now();
          $badge.textContent = (performance.now()-lastNZ < 2000) ? '• Audio in' : '• (stumm?)';
          requestAnimationFrame(ping);
        })();
      }catch(_){}
    };

    // Local Tracks
    if (localStream){
      localStream.getTracks().forEach(t => pc.addTrack(t, localStream));
    }

    // DataChannel
    DC = pc.createDataChannel('oai-events');
    DC.onopen = () => {
      dcIsOpen = true;
      log('[RTC] DC open');
      try {
        const pending = dcQueue.splice(0);
        for (const msg of pending) DC.send(JSON.stringify(msg));
      } catch(e){ console.warn('[RTC] flush queue fail', e); }
    };
    DC.onmessage = onRealtimeEvent;

    const offer = await pc.createOffer({ offerToReceiveAudio:true, offerToReceiveVideo:false });
    await pc.setLocalDescription(offer);

    const r = await fetch('https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${clientSecret}`,
        'Content-Type': 'application/sdp'
      },
      body: offer.sdp
    });

    if (!r.ok){
      const t = await r.text().catch(()=>r.statusText);
      throw new Error('realtime_handshake_failed: ' + t);
    }

    const answer = { type: 'answer', sdp: await r.text() };
    await pc.setRemoteDescription(answer);
  }

  function onRealtimeEvent(ev){
    let data = ev && ev.data;
    try{ if (typeof data === 'string') data = JSON.parse(data); }catch(_){}
    if (!data) return;

    if (data.type === 'response.state' && data.state){
      parent?.postMessage?.({ type:'clarity.avatar', data:{ state:data.state } }, '*');
    }
  }

  // -------------- Vorbereitungs-Phase / Priming --
  function buildSystemInstructions(ctx){
    const isDE = (ctx.reportLang === 'de');
    const langLine = isDE
      ? 'Sprich ausschließlich DEUTSCH. Wechsle zu keinem Zeitpunkt die Sprache.'
      : 'Speak strictly in ENGLISH. Do not switch languages at any time.';
    const structure = isDE
      ? 'Führe ein strukturiertes Job-Interview: Stelle genau EINE Frage zur Zeit, max. 1 Nachfrage, bleibe strikt beim Jobprofil.'
      : 'Conduct a structured job interview: ask exactly ONE question at a time, max 1 follow-up, stay strictly on the job profile.';
    const scope = isDE
      ? 'Antworte nur zu Rolle/Anzeige/Position. Für andere Fragen verweise freundlich auf direkten Kontakt.'
      : 'Answer only about role/ad/job. For other questions, kindly refer to direct contact.';
    return [langLine, structure, scope].join(' ');
  }

  function primeStrictLanguage(reportLang){
    const spoken = (String(reportLang||'de').toLowerCase().startsWith('de')) ? 'de' : 'en';
    const sys = buildSystemInstructions({ reportLang: spoken==='de' ? 'de' : 'en' });

    // Aufnahme temporär AUS, wir sprechen zuerst
    dcSend({ type:'session.update', session:{
      voice: VOICE,
      instructions: sys,
      spoken_language: spoken,
      input_audio_format:'pcm16',
      output_audio_format:'pcm16',
      input_audio_transcription:{ model:'whisper-1', language: spoken },
      turn_detection:{ type:'none' },
      modalities:['audio']
    }});

    // doppelt sichern (langsamer Netzpfad etc.)
    setTimeout(()=> dcSend({ type:'session.update', session:{
      spoken_language: spoken,
      input_audio_transcription:{ model:'whisper-1', language: spoken }
    }}), 700);

    setTimeout(()=> dcSend({ type:'session.update', session:{ spoken_language: spoken }}), 2200);
  }

  function speakPrepThenWelcome(){
    const comp = (String(CTX.reportLang||'de').toLowerCase().startsWith('de')) ? 'de' : 'en';
    const prep = (comp==='de')
      ? 'Einen Moment bitte, ich bereite das Interview vor …'
      : 'One moment please, I am preparing the interview …';
    const welcome = (comp==='de')
      ? `Willkommen! Dieses Interview wird im Auftrag von ${CTX.companyName||'Clarity'} geführt. Sind Sie bereit zu starten?`
      : `Welcome! This interview is conducted on behalf of ${CTX.companyName||'Clarity'}. Are you ready to begin?`;

    // kurze Vorbereitungsansage
    dcSend({ type:'response.create', response:{ instructions: prep, modalities:['audio'] }});

    // danach Welcome + VAD wieder AN
    setTimeout(()=>{
      dcSend({ type:'response.create', response:{ instructions: welcome, modalities:['audio'] }});
      dcSend({ type:'session.update', session:{
        turn_detection:{ type:'server_vad', threshold:0.9, prefix_padding_ms:200, silence_duration_ms:700 }
      }});
    }, 1500);
  }

  // -------------------- Media / Recording --------
  async function setupLocalMedia(){
    try{
      localStream = await navigator.mediaDevices.getUserMedia({
        audio:{ echoCancellation:true, noiseSuppression:true, autoGainControl:true },
        video:true
      });
    } catch(e) {
      setStatus('err','Mic/Kamera blockiert. In Browser Freigabe erteilen.');
      log('[Media] getUserMedia error', e);
      throw e;
    }
    $localV.srcObject = localStream;

    // Mic-Level
    try{
      const ctx = new (window.AudioContext || window.webkitAudioContext)();
      const src = ctx.createMediaStreamSource(localStream);
      const analyser = ctx.createAnalyser();
      analyser.fftSize = 512;
      const buf = new Uint8Array(analyser.frequencyBinCount);
      src.connect(analyser);
      (function tick(){
        if (destroyed) return;
        analyser.getByteTimeDomainData(buf);
        let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
        $mic.style.width = Math.min(100, dev*2) + '%';
        requestAnimationFrame(tick);
      })();
    }catch(_){}
  }

  function ensureAudioPlayback(){ return $remoteA.play().then(()=>true).catch(()=>false); }
  function showAudioGate(){ setStatus('warn','Autoplay blockiert – bitte kurz in den Player klicken.'); }

  async function blobToDataURL(blob){
    return new Promise((resolve,reject)=>{
      const reader = new FileReader();
      reader.onloadend = ()=> resolve(reader.result);
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  function startRecording(){
    if (!localStream){ log('[REC] cannot start – no localStream'); return; }
    const ctx = new (window.AudioContext || window.webkitAudioContext)();
    const dest = ctx.createMediaStreamDestination();

    const micSrc = ctx.createMediaStreamSource(localStream);
    micSrc.connect(dest);

    if (remoteStream){
      try{
        const remSrc = ctx.createMediaStreamSource(remoteStream);
        remSrc.connect(dest);
      }catch(e){ log('[REC] remote mix fail', e); }
    }

    mixedStream = new MediaStream();
    dest.stream.getAudioTracks().forEach(t => mixedStream.addTrack(t));
    const cam = localStream.getVideoTracks()[0];
    if (cam) mixedStream.addTrack(cam);

    const hasVideo = !!mixedStream.getVideoTracks().length;
    mediaRecorder = new MediaRecorder(mixedStream, {
      mimeType: hasVideo ? 'video/webm;codecs=vp8,opus' : 'audio/webm;codecs=opus',
      videoBitsPerSecond: hasVideo ? 1_200_000 : undefined,
      audioBitsPerSecond: 128_000
    });
    const chunks = [];
    mediaRecorder.onstart = ()=>{ $recState.textContent='recording'; log('[REC] started'); };
    mediaRecorder.ondataavailable = (e)=>{ if (e.data && e.data.size>0) chunks.push(e.data); };
    mediaRecorder.onstop = async ()=>{
      $recState.textContent='stopped';
      const hasVid = !!mixedStream.getVideoTracks().length;
      const blob = new Blob(chunks, { type: hasVid ? 'video/webm' : 'audio/webm' });
      log('[REC] stop, size=', blob.size);

      try{
        $muxState.textContent='exporting';
        $recProg.style.width = '60%';

        if (PARENT_UPLOAD) {
          const dataUrl = await blobToDataURL(blob);
          parent?.postMessage?.({ type:'recorder:export', data: { dataUrl, kind: (hasVid?'video':'audio'), bytes: blob.size } }, '*');
          $recProg.style.width = '100%';
          $muxState.textContent='export sent';
        } else {
          // (Optional: interner Upload – aktuell nicht verwendet)
          $recProg.style.width = '100%';
          $muxState.textContent='no parent upload';
        }

        sendEndedOnce();
      } catch(e){
        $muxState.textContent='export failed';
        log('[REC] export failed', e);
        parent?.postMessage?.({ type:'clarity.live.error', data:{ message: String(e?.message||e) } }, '*');
      } finally {
        chunks.length = 0;
      }
    };

    if (RECORD_ENABLED !== false) mediaRecorder.start();
  }

  function stopRecording(){
    try{ if (mediaRecorder && mediaRecorder.state!=='inactive') mediaRecorder.stop(); }catch(_){}
    try{ mixedStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
  }

  function sendEndedOnce(){
    if (ENDED_SENT) return;
    ENDED_SENT = true;
    parent?.postMessage?.({ type:'clarity.live.ended', data:{} }, '*');
  }

  // -------------------- Start / Stop Flow --------
  async function startInterview(){
    destroyed = false;
    ENDED_SENT = false;
    setStatus('info','Initialisiere…');

    // Mic/Camera
    await setupLocalMedia();

    // Token
    const { secret, ctx } = await getRealtimeToken({ uid:UID, companyId:COMPANY_ID, lang:LANG, voice:VOICE, debug:true, allowNoInvite:true });

    // Realtime verbinden
    await connectRealtime(secret);
    const unlocked = await ensureAudioPlayback();
    if (!unlocked) showAudioGate();

    // Recorder
    startRecording();

    // HARTES Sprach-Priming + Vorbereitungs-Ansage → Welcome → VAD an
    await waitDCOpen();
    const usedLang = (CTX.reportLang || ctx.reportLang || LANG);
    CTX = { ...CTX, companyName: ctx.companyName || CTX.companyName, reportLang: usedLang };
    primeStrictLanguage(usedLang);
    speakPrepThenWelcome();

    setStatus('ok','Verbunden • Interview läuft');
  }

  async function hangup(){
    destroyed = true;
    try{ DC?.close(); }catch(_){}
    try{ pc?.getSenders()?.forEach(s=>s.track && s.track.stop()); }catch(_){}
    try{ pc?.getReceivers()?.forEach(r=>r.track && r.track.stop()); }catch(_){}
    try{ pc?.close(); }catch(_){}
    try{ localStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
    try{ remoteStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
    stopRecording();
    setStatus('warn','Beendet');
    log('[Live] hangup done');
    sendEndedOnce();
  }

  // -------------------- Parent-Messaging ---------
  window.addEventListener('message', (ev)=>{
    const msg = ev.data || {};
    const type = msg.type;

    if (type === 'clarity.live.context') {
      CTX = { ...CTX, ...(msg.data || {}) };
      PARENT_UPLOAD  = (CTX.uploadMode === 'parent');
      log('[CTX]', CTX);
      return;
    }
    if (type === 'clarity.live.record') {
      RECORD_ENABLED = !!(msg.data && msg.data.enabled);
      log('[REC] record flag from parent:', RECORD_ENABLED);
      return;
    }
    if (type === 'clarity.live.start') {
      startInterview().catch(e => {
        setStatus('err', 'Startfehler: ' + (e?.message||e));
        log(e);
        parent?.postMessage?.({ type:'clarity.live.error', data:{ message:String(e?.message||e) } }, '*');
      });
      return;
    }
    if (type === 'clarity.live.stop' || type === 'clarity.live.hangup') {
      hangup(); return;
    }
    if (type === 'recorder:export') {
      // Parent fordert Export an → Recording sauber stoppen (onstop → export)
      stopRecording(); return;
    }
  });

  document.getElementById('btnStop').addEventListener('click', hangup);

  // Hinweis: im Parent-<iframe> muss stehen: allow="microphone; camera; autoplay"
  // Optional: beim Schließen beenden (Token sparen)
  window.addEventListener('beforeunload', hangup);
  </script>
</body>
</html>
