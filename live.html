<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <title>Interview Live</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body{font-family:system-ui,Segoe UI,Roboto,Arial,sans-serif;margin:0;padding:16px;background:#0b1020;color:#e9ecf1}
    h1{font-size:18px;margin:0 0 12px}
    .row{display:flex;gap:12px;flex-wrap:wrap}
    .card{background:#121a38;border:1px solid #1c2750;border-radius:10px;padding:12px}
    .w-100{width:100%}.w-50{width:calc(50% - 6px)}
    textarea, input, select, button{background:#0e1530;color:#e9ecf1;border:1px solid #273268;border-radius:8px;padding:8px}
    textarea{width:100%;min-height:140px;resize:vertical;font-family:ui-monospace,Consolas,monospace;font-size:12px;line-height:1.4}
    button{cursor:pointer}
    .pill{padding:4px 8px;border-radius:999px;background:#0e1530;border:1px solid #273268}
    .log{font-family:ui-monospace,Consolas,monospace;font-size:12px;white-space:pre-wrap}
    .muted{opacity:.7}
    label{font-size:12px;color:#b8c3ff}
    .mini{font-size:12px}
  </style>
</head>
<body>
  <h1>Interview – Live</h1>

  <div class="row">
    <div class="card w-50">
      <div class="row" style="align-items:center;gap:8px">
        <button id="btnStart">Start Live</button>
        <button id="btnStop" class="muted">Stop</button>
        <span class="pill mini" id="pillState">idle</span>
        <label>Voice</label>
        <select id="selVoice">
          <option value="alloy">alloy</option>
          <option value="verse" selected>verse</option>
          <option value="ash">ash</option>
          <option value="ballad">ballad</option>
          <option value="coral">coral</option>
          <option value="echo">echo</option>
          <option value="sage">sage</option>
          <option value="shimmer">shimmer</option>
          <option value="marin">marin</option>
          <option value="cedar">cedar</option>
        </select>
      </div>

      <div style="margin-top:10px">
        <label for="txtAgent">Agent-Textausgabe (Streaming)</label>
        <textarea id="txtAgent" placeholder="Agent streamt hier Text..."></textarea>
      </div>

      <div style="margin-top:10px">
        <label for="txtSend">Manual send (debug)</label>
        <textarea id="txtSend" placeholder='JSON, z.B. {"type":"response.create","response":{"modalities":["text"],"instructions":"Sag Hallo als Text."}}'></textarea>
        <div class="row" style="margin-top:8px;gap:8px">
          <button id="btnSend">an DC senden</button>
          <button id="btnClearTxt">Ausgaben leeren</button>
        </div>
      </div>
    </div>

    <div class="card w-50">
      <div>
        <label>Remote-Audio</label>
        <audio id="remoteAudio" autoplay playsinline></audio>
      </div>
      <div style="margin-top:8px">
        <button id="btnBeep">Test-Ton abspielen</button>
      </div>
      <div style="margin-top:10px">
        <label>Logs</label>
        <textarea id="txtLog" class="log" placeholder="Logs…"></textarea>
      </div>
    </div>
  </div>

  <script>
    // ------- kleine Helfer -------
    const els = {
      btnStart: document.getElementById('btnStart'),
      btnStop: document.getElementById('btnStop'),
      selVoice: document.getElementById('selVoice'),
      txtAgent: document.getElementById('txtAgent'),
      txtLog: document.getElementById('txtLog'),
      txtSend: document.getElementById('txtSend'),
      btnSend: document.getElementById('btnSend'),
      btnClearTxt: document.getElementById('btnClearTxt'),
      remoteAudio: document.getElementById('remoteAudio'),
      pillState: document.getElementById('pillState'),
      beep: null,
    };
    function log(...a){
      const line = a.map(v=> typeof v==='string' ? v : JSON.stringify(v)).join(' ');
      const ts = new Date().toTimeString().slice(0,5);
      els.txtLog.value += `[${ts}] ${line}\n`;
      els.txtLog.scrollTop = els.txtLog.scrollHeight;
      console.log(...a);
    }
    function setState(s){
      els.pillState.textContent = s;
    }

    // ------- Realtime globals -------
    let pc = null;
    let dc = null;
    let ws = null; // Realtime nutzt DataChannel; belassen für evtl. Erweiterungen
    let remoteStream = null;

    // ------- Start/Stop -------
    els.btnStart.onclick = startLive;
    els.btnStop.onclick = stopLive;
    els.btnSend.onclick = () => {
      try{
        if(!dc) return;
        const msg = JSON.parse(els.txtSend.value);
        dc.send(JSON.stringify(msg));
        log('[dc] sent', msg.type || Object.keys(msg));
      }catch(e){ log('send parse error', e.message); }
    };
    els.btnClearTxt.onclick = () => { els.txtAgent.value = ''; els.txtLog.value=''; };

    // Test-Ton
    els.btnBeep.onclick = async () => {
      if(!els.beep){
        const ctx = new (window.AudioContext||window.webkitAudioContext)();
        const o = ctx.createOscillator(); const g = ctx.createGain();
        o.connect(g).connect(ctx.destination); o.type='sine'; o.frequency.value=880;
        els.beep = {ctx,o,g};
      }
      const {ctx,o,g} = els.beep;
      if(ctx.state==='suspended') await ctx.resume();
      g.gain.setValueAtTime(0.0001, ctx.currentTime);
      o.start();
      g.gain.exponentialRampToValueAtTime(0.2, ctx.currentTime+0.02);
      setTimeout(()=>{ g.gain.exponentialRampToValueAtTime(0.0001, ctx.currentTime+0.02); }, 300);
      setTimeout(()=>{ try{o.stop();}catch{} }, 380);
    };

    async function startLive(){
      setState('starting');
      log('Start Live');
      // Token vom Proxy holen (Wix-HTTP-Fn spiegelt zu OpenAI)
      const voice = els.selVoice.value || 'alloy';
      const req = { uid: 'IV-2PTWPRKM-WX2TFPTE7N-NVL02C', companyId:'NVL-02', lang:'de', voice };
      const tokenRes = await fetch('/api/wix-token-proxy', {
        method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify(req)
      }).catch(()=>null);

      if(!tokenRes){ log('token error: fetch failed'); setState('error'); return; }

      let tokenJson = null;
      try{ tokenJson = await tokenRes.json(); }catch{}
      log('Token response (proxy) status:', tokenRes.status, 'body:', tokenJson||{});
      if(!tokenRes.ok || !tokenJson?.ok){ setState('error'); return; }

      const { token, model } = tokenJson;

      // PeerConnection
      pc = new RTCPeerConnection();
      remoteStream = new MediaStream();
      els.remoteAudio.srcObject = remoteStream;

      pc.onicecandidate = (e) => {
        if(e.candidate && dc && dc.readyState==='open'){
          dc.send(JSON.stringify({type:'ice.candidate', candidate:e.candidate}));
        }
      };

      // Nur Empfang einer Audiospur (recvonly)
      pc.addTransceiver('audio', { direction:'recvonly' });

      pc.ontrack = (ev) => {
        // Remote-Audio Track
        const [track] = ev.streams[0].getAudioTracks();
        log('[live] remote track flags: muted=', track.muted, 'enabled=', track.enabled, 'state=', track.readyState);
        remoteStream.addTrack(track);
        try{
          els.remoteAudio.muted = false;
          const playPromise = els.remoteAudio.play();
          if(playPromise?.catch) playPromise.catch(()=>{});
        }catch{}
      };

      // DataChannel für Realtime-Events
      dc = pc.createDataChannel('oai-events');
      dc.onopen = onDataChannelOpen;
      dc.onmessage = onAgentEvent;
      dc.onclose = ()=> log('DataChannel closed');
      dc.onerror = (e)=> log('DataChannel error', e?.message||e);

      // Offer bauen
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      // An OpenAI Realtime (WebRTC) senden
      const sdpResp = await fetch(`https://api.openai.com/v1/realtime?model=${encodeURIComponent(model||'gpt-4o-realtime-preview-2025-09-12')}`, {
        method:'POST',
        headers:{
          'Authorization': `Bearer ${token}`,
          'Content-Type':'application/sdp',
          'OpenAI-Beta':'realtime=v1'
        },
        body: offer.sdp
      });

      if(!sdpResp.ok){
        const t = await sdpResp.text().catch(()=>null);
        log('SDP answer error', sdpResp.status, t);
        setState('error'); return;
      }

      const answerSdp = await sdpResp.text();
      await pc.setRemoteDescription({ type:'answer', sdp: answerSdp });
      log('pc: connected (SDP answer set)');
      setState('connected');
    }

    async function stopLive(){
      setState('stopping');
      try{ dc && dc.close(); }catch{}
      try{ pc && pc.close(); }catch{}
      dc=null; pc=null; remoteStream=null;
      setState('stopped');
      log('stopped');
    }

    /* ---------- DataChannel events ---------- */
    function onDataChannelOpen(){
      log('DataChannel open');

      // PATCH 2: Session minimal halten (nur Voice + Modalitäten)
      const sess = {
        voice: els.selVoice.value || 'alloy',
        modalities: ['audio','text']
        // KEIN turn_detection, KEIN output_audio usw.
      };
      dc.send(JSON.stringify({ type: 'session.update', session: sess }));
      log('[rt] session.update sent', JSON.stringify(sess));
      // response.create NICHT hier senden — erst wenn die Session bestätigt ist (siehe onAgentEvent → session.created)
    }

    // PATCH 1 + 3: komplettes Logging + Text-Only-Probe nach session.created
    function onAgentEvent(ev){
      try{
        const msg = JSON.parse(ev.data);

        // Rohes Event immer in Console und Log-Box (ohne Kürzung)
        console.log('[rt raw]', msg);
        log('[rt] event', JSON.stringify(msg));

        // Wenn eine Response fehlschlägt, Fehlergrund groß ausgeben
        if (msg?.type === 'response.done') {
          const r = msg.response;
          if (r?.status === 'failed') {
            log('[rt] RESPONSE FAILED', JSON.stringify(r.status_details, null, 2));
          }
        }

        // Nach Session-Start die ersten Requests schicken
        if (msg?.type === 'session.created') {
          // (a) Audio + Text – sehr simple Anweisung, damit Audio überhaupt erzeugt wird
          dc.send(JSON.stringify({
            type:'response.create',
            response:{
              modalities:['audio','text'],
              instructions:'Sag bitte genau: TESTTEST. Danach zähle 1 2 3.',
              voice: els.selVoice.value || 'alloy'
            }
          }));
          log('[rt] response.create (nach session.created) sent');

          // (b) Direkt im Anschluss eine Text-Only-Probe (um zu prüfen, ob nur Audio blockiert ist)
          setTimeout(()=>{
            dc.send(JSON.stringify({
              type:'response.create',
              response:{ modalities:['text'], instructions:'TEXT-ONLY-PROBE: Schreibe genau "TEXT-OK".' }
            }));
            log('[rt] response.create (TEXT-ONLY probe) sent');
          }, 300);
        }

        // Textstream in die Ausgabe schieben
        if(msg?.type==='response.output_text.delta' && msg?.delta){
          els.txtAgent.value += msg.delta;
          els.txtAgent.scrollTop = els.txtAgent.scrollHeight;
        }
        if(msg?.type==='response.delta' && msg?.delta){
          els.txtAgent.value += msg.delta;
          els.txtAgent.scrollTop = els.txtAgent.scrollHeight;
        }
        if(msg?.type==='response.completed'){
          els.txtAgent.value += '\n';
        }

        // Optionale Muted-Diagnose, wenn Track vorhanden
        if (msg?.type === 'response.output_audio.delta'){
          // Wenn Audio ankommt, sollte der Track wenige Sekunden später unmuted sein.
          // (Die eigentliche Track-Mute/State-Ausgabe passiert bei ontrack.)
        }
      }catch{
        // Nicht-JSON ignorieren
      }
    }
  </script>
</body>
</html>
