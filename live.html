<!-- public/live.html -->
<!-- Realtime-Interview Client: WebRTC + Turn-Taking (Server-VAD) + Willkommensflow + Avatar-Status Events -->

<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Clarity Live</title>
  <style>
    :root { --muted:#e5e7eb; --fg:#111827; --ok:#16a34a; --warn:#d97706; --err:#dc2626; }
    body{font-family:system-ui,Segoe UI,Roboto,Arial,sans-serif;margin:0;background:#fff;color:var(--fg)}
    .wrap{padding:12px}
    .row{display:grid;grid-template-columns:1fr 1fr;gap:12px;align-items:start}
    .card{border:1px solid var(--muted); border-radius:12px; padding:12px}
    textarea{width:100%;height:180px;border:1px solid var(--muted);border-radius:10px;padding:8px;font-family:ui-monospace,Consolas,monaco,monospace}
    select,button,input{padding:8px 10px;border-radius:10px;border:1px solid var(--muted)}
    .state{font-size:12px;opacity:.8}
    audio{width:100%;margin-top:8px}
    .hidden{display:none}
  </style>
</head>
<body>
  <div class="wrap">
    <div class="row">
      <div class="card">
        <div style="display:flex;gap:8px;align-items:center;flex-wrap:wrap">
          <label>Sprache:
            <select id="lang">
              <option value="de">Deutsch</option>
              <option value="en">English</option>
            </select>
          </label>
          <label>Stimme:
            <select id="voice">
              <option value="alloy">alloy</option>
              <option value="verse">verse</option>
              <option value="aria">aria</option>
            </select>
          </label>
          <button id="btnStart">Start</button>
          <span id="pillState" class="state">idle</span>
        </div>
        <audio id="audio" autoplay></audio>
      </div>
      <div class="card">
        <div style="display:flex;gap:8px;align-items:center;flex-wrap:wrap">
          <button id="btnMute">Mute</button>
          <button id="btnUnmute">Unmute</button>
          <button id="btnHangup">Hang up</button>
        </div>
        <textarea id="txt" placeholder="Live transcript / agent output …" spellcheck="false"></textarea>
      </div>
    </div>
  </div>

  <script>
  // --- Elements ---
  const els = {
    lang: document.getElementById('lang'),
    voice: document.getElementById('voice'),
    audio: document.getElementById('audio'),
    btnStart: document.getElementById('btnStart'),
    btnHangup: document.getElementById('btnHangup'),
    btnMute: document.getElementById('btnMute'),
    btnUnmute: document.getElementById('btnUnmute'),
    text: document.getElementById('txt'),
    pill: document.getElementById('pillState'),
  };

  // --- State ---
  let pc, dc, micStream, awaitingSessionCreated = false, activeResponseId = null;
  let queuedResponsePayload = null;

  function log(...a){ try{ console.log('[live]', ...a); }catch{} }
  function parentPost(msg){ try{ window.parent && window.parent.postMessage(msg, '*'); }catch{} }
  function setState(s){ els.pill.textContent = s; }
  function avatarState(state){
    setState(state);
    parentPost({ type:'clarity.avatar', data:{ state } });
  }

  // --- Build system instructions (Company/Role/Language from parent context) ---
  function buildSystemInstructions(ctx){
    const companyName = (ctx && ctx.companyName) || 'Ihr Unternehmen';
    const position    = (ctx && ctx.position) || 'die ausgeschriebene Position';
    const lang        = ((ctx && ctx.lang) || els.lang.value || 'de').toLowerCase();

    const base = `
You are "Clarity Interview Agent".
Context: Company "${companyName}", Role "${position}".
Goal: Run a structured voice interview in ${lang.toUpperCase()} with strict turn taking.
Rules:
- Start with a short warm welcome (1 sentence). Then ask: "Sind Sie bereit?" (localized).
- Wait and listen (server VAD). When candidate confirms, proceed.
- Ask concise, job-relevant questions ONE at a time. No multi-part prompts.
- Keep answers brief. Always return to listening after each question.
- After ~5 questions, say you’ll hand over to the company and politely close.
- If asked to repeat/clarify, briefly rephrase and ask again.
- Voice only; produce spoken output every turn. No meta talk.

Language: ${lang}.
`.trim();

    return base;
  }

  // --- Messaging to Realtime over DataChannel JSON ---
  function rtSend(obj){
    try{ dc && dc.readyState === 'open' && dc.send(JSON.stringify(obj)); }catch(e){ log('rtSend error', e); }
  }

  function createResponseSafe(extra){
    const payload = Object.assign({
      type: 'response.create',
      response: {
        modalities: ['audio','text'],
        instructions: '',
        conversation: 'default',
        voice: els.voice.value || 'alloy'
      }
    }, { response: extra || {} });

    try {
      rtSend(payload);
    } catch {
      queuedResponsePayload = payload;
    }
  }

  // --- WebRTC setup to OpenAI Realtime ---
  async function start() {
    if (pc) return;
    els.text.value = '';
    setState('starting');

    // Token holen
    const tokenRes = await fetch('/_functions/realtimeToken', {
      method: 'POST',
      headers: { 'Content-Type':'application/json' },
      body: JSON.stringify({ model: 'gpt-4o-realtime-preview-2024-12-17', voice: els.voice.value || 'alloy' })
    });
    if (!tokenRes.ok) {
      setState('error');
      return alert('Konnte Realtime-Token nicht erzeugen.');
    }
    const { client_secret } = await tokenRes.json();

    // PeerConnection
    pc = new RTCPeerConnection();
    pc.onconnectionstatechange = () => log('pc state:', pc.connectionState);
    pc.oniceconnectionstatechange = () => log('ice state:', pc.iceConnectionState);

    // Agent Audio abspielen
    const audioEl = els.audio;
    const remoteStream = new MediaStream();
    audioEl.srcObject = remoteStream;
    pc.ontrack = (e) => {
      remoteStream.addTrack(e.track);
    };

    // Mikrofon holen
    micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    for (const track of micStream.getTracks()) pc.addTrack(track, micStream);

    // DataChannel
    dc = pc.createDataChannel('oai-events');
    dc.onopen = () => {
      log('dc open');
      // Basis-Instruktionen setzen
      rtSend({ type:'session.update', session:{ instructions: buildSystemInstructions(window.__clarityCtx||{}) } });
      awaitingSessionCreated = true;
    };
    dc.onmessage = onAgentEvent;
    dc.onclose = () => log('dc closed');

    // Offer erzeugen
    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);

    // An OpenAI senden (mit Client-Secret als Bearer)
    const sdpRes = await fetch(`https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${client_secret}`,
        'Content-Type': 'application/sdp'
      },
      body: offer.sdp
    });

    const answer = { type: 'answer', sdp: await sdpRes.text() };
    await pc.setRemoteDescription(answer);
    setState('connected');
    avatarState('listening');
  }

  function hangup() {
    try { dc && dc.close(); } catch {}
    try { pc && pc.close(); } catch {}
    try { micStream && micStream.getTracks().forEach(t => t.stop()); } catch {}
    pc = null; dc = null; micStream = null;
    setState('idle');
    avatarState('idle');
  }

  // --- Events vom Agenten auswerten ---
  function onAgentEvent(ev){
    try{
      const msg = JSON.parse(ev.data);

      // Session
      if (msg?.type === 'session.created') {
        log('session.created');
        if (awaitingSessionCreated) {
          awaitingSessionCreated = false;

          // System-Instruktionen (redundant) setzen
          rtSend({ type:'session.update', session:{ instructions: buildSystemInstructions(window.__clarityCtx||{}) } });

          // Willkommensnachricht + "Sind Sie bereit?"
          avatarState('speaking');
          createResponseSafe({
            modalities: ['audio','text'],
            voice: els.voice.value,
            instructions:
`Sage in ${(window.__clarityCtx?.lang||els.lang.value||'de').toUpperCase()} eine kurze Willkommensnachricht im Namen des Unternehmens.
Dann stelle genau eine Ja/Nein-Frage: "Sind Sie bereit, das Interview zu beginnen?". Sprich freundlich und knapp.`
          });

          queuedResponsePayload = null;
        }
        return;
      }

      // Agent-Text
      if (msg?.type === 'response.output_text.delta' && msg?.delta) {
        els.text.value += msg.delta; els.text.scrollTop = els.text.scrollHeight;
      }
      if (msg?.type === 'response.delta' && msg?.delta) {
        els.text.value += msg.delta; els.text.scrollTop = els.text.scrollHeight;
      }
      if (msg?.type === 'response.completed') {
        els.text.value += '\n';
      }

      // Lifecycle
      if (msg?.type === 'response.created') {
        activeResponseId = msg?.response?.id || true;
        avatarState('speaking');
      }
      if (msg?.type === 'output_audio_buffer.started') {
        avatarState('speaking');
      }
      if (msg?.type === 'output_audio_buffer.stopped') {
        avatarState('listening');
      }
      if (msg?.type === 'response.done') {
        activeResponseId = null;
        avatarState('listening');
      }
    }catch{
      // Fallback: Plain text
      els.text.value += (ev.data||'');
    }
  }

  // --- Parent-Kommunikation (Wix-Seite steuert Kontext) ---
  window.addEventListener('message', (e) => {
    const { data } = e || {};
    const type = data && data.type;
    const payload = data && data.data;

    if (type === 'clarity.live.start') {
      start();
    }
    if (type === 'clarity.live.hangup') {
      hangup();
    }
    if (type === 'clarity.live.mute') {
      try { micStream && micStream.getAudioTracks().forEach(t => t.enabled = false); }catch{}
    }
    if (type === 'clarity.live.unmute') {
      try { micStream && micStream.getAudioTracks().forEach(t => t.enabled = true); }catch{}
    }
    if (type === 'clarity.live.context') {
      // Kontext übernehmen und Instruktionen live aktualisieren
      window.__clarityCtx = Object.assign({}, window.__clarityCtx||{}, payload||{});
      const sys = buildSystemInstructions(window.__clarityCtx);
      rtSend({ type:'session.update', session:{ instructions: sys }});
    }
  });

  // --- UI Buttons (für lokale Tests; im Wix-Flow nutzt du Parent-Buttons) ---
  els.btnStart.addEventListener('click', start);
  els.btnHangup.addEventListener('click', hangup);
  els.btnMute.addEventListener('click', () => window.parent && window.parent.postMessage({ type:'clarity.live.mute' }, '*'));
  els.btnUnmute.addEventListener('click', () => window.parent && window.parent.postMessage({ type:'clarity.live.unmute' }, '*'));
  </script>
</body>
</html>
