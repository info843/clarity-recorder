<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <title>Live Interview</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#0b0c10; --fg:#e5e7eb; --muted:#9ca3af; --ok:#10b981; --warn:#f59e0b; --err:#ef4444; --card:#111217; --line:#1f2330; }
    html,body{margin:0;padding:0;background:var(--bg);color:var(--fg);font:14px/1.45 system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif}
    .wrap{display:grid;grid-template-columns:1fr 320px;gap:16px;padding:14px;align-items:start}
    .card{background:var(--card);border:1px solid var(--line);border-radius:12px;padding:12px}
    .row{display:flex;gap:10px;align-items:center}
    .bar{height:6px;background:#0f172a;border-radius:999px;overflow:hidden}
    .bar > i{display:block;height:100%;width:0;background:#4f46e5;transition:width 50ms linear}
    .badge{display:inline-block;padding:2px 6px;border-radius:8px;background:#0f172a;border:1px solid #24314f;color:#cbd5e1;font-size:12px}
    .muted{color:var(--muted)}
    .btn{appearance:none;border:1px solid #2b3244;background:#141827;color:#f8fafc;border-radius:8px;padding:8px 10px;cursor:pointer}
    .btn:disabled{opacity:.5;cursor:not-allowed}
    video{width:100%;max-height:320px;background:#000;border-radius:8px}
    audio{width:100%}
    .grid2{display:grid;grid-template-columns:1fr 1fr;gap:10px}
    .log{font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px;color:#9ca3af;white-space:pre-wrap;max-height:220px;overflow:auto}
    .ghost{opacity:.65}
  </style>
</head>
<body>
  <div class="wrap">
    <section class="card">
      <div class="row" style="justify-content:space-between">
        <div>
          <div id="status" class="badge">Init…</div>
          <div id="audioBadge" class="badge" style="margin-left:6px">• (stumm?)</div>
        </div>
        <div class="row">
          <button id="btnStop" class="btn">Interview abbrechen</button>
        </div>
      </div>

      <div class="grid2" style="margin-top:12px">
        <div>
          <div class="muted" style="margin-bottom:6px">Kamera-Vorschau</div>
          <video id="localVideo" autoplay playsinline muted></video>
          <div class="muted" style="margin:8px 0 4px">Mic-Level</div>
          <div class="bar"><i id="micLevel"></i></div>
        </div>
        <div>
          <div class="muted" style="margin-bottom:6px">Assistent-Audio</div>
          <audio id="remoteAudio" controls></audio>
          <div class="muted" style="margin:8px 0 4px">System</div>
          <div class="bar"><i id="sysLevel"></i></div>
          <div class="muted" style="margin-top:10px">Hinweis: Bei blockiertem Autoplay ggf. in den Player klicken.</div>
        </div>
      </div>
    </section>

    <aside class="card">
      <div style="font-weight:600;margin-bottom:8px">Recorder / Upload</div>
      <div class="row">
        <div class="badge ghost" id="recState">idle</div>
        <div class="badge ghost" id="muxState" style="margin-left:6px">no upload</div>
      </div>
      <div class="muted" style="margin:8px 0 4px">Fortschritt</div>
      <div class="bar"><i id="recProgress"></i></div>
      <div class="muted" style="margin:10px 0 4px">Logs</div>
      <div id="log" class="log"></div>
    </aside>
  </div>

  <script>
  // -------------------- Query --------------------
  const Q = new URLSearchParams(location.search);
  const UID = (Q.get('uid')||'').trim();
  const COMPANY_ID = (Q.get('companyId')||'').trim();
  const LANG = (Q.get('lang')||'de').toLowerCase();
  const VOICE = (Q.get('voice')||'verse').toLowerCase();

  // -------------------- UI Els -------------------
  const $status = document.getElementById('status');
  const $badge  = document.getElementById('audioBadge');
  const $mic    = document.getElementById('micLevel');
  const $sys    = document.getElementById('sysLevel');
  const $log    = document.getElementById('log');
  const $localV = document.getElementById('localVideo');
  const $remoteA= document.getElementById('remoteAudio');
  const $btnStop= document.getElementById('btnStop');
  const $recState = document.getElementById('recState');
  const $muxState = document.getElementById('muxState');
  const $recProg  = document.getElementById('recProgress');

  function log(...a){
    console.log(...a);
    const s = a.map(x => typeof x==='object' ? JSON.stringify(x) : String(x)).join(' ');
    $log.textContent += s + "\n";
    $log.scrollTop = $log.scrollHeight;
  }
  function setStatus(kind, text){
    $status.textContent = text || kind || '';
    $status.style.background = kind==='err' ? '#3b0d0d' : (kind==='ok' ? '#0d3b2a' : (kind==='warn' ? '#3b2f0d' : '#0f172a'));
    $status.style.borderColor = kind==='err' ? '#7f1d1d' : (kind==='ok' ? '#14532d' : (kind==='warn' ? '#7f6a1d' : '#24314f'));
  }

  // -------------------- Endpunkt -----------------
  const TOKEN_URL = 'https://www.clarity-nvl.com/_functions/realtimeToken';

  // -------------------- Global State -------------
  let pc, DC;
  let localStream, mixedStream, mediaRecorder;
  let remoteStream;
  let audioCtx, remoteSource;
  let destroyed = false;

  // Mic gate
  let micTrack = null;
  let MIC_UPLINK_OPEN = false;

  // Parent-Handshake / Flags
  let PARENT_UPLOAD = false;
  let RECORD_ENABLED = false;              // Parent toggled
  let READY_FOR_RECORDING = false;         // intern: erst nach prepared
  let CTX = { reportLang:'de', interviewMode:'structured', roleProfileText:'', docsText:'' };
  let ENDED_SENT = false;
  let DC_OPEN = false;
  let AUDIO_UNLOCKED = false;

  // Interview control
  let PRIMED = false;
  let PRIME_TIMER = null;
  let FIRST_Q_SENT = false;
  let READY_DETECTED = false;
  let AUTO_FIRST_Q_TIMER = null;

  // ===== DC-Queue + Wait =====
  let dcQueue = [];
  function dcSend(obj){
    if (destroyed) return;
    try{
      if (DC_OPEN && DC && DC.readyState === 'open') {
        DC.send(JSON.stringify(obj));
      } else {
        dcQueue.push(obj);
      }
    }catch(e){ console.warn('[RTC] dcSend fail', e); }
  }
  function flushDCQueue(){
    if (!DC_OPEN || !DC || DC.readyState!=='open') return;
    try { const pending = dcQueue.splice(0); for (const msg of pending) DC.send(JSON.stringify(msg)); } catch(e){}
  }

  // -------------------- Token --------------------
  async function getRealtimeToken(payload){
    log('[Live] Fetching token…', TOKEN_URL, JSON.stringify(payload));
    const r = await fetch(TOKEN_URL,{ method:'POST', headers:{'Content-Type':'application/json'}, body:JSON.stringify(payload) });
    if (!r.ok){
      const t = await r.text().catch(()=>r.statusText);
      throw new Error('token_http_'+r.status+': '+t);
    }
    const data = await r.json().catch(e=>{ throw new Error('token_json_parse: '+e); });
    log('[Live] Token response:', r.status, data);
    if (!data?.ok) throw new Error('token_error: '+(data?.error||'unknown')+' '+(data?.message||''));
    const sec = extractClientSecret(data);
    if (!sec) throw new Error('client_secret_missing');
    log('[Live] effective', { lang:data.reportLang||payload.lang, voice:data.voice, model:data.model });
    return { secret: sec, ctx: data };
  }
  function extractClientSecret(obj){
    if (!obj || typeof obj !== 'object') return null;
    if (typeof obj.client_secret === 'string') return obj.client_secret;
    if (obj.client_secret && typeof obj.client_secret.value === 'string') return obj.client_secret.value;
    if (obj.data && typeof obj.data.client_secret === 'string') return obj.data.client_secret;
    if (obj.token && typeof obj.token === 'string') return obj.token;
    if (obj.clientSecret && typeof obj.clientSecret === 'string') return obj.clientSecret;
    return null;
  }

  // -------------------- Realtime WebRTC ----------
  async function connectRealtime(clientSecret){
    pc = new RTCPeerConnection({ iceServers: [{ urls: ['stun:stun.l.google.com:19302'] }] });

    pc.onconnectionstatechange = () => {
      if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected'){
        setStatus('err','Verbindung unterbrochen');
      }
    };

    pc.ontrack = async (ev) => {
      log('[RTC] ontrack kind=', ev.track.kind, 'streams=', ev.streams.length);
      if (!remoteStream) remoteStream = new MediaStream();
      remoteStream.addTrack(ev.track);
      $remoteA.srcObject = remoteStream;

      try{
        audioCtx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();
        await audioCtx.resume().catch(()=>{});
        remoteSource = audioCtx.createMediaStreamSource(remoteStream);
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 512;
        const buf = new Uint8Array(analyser.frequencyBinCount);
        remoteSource.connect(analyser);
        (function tick(){
          if (destroyed) return;
          analyser.getByteTimeDomainData(buf);
          let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
          $sys.style.width = Math.min(100, dev*2) + '%';
          requestAnimationFrame(tick);
        })();
      }catch(_){}

      try {
        $remoteA.muted = false; $remoteA.volume = 1.0;
        $remoteA.setAttribute('autoplay',''); $remoteA.setAttribute('playsinline','');
        await $remoteA.play();
        AUDIO_UNLOCKED = true;
        maybeSignalReady();
        log('[RTC] remote audio playing.');
      } catch(e) {
        log('[RTC] play() blocked', e);
        setStatus('warn','Autoplay blockiert – bitte in den Player klicken.');
      }

      // Simple input meter
      try {
        const levelAnalyser = audioCtx.createAnalyser();
        levelAnalyser.fftSize = 1024;
        remoteSource.connect(levelAnalyser);
        const tbuf = new Uint8Array(levelAnalyser.frequencyBinCount);
        let lastNZ = 0;
        (function ping(){
          if (destroyed) return;
          levelAnalyser.getByteTimeDomainData(tbuf);
          let dev = 0;
          for (let i=0;i<tbuf.length;i++) dev = Math.max(dev, Math.abs(tbuf[i]-128));
          if (dev>2) lastNZ = performance.now();
          $badge.textContent = (performance.now()-lastNZ < 2000) ? '• Audio in' : '• (stumm?)';
          requestAnimationFrame(ping);
        })();
      }catch(_){}
    };

    // *** Nur AUDIO an Server senden – aber stumm bis nach Welcome ***
    if (localStream){
      micTrack = localStream.getAudioTracks()[0] || null;
      if (micTrack){ micTrack.enabled = false; }  // harte Stummschaltung für Uplink
      if (micTrack) pc.addTrack(micTrack, localStream);
    }

    // DataChannel
    DC = pc.createDataChannel('oai-events');
    DC.onopen = () => {
      DC_OPEN = true;
      log('[RTC] DC open');
      flushDCQueue();
      maybeSignalReady();
    };
    DC.onmessage = onRealtimeEvent;

    const offer = await pc.createOffer({ offerToReceiveAudio:true, offerToReceiveVideo:false });
    await pc.setLocalDescription(offer);

    const r = await fetch('https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${clientSecret}`,
        'Content-Type': 'application/sdp'
      },
      body: offer.sdp
    });

    if (!r.ok){
      const t = await r.text().catch(()=>r.statusText);
      throw new Error('realtime_handshake_failed: ' + t);
    }

    const answer = { type: 'answer', sdp: await r.text() };
    await pc.setRemoteDescription(answer);
  }

  // ---------------- Event Handler (DC) -----------
  function onRealtimeEvent(ev){
    let data = ev && ev.data;
    try{ if (typeof data === 'string') data = JSON.parse(data); }catch(_){}
    if (!data) return;

    if (data.type === 'response.state' && data.state){
      parent?.postMessage?.({ type:'clarity.avatar', data:{ state:data.state } }, '*');
      return;
    }

    // Naive Transcript-Keyword-Erkennung
    try{
      const t = String(data.text || data.transcript || data.message || '').toLowerCase();
      if (t) {
        if (!READY_DETECTED && (t.includes('bereit') || t.includes('ready') || t.includes('start') || t.includes('los'))) {
          READY_DETECTED = true;
          maybeAskFirstQuestion();
        }
      }
    }catch(_){}
  }

  // ---------------- Interview-Plan / Priming -----
  function buildInterviewPlan(ctx){
    const isDE = String(ctx.reportLang||'de').toLowerCase().startsWith('de');
    const txt = ((ctx.roleProfileText||'') + '\n' + (ctx.docsText||'')).toLowerCase();
    const position = ctx.position || '';
    const maxQ = Math.max(3, Math.min(12, (ctx.interviewConfig?.maxQuestions ?? 10)));
    const kws = [];
    [['typescript','TypeScript'],['javascript','JavaScript'],['react','React'],['python','Python'],['java','Java'],['aws','AWS'],['sql','SQL'],['docker','Docker'],['kubernetes','Kubernetes'],['sales','Sales'],['seo','SEO']]
      .forEach(([n,l])=>{ if(txt.includes(n)) kws.push(l); });
    const bankDE = [
      'Bitte stellen Sie sich kurz vor und skizzieren Sie Ihren beruflichen Hintergrund.',
      position ? `Welche Erfahrung haben Sie in Bezug auf die Position „${position}“?` : 'Welche Ihrer Erfahrungen passt am besten zur ausgeschriebenen Position?',
      kws.length ? `Wie sicher sind Sie in ${kws.slice(0,3).join(', ')}? Bitte mit kurzen Beispielen.` : 'Nennen Sie zwei fachliche Stärken, die Sie mit Beispielen belegen.',
      'Beschreiben Sie eine Herausforderung in Ihrem letzten Projekt und wie Sie sie gelöst haben.',
      'Wie priorisieren Sie, wenn mehrere Aufgaben gleichzeitig anstehen?',
      'Welche Erwartungen haben Sie an Team, Führung und Arbeitsweise?',
      'Wie halten Sie Ihr Wissen aktuell?',
      'Was motiviert Sie besonders an dieser Rolle?',
      'Welche Gehalts- und Starttermin-Vorstellungen haben Sie?',
      'Haben Sie noch Fragen an uns?'
    ];
    const bankEN = [
      'Please introduce yourself and summarize your professional background.',
      position ? `What relevant experience do you have for the “${position}” role?` : 'Which parts of your background fit this role best?',
      kws.length ? `How confident are you with ${kws.slice(0,3).join(', ')}? Give brief examples.` : 'Name two core strengths and back them with examples.',
      'Describe a challenge in a recent project and how you solved it.',
      'How do you prioritize when several tasks compete?',
      'What do you expect from team, leadership, and ways of working?',
      'How do you keep your knowledge up to date?',
      'What motivates you most about this role?',
      'What are your salary expectations and earliest start date?',
      'Any questions for us?'
    ];
    return (isDE?bankDE:bankEN).slice(0,maxQ);
  }

  function buildSystemInstructions(ctx){
    const isDE = String(ctx.reportLang||'de').toLowerCase().startsWith('de');
    const plan = buildInterviewPlan(ctx);
    const planTxt = plan.map((q,i)=>`${i+1}. ${q}`).join('\n');
    const hard = isDE
      ? ['SPRICHT STRENG DEUTSCH. Keine andere Sprache verwenden.',
         'Geführtes Interview: 1 Frage pro Runde, max. 1 Nachfrage.',
         'Nach jeder Frage still warten (Server-VAD).',
         'Strikt bei Rolle/Ausschreibung bleiben. Kein Smalltalk.',
         'Erst starten, wenn Kandidat:in „bereit“ signalisiert.',
         'Rückfragen der Kandidat:in freundlich für das Ende parken.',
         'Am Ende kurz bedanken und „Auswertung folgt“ sagen.']
      : ['SPEAK STRICTLY ENGLISH.',
         'Structured interview: one question at a time, max one follow-up.',
         'After each question, wait silently (server VAD).',
         'Stay on the role/ad. No small talk.',
         'Start only after candidate signals “ready”.',
         'Park candidate questions for the end.',
         'At the end thank them and say the report will follow.'];
    const header = isDE ? '--- INTERVIEW-PLAN (DE) ---' : '--- INTERVIEW PLAN (EN) ---';
    return [hard.join(' '), header, planTxt].join('\n');
  }

  function primeStrictLanguage(reportLang){
    const spoken = String(reportLang||'de').toLowerCase().startsWith('de') ? 'de' : 'en';
    const sys = buildSystemInstructions(CTX);

    // Turn detection AUS, damit nur der Assistent spricht
    dcSend({ type:'session.update', session:{
      voice: VOICE,
      instructions: sys,
      spoken_language: spoken,
      input_audio_format:'pcm16',
      output_audio_format:'pcm16',
      input_audio_transcription:{ model:'whisper-1', language: spoken },
      turn_detection:{ type:'none' },
      modalities:['audio']
    }});
    log('[PRIME] session.update (lang+none VAD)', spoken);

    // Verstärkung
    setTimeout(()=> dcSend({ type:'session.update', session:{
      spoken_language: spoken,
      input_audio_transcription:{ model:'whisper-1', language: spoken }
    }}), 500);
  }

  function speakPrepThenWelcome(){
    const isDE = String(CTX.reportLang||'de').toLowerCase().startsWith('de');
    const prep = isDE ? 'Einen Moment bitte, ich bereite das Interview vor …' : 'One moment please, I am preparing the interview …';
    const welcome = isDE
      ? `Willkommen! Dieses Interview wird im Auftrag von ${CTX.companyName||'Clarity'} geführt. Sind Sie bereit zu starten? Bitte sagen Sie einfach „bereit“.`
      : `Welcome! This interview is conducted on behalf of ${CTX.companyName||'Clarity'}. Are you ready to begin? Please say “ready”.`;

    dcSend({ type:'response.create', response:{ conversation:'default', instructions: prep, modalities:['audio'] }});
    log('[WELCOME] prep spoken');

    setTimeout(()=>{
      dcSend({ type:'response.create', response:{ conversation:'default', instructions: welcome, modalities:['audio'] }});
      log('[WELCOME] welcome spoken');

      // Jetzt VAD AN …
      dcSend({ type:'session.update', session:{
        turn_detection:{ type:'server_vad', threshold:0.9, prefix_padding_ms:200, silence_duration_ms:700 }
      }});
      log('[PRIME] VAD enabled');

      // … und Uplink-Mic öffnen (erst jetzt kann GPT was hören)
      enableMicUplink();

      // Parent: prepared (Overlay schließen)
      READY_FOR_RECORDING = true;
      parent?.postMessage?.({ type:'clarity.live.prepared', data:{} }, '*');
      log('[PREPARED] sent to parent');

      // Falls niemand „bereit“ sagt: nach 6s automatisch starten
      clearTimeout(AUTO_FIRST_Q_TIMER);
      AUTO_FIRST_Q_TIMER = setTimeout(()=>{ if (!READY_DETECTED) maybeAskFirstQuestion(); }, 6000);
    }, 1500);
  }

  function enableMicUplink(){
    try{
      if (micTrack && !MIC_UPLINK_OPEN){
        micTrack.enabled = true;
        MIC_UPLINK_OPEN = true;
        log('[MIC] uplink enabled');
      }
    }catch(e){ log('[MIC] enable failed', e); }
  }

  function askFirstQuestion(){
    if (FIRST_Q_SENT) return;
    FIRST_Q_SENT = true;
    const plan = buildInterviewPlan(CTX);
    const isDE = String(CTX.reportLang||'de').toLowerCase().startsWith('de');
    const first = plan[0] || (isDE ? 'Bitte stellen Sie sich kurz vor.' : 'Please introduce yourself.');
    const q = (isDE ? `Erste Frage: ${first}` : `First question: ${first}`);
    dcSend({ type:'response.create', response:{ conversation:'default', instructions: q, modalities:['audio'] }});
    log('[QUESTION] first sent');
  }
  function maybeAskFirstQuestion(){ try{ askFirstQuestion(); }catch(_){} }

  // ---------------- Media / Recording ------------
  async function setupLocalMedia(){
    try{
      localStream = await navigator.mediaDevices.getUserMedia({
        audio:{ echoCancellation:true, noiseSuppression:true, autoGainControl:true },
        video:true
      });
    } catch(e) {
      setStatus('err','Mic/Kamera blockiert. In Browser Freigabe erteilen.');
      log('[Media] getUserMedia error', e);
      throw e;
    }
    $localV.srcObject = localStream;

    try{
      const ctx = new (window.AudioContext || window.webkitAudioContext)();
      const src = ctx.createMediaStreamSource(localStream);
      const analyser = ctx.createAnalyser();
      analyser.fftSize = 512;
      const buf = new Uint8Array(analyser.frequencyBinCount);
      src.connect(analyser);
      (function tick(){
        if (destroyed) return;
        analyser.getByteTimeDomainData(buf);
        let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
        $mic.style.width = Math.min(100, dev*2) + '%';
        requestAnimationFrame(tick);
      })();
    }catch(_){}
  }

  function ensureAudioPlayback(){ return $remoteA.play().then(()=>{AUDIO_UNLOCKED=true; maybeSignalReady(); return true;}).catch(()=>false); }

  async function blobToDataURL(blob){
    return new Promise((resolve,reject)=>{
      const reader = new FileReader();
      reader.onloadend = ()=> resolve(reader.result);
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  function maybeStartRecording(){
    if (!RECORD_ENABLED){ log('[REC] request ignored (flag=false)'); return; }
    if (!READY_FOR_RECORDING){ log('[REC] request queued (not prepared yet)'); return; }
    if (mediaRecorder){ log('[REC] already running'); return; }
    if (!localStream) return;

    const ctx = new (window.AudioContext || window.webkitAudioContext)();
    const dest = ctx.createMediaStreamDestination();

    const micSrc = ctx.createMediaStreamSource(localStream);
    micSrc.connect(dest);

    if (remoteStream){
      try{
        const remSrc = ctx.createMediaStreamSource(remoteStream);
        remSrc.connect(dest);
      }catch(e){ log('[REC] remote mix fail', e); }
    }

    mixedStream = new MediaStream();
    dest.stream.getAudioTracks().forEach(t => mixedStream.addTrack(t));
    const cam = localStream.getVideoTracks()[0];
    if (cam) mixedStream.addTrack(cam);

    const hasVideo = !!mixedStream.getVideoTracks().length;
    mediaRecorder = new MediaRecorder(mixedStream, {
      mimeType: hasVideo ? 'video/webm;codecs=vp8,opus' : 'audio/webm;codecs=opus',
      videoBitsPerSecond: hasVideo ? 1_200_000 : undefined,
      audioBitsPerSecond: 128_000
    });

    const chunks = [];
    mediaRecorder.onstart = ()=>{ $recState.textContent='recording'; log('[REC] started'); };
    mediaRecorder.ondataavailable = (e)=>{ if (e.data && e.data.size>0) chunks.push(e.data); };
    mediaRecorder.onstop = async ()=>{
      $recState.textContent='stopped';
      const hasVid = !!mixedStream.getVideoTracks().length;
      const blob = new Blob(chunks, { type: hasVid ? 'video/webm' : 'audio/webm' });
      log('[REC] stop, size=', blob.size);

      try{
        $muxState.textContent='exporting';
        $recProg.style.width = '60%';

        if (PARENT_UPLOAD) {
          const dataUrl = await blobToDataURL(blob);
          parent?.postMessage?.({ type:'recorder:export', data: { dataUrl, kind: (hasVid?'video':'audio'), bytes: blob.size } }, '*');
          $recProg.style.width = '100%';
          $muxState.textContent='export sent';
        } else {
          $recProg.style.width = '100%';
          $muxState.textContent='no parent upload';
        }

        sendEndedOnce();
      } catch(e){
        $muxState.textContent='export failed';
        log('[REC] export failed', e);
        parent?.postMessage?.({ type:'clarity.live.error', data:{ message: String(e?.message||e) } }, '*');
      } finally {
        chunks.length = 0;
      }
    };

    mediaRecorder.start();
  }

  function stopRecording(){
    try{ if (mediaRecorder && mediaRecorder.state!=='inactive') mediaRecorder.stop(); }catch(_){}
    try{ mixedStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
  }

  function sendEndedOnce(){
    if (ENDED_SENT) return;
    ENDED_SENT = true;
    parent?.postMessage?.({ type:'clarity.live.ended', data:{} }, '*');
  }

  function maybeSignalReady(){
    if (DC_OPEN && AUDIO_UNLOCKED){
      parent?.postMessage?.({ type:'clarity.live.ready', data:{} }, '*');
      // Fallback: falls Parent nicht primt, nach 1.0s selbst primen
      clearTimeout(PRIME_TIMER);
      PRIME_TIMER = setTimeout(()=>{
        if (!PRIMED){
          log('[PRIME] parent prime missing → auto-prime');
          PRIMED = true;
          const rl = CTX.reportLang || LANG;
          primeStrictLanguage(rl);
          speakPrepThenWelcome();
        }
      }, 1000);
    }
  }

  // -------------------- Start / Stop Flow --------
  async function startInterview(){
    destroyed = false;
    ENDED_SENT = false;
    MIC_UPLINK_OPEN = false;
    setStatus('info','Initialisiere…');

    await setupLocalMedia();

    const { secret } = await getRealtimeToken({ uid:UID, companyId:COMPANY_ID, lang:LANG, voice:VOICE, debug:true, allowNoInvite:true });

    await connectRealtime(secret);
    await ensureAudioPlayback();

    setStatus('ok','Verbunden • bereit');
  }

  async function hangup(){
    destroyed = true;
    try{ DC?.close(); }catch(_){}
    try{ pc?.getSenders()?.forEach(s=>s.track && s.track.stop()); }catch(_){}
    try{ pc?.getReceivers()?.forEach(r=>r.track && r.track.stop()); }catch(_){}
    try{ pc?.close(); }catch(_){}
    try{ localStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
    try{ remoteStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
    stopRecording();
    setStatus('warn','Beendet');
    log('[Live] hangup done');
    sendEndedOnce();
  }

  // -------------------- Parent-Messaging ---------
  window.addEventListener('message', (ev)=>{
    const msg = ev.data || {};
    const type = msg.type;

    if (type === 'clarity.live.context') {
      CTX = { ...CTX, ...(msg.data || {}) };
      PARENT_UPLOAD  = (CTX.uploadMode === 'parent');
      log('[CTX]', CTX);
      return;
    }

    if (type === 'clarity.live.record') {
      RECORD_ENABLED = !!(msg.data && msg.data.enabled);
      log('[REC] record flag from parent:', RECORD_ENABLED);
      if (RECORD_ENABLED) maybeStartRecording();
      return;
    }

    if (type === 'clarity.live.start') {
      startInterview().catch(e => {
        setStatus('err', 'Startfehler: ' + (e?.message||e));
        log(e);
        parent?.postMessage?.({ type:'clarity.live.error', data:{ message:String(e?.message||e) } }, '*');
      });
      return;
    }

    // PRIME (vom Parent) – sofort Welcome + später prepared
    if (type === 'clarity.live.prime') {
      PRIMED = true;
      clearTimeout(PRIME_TIMER);
      const rl = (msg.data && msg.data.reportLang) || CTX.reportLang || LANG;
      CTX.reportLang = rl;
      primeStrictLanguage(rl);
      speakPrepThenWelcome();
      return;
    }

    if (type === 'clarity.live.stop' || type === 'clarity.live.hangup') { hangup(); return; }
    if (type === 'recorder:export') { stopRecording(); return; }
  });

  document.getElementById('btnStop').addEventListener('click', hangup);
  window.addEventListener('beforeunload', hangup);
  // Parent-iframe braucht: allow="microphone; camera; autoplay"
  </script>
</body>
</html>
