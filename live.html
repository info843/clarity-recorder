<!-- public/live.html -->
<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <title>Clarity Live Interview</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root{color-scheme:dark}
    html,body{margin:0;padding:0;background:#0b0e12;color:#e5e7eb;font-family:system-ui,ui-sans-serif,Segoe UI,Roboto}
    .wrap{display:flex;flex-direction:column;gap:12px;max-width:920px;margin:0 auto;padding:16px}
    .row{display:flex;gap:12px;align-items:center;flex-wrap:wrap}
    .videoBox{position:relative;width:100%;max-width:720px;aspect-ratio:16/9;background:#0f172a;border:1px solid #1f2937;border-radius:12px;overflow:hidden}
    video{width:100%;height:100%;object-fit:cover;background:#0f172a}
    .status{font-size:13px;opacity:.95}
    .meter{height:6px;background:#1f2937;border-radius:8px;overflow:hidden}
    .bar{height:100%;width:0%;background:#22c55e;transition:width .1s linear}
    .badge{font-size:12px;color:#9ca3af}
    .btn{appearance:none;border:1px solid #334155;background:#111827;color:#e5e7eb;border-radius:10px;padding:8px 12px;cursor:pointer;font-size:14px}
    .btn:hover{background:#0b1220}
    .danger{border-color:#dc2626;color:#fecaca}
    .overlay{position:absolute;inset:0;display:none;align-items:center;justify-content:center;background:rgba(2,6,23,.6);backdrop-filter:saturate(120%) blur(2px)}
    .overlay.show{display:flex}
    .overlay .card{border:1px solid #334155;background:#0b1220;color:#e5e7eb;border-radius:12px;padding:18px 20px;text-align:center;max-width:420px}
    .overlay .card .title{font-weight:600;margin-bottom:6px}
    .overlay .card .txt{font-size:13px;opacity:.9;margin-bottom:12px}
    .overlay .card .act{display:flex;gap:10px;justify-content:center}
  </style>
</head>
<body>
<div class="wrap">
  <div class="videoBox">
    <video id="local" autoplay playsinline muted></video>
    <div id="audioGate" class="overlay">
      <div class="card">
        <div class="title">Audio aktivieren</div>
        <div class="txt">Bitte einmal klicken, um die Sprachausgabe zu erlauben.</div>
        <div class="act">
          <button id="btnEnableAudio" class="btn">Audio einschalten</button>
          <button id="btnStopFromOverlay" class="btn danger">Abbrechen</button>
        </div>
      </div>
    </div>
  </div>
  <div class="meter" aria-hidden="true"><div id="meterBar" class="bar"></div></div>
  <div class="row">
    <div id="status" class="status">Initialisiere…</div>
    <div id="badge" class="badge"></div>
  </div>
  <div class="row">
    <button id="btnStop" class="btn danger">Interview abbrechen</button>
  </div>
</div>

<script>
(() => {
  // ---------- Endpoints / Konst ----------
  const TOKEN_ENDPOINTS = [
    'https://www.clarity-nvl.com/_functions/realtimeToken',
    '/api/wix-token-proxy'
  ];
  const MUX_ENDPOINTS = [
    'https://www.clarity-nvl.com/_functions/muxDirectUpload',
    '/api/mux-upload'
  ];
  const MODEL_DEFAULT = 'gpt-4o-realtime-preview';
  const ALLOWED_VOICES = new Set(['alloy','ash','ballad','coral','echo','sage','shimmer','verse','marin','cedar']);

  const qs = new URLSearchParams(location.search);
  const PARAMS = {
    uid: qs.get('uid') || '',
    companyId: qs.get('companyId') || '',
    lang: (qs.get('lang') || 'de').toLowerCase(),
    voice: (qs.get('voice') || 'verse').toLowerCase()
  };
  if (!ALLOWED_VOICES.has(PARAMS.voice)) PARAMS.voice = 'verse';

  let CTX = { companyName:'', position:'', lang:PARAMS.lang, uid:PARAMS.uid, companyId:PARAMS.companyId, voice:PARAMS.voice, mode:'live-video' };

  // ---------- Refs / State ----------
  const $video = document.getElementById('local');
  const $status = document.getElementById('status');
  const $badge = document.getElementById('badge');
  const $bar = document.getElementById('meterBar');
  const $audioGate = document.getElementById('audioGate');
  const $btnEnableAudio = document.getElementById('btnEnableAudio');
  const $btnStop = document.getElementById('btnStop');
  const $btnStopFromOverlay = document.getElementById('btnStopFromOverlay');

  let PC = null, DC = null, REMOTE_AUDIO = null;
  let mediaStream = null;         // Original: Mic (+ ggf. Cam)
  let mixedStream = null;         // Für Recording: Video + (Mic + GPT)
  let mediaRecorder = null; let recChunks = [];
  let muxUploadUrl = null;
  let started = false; let stopped = false;

  // WebAudio: Mixing
  let audioCtx;
  let micSource, remoteSource;
  let micGain, remoteGain;
  let recordDest;                 // MediaStreamDestination for recording
  const REMOTE_PLAYBACK_GAIN = 1.0;  // Lautstärke für Lautsprecher
  const REMOTE_RECORD_GAIN  = 1.0;   // Lautstärke im Recording-Mix
  const MIC_RECORD_GAIN     = 1.0;   // Mic im Recording-Mix

  // ---------- UI / Utils ----------
  function setStatus(text, badge='') { $status.textContent = text; $badge.textContent = badge ? `• ${badge}` : ''; }
  function postToParent(obj){ try{ window.parent.postMessage(obj, '*'); }catch(_){} }
  function setAvatarState(st){ postToParent({ type:'clarity.avatar', data:{ state: st } }); }

  // Mic Level Meter (Time-Domain Peak)
  let analyser, meterRAF;
  async function startLevelMeter(stream) {
    try{
      audioCtx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();
      const src = audioCtx.createMediaStreamSource(stream);
      analyser = audioCtx.createAnalyser();
      analyser.fftSize = 2048;
      src.connect(analyser);
      const data = new Uint8Array(analyser.frequencyBinCount);
      const tick = () => {
        analyser.getByteTimeDomainData(data);
        let maxDev = 0;
        for (let i=0;i<data.length;i++){
          const v = (data[i]-128)/128;
          maxDev = Math.max(maxDev, Math.abs(v));
        }
        const pct = Math.min(100, Math.round(maxDev * 180));
        $bar.style.width = pct + '%';
        meterRAF = requestAnimationFrame(tick);
      };
      meterRAF = requestAnimationFrame(tick);
    }catch(e){}
  }

  async function warmBeep(){
    try{
      const ctx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();
      const o = ctx.createOscillator(); const g = ctx.createGain();
      o.type = 'sine'; o.frequency.value = 880; g.gain.value = 0.001;
      o.connect(g).connect(ctx.destination);
      o.start(); setTimeout(()=>{ o.stop(); }, 120);
    }catch(_){}
  }

  function showAudioGate(){ $audioGate.classList.add('show'); }
  function hideAudioGate(){ $audioGate.classList.remove('show'); }

  async function ensureAudioPlayback() {
    try { await (audioCtx?.resume?.()); } catch(_){}
    try {
      if (!REMOTE_AUDIO) return false;
      REMOTE_AUDIO.muted = false;
      REMOTE_AUDIO.volume = 1.0;
      await REMOTE_AUDIO.play();
      hideAudioGate();
      return true;
    } catch (err) {
      showAudioGate();
      return false;
    }
  }

  $btnEnableAudio.addEventListener('click', async ()=>{
    await warmBeep();
    await ensureAudioPlayback();
  });
  $btnStop.addEventListener('click', hangup);
  $btnStopFromOverlay.addEventListener('click', hangup);

  function hangup(){
    if (stopped) return;
    stopped = true;
    try{ if (mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop(); }catch(_){}
    try{ mediaStream?.getTracks()?.forEach(t=>t.stop()); }catch(_){}
    try{ PC?.getSenders()?.forEach(s=>{ try{s.track?.stop();}catch(_){}}); }catch(_){}
    try{ PC?.close?.(); }catch(_){}
    setAvatarState('idle');
    setStatus(CTX.lang.startsWith('de')? 'Beendet.' : 'Ended.', '');
    postToParent({ type:'clarity.live.hangup' });
  }

  // ---------- Preflight (Devices) ----------
  async function preflight(){
    setStatus(CTX.lang.startsWith('de') ? 'Gerätecheck…' : 'Running preflight…');
    try{
      mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: { echoCancellation:true, noiseSuppression:true, channelCount:1, sampleRate:48000 },
        video: CTX.mode === 'audio' ? false : { width: { ideal: 640 }, height: { ideal: 360 } }
      });
      $video.srcObject = mediaStream; // Preview
      await startLevelMeter(mediaStream);
      await warmBeep();
      setStatus(CTX.lang.startsWith('de') ? 'Sagen Sie „Ich bin bereit“.' : 'Please say “I am ready”.', 'Preflight');
      setTimeout(startInterview, 1200);
    }catch(e){
      console.error('getUserMedia error', e);
      setStatus(CTX.lang.startsWith('de') ? 'Kamera/Mikrofon fehlen.' : 'Camera/Microphone unavailable.', 'Error');
    }
  }

  // ---------- Token / Mux ----------
  function extractClientSecret(json){
    if (!json || typeof json !== 'object') return null;
    if (typeof json.client_secret === 'string') return json.client_secret;
    if (json.client_secret && typeof json.client_secret.value === 'string') return json.client_secret.value;
    if (json.data && json.data.client_secret && typeof json.data.client_secret.value === 'string') return json.data.client_secret.value;
    if (typeof json.value === 'string') return json.value;
    if (typeof json.secret === 'string') return json.secret;
    if (typeof json.clientSecret === 'string') return json.clientSecret;
    if (typeof json.token === 'string') return json.token;
    return null;
  }

  async function getRealtimeToken(payload){
    const body = JSON.stringify(payload);
    let last = { status: 0, json: null };
    for (const url of TOKEN_ENDPOINTS){
      try{
        console.log('[Live] Fetching token…', url, body);
        const r = await fetch(url, { method:'POST', headers:{'Content-Type':'application/json'}, body });
        const txt = await r.text();
        let json=null; try{ json = txt ? JSON.parse(txt) : null; }catch(_){}
        console.log('[Live] Token response:', r.status, json||txt);

        const secret = extractClientSecret(json);
        const model  = json?.model || MODEL_DEFAULT;
        if (secret) return { ok:true, client_secret: secret, model, raw: json };
        last = { status:r.status, json };
      }catch(e){
        console.warn('[Live] token fetch failed', url, e);
        last = { status: 0, json: { error: String(e?.message||e) } };
      }
    }
    return { ok:false, status:last.status, raw:last.json };
  }

  async function getMuxUploadUrl(payload){
    const body = JSON.stringify(payload);
    for (const url of MUX_ENDPOINTS){
      try{
        console.log('[Live] Request Mux upload…', url, body);
        const r = await fetch(url, { method:'POST', headers:{'Content-Type':'application/json'}, body });
        const txt = await r.text();
        let json=null; try{ json = txt ? JSON.parse(txt) : null; }catch(_){}
        console.log('[Live] Mux response:', r.status, json||txt);
        if (json && json.ok && json.upload && json.upload.url) return json.upload.url;
        if (json && json.url) return json.url;
      }catch(e){
        console.warn('[Live] mux fetch failed', url, e);
      }
    }
    return null;
  }

  // ---------- Realtime / WebRTC ----------
  function onRealtimeEvent(ev){
    try{
      const msg = typeof ev.data === 'string' ? JSON.parse(ev.data) : ev.data;
      const type = msg?.type || msg?.event || '';
      if (type.includes('delta') && msg?.delta) {
        postToParent({ type:'rt.transcript.partial', data:{ text: String(msg.delta) } });
      }
      if (type.includes('completed')) {
        postToParent({ type:'rt.transcript.final', data:{} });
      }
    }catch(e){}
  }

  function dcSend(obj){
    try{ DC && DC.readyState === 'open' && DC.send(JSON.stringify(obj)); }catch(_){}
  }
  function primeSession(){
    dcSend({
      type: 'session.update',
      session: {
        voice: CTX.voice,
        modalities: ['audio'],         // default: audio responses
        turn_detection: { type:'server_vad' },
        input_audio_format: 'pcm16',
        output_audio_format: 'wav'
      }
    });
  }
  function sendGreeting(ctx){
    const de = ctx.lang?.startsWith('de');
    const intro = de
      ? `Willkommen! Dieses Interview führen wir im Auftrag von ${ctx.companyName||'Clarity'}. Bitte antworten Sie frei ohne Hilfsmittel.`
      : `Welcome! This interview is conducted on behalf of ${ctx.companyName||'Clarity'}. Please answer freely without external aids.`;
    const q1 = de
      ? `Erste Frage: Warum möchten Sie die Position ${ctx.position||''} bei ${ctx.companyName||'uns'}?`
      : `First question: Why are you interested in the ${ctx.position||''} role at ${ctx.companyName||'our company'}?`;
    dcSend({ type:'response.create', response:{ modalities:['audio'], instructions:`${intro}\n${q1}` }});
  }

  async function connectRealtime(clientSecret, model){
    if (!mediaStream) throw new Error('no_media');

    // STUN hilft bei NAT
    PC = new RTCPeerConnection({
      iceServers: [{ urls: ['stun:stun.l.google.com:19302'] }]
    });

    // 1) Mic senden
    mediaStream.getAudioTracks().forEach(tr => PC.addTrack(tr, mediaStream));

    // 2) Assistant-Audio empfangen (entscheidend für hörbare Stimme!)
    PC.addTransceiver('audio', { direction: 'recvonly' });

    // DataChannel für Events
    DC = PC.createDataChannel('oai-events');
    DC.onopen = () => console.log('[RTC] DC open');
    DC.onmessage = onRealtimeEvent;

    // Remote Audio-Track -> Playback + Mix
    PC.ontrack = async (e) => {
      console.log('[RTC] ontrack kind=', e.track?.kind, 'streams=', e.streams?.length);
      const [remoteStream] = e.streams;

      // Playback via <audio> (für Autoplay-Handling)
      if (!REMOTE_AUDIO) {
        REMOTE_AUDIO = new Audio();
        REMOTE_AUDIO.autoplay = true;
        REMOTE_AUDIO.playsInline = true;
        REMOTE_AUDIO.muted = false;
        REMOTE_AUDIO.volume = 1.0;
        document.body.appendChild(REMOTE_AUDIO);
      }
      REMOTE_AUDIO.srcObject = remoteStream;

      // --- WebAudio Mixing initialisieren ---
      audioCtx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();

      // Quellen
      if (!micSource)  micSource  = audioCtx.createMediaStreamSource(mediaStream);
      if (!remoteSource){
        const remoteTrack = remoteStream.getAudioTracks()[0];
        const remoteOnly = new MediaStream([remoteTrack]);
        remoteSource = audioCtx.createMediaStreamSource(remoteOnly);
      }

      // Gains
      if (!micGain){ micGain = audioCtx.createGain(); micGain.gain.value = MIC_RECORD_GAIN; }
      if (!remoteGain){ remoteGain = audioCtx.createGain(); remoteGain.gain.value = REMOTE_RECORD_GAIN; }

      // Ziel für Recording (nur 1x anlegen)
      if (!recordDest){ recordDest = audioCtx.createMediaStreamDestination(); }

      // Verkabeln (idempotent ausreichend, Browser schützt doppelte Verbindungen)
      try{ micSource.connect(micGain).connect(recordDest); }catch(_){}
      try{ remoteSource.connect(remoteGain).connect(recordDest); }catch(_){}

      // Playback: NUR Assistant zur Ausgabe -> kein Echo
      try{
        const playGain = audioCtx.createGain();
        playGain.gain.value = REMOTE_PLAYBACK_GAIN;
        remoteSource.connect(playGain).connect(audioCtx.destination);
      }catch(_){}

      // Recording-Stream aufbauen: Video (falls vorhanden) + Mix-Audio
      const recAudioTrack = recordDest.stream.getAudioTracks()[0];
      const tracks = [];
      mediaStream.getVideoTracks().forEach(v => tracks.push(v));
      if (recAudioTrack) tracks.push(recAudioTrack);
      mixedStream = new MediaStream(tracks);

      try { await (audioCtx?.resume?.()); } catch {}
      try {
        await REMOTE_AUDIO.play();
        hideAudioGate();
        console.log('[RTC] remote audio playing.');
      } catch (err) {
        console.warn('[RTC] autoplay blocked, showing gate:', err);
        showAudioGate();
      }
    };

    const offer = await PC.createOffer({ offerToReceiveAudio: true, offerToReceiveVideo: false });
    await PC.setLocalDescription(offer);

    const url = `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model||MODEL_DEFAULT)}`;
    const ansResp = await fetch(url, {
      method: 'POST',
      headers: { 'Authorization': `Bearer ${clientSecret}`, 'Content-Type': 'application/sdp' },
      body: offer.sdp
    });
    const answerSdp = await ansResp.text();
    await PC.setRemoteDescription({ type:'answer', sdp: answerSdp });

    console.log('[RTC] connected');
  }

  function startRecording(){
    const src = mixedStream || mediaStream;
    if (!src) return;
    try{
      recChunks = [];
      // Video vorhanden? -> webm (video+audio). Sonst audio-only webm/opus
      const hasVideo = src.getVideoTracks().length > 0;
      const mime = hasVideo
        ? 'video/webm;codecs=vp9,opus'
        : 'audio/webm;codecs=opus';

      mediaRecorder = new MediaRecorder(src, { mimeType: mime, videoBitsPerSecond: 800_000 });
      mediaRecorder.ondataavailable = (e)=>{ if (e.data?.size) recChunks.push(e.data); };
      mediaRecorder.onstop = async ()=>{
        if (!muxUploadUrl) return;
        try{
          const blob = new Blob(recChunks, { type: mime });
          await fetch(muxUploadUrl, { method:'PUT', body: blob });
          postToParent({ type:'mux.upload.done', data:{ uploadId: muxUploadUrl.split('/').pop() || '' } });
        }catch(e){
          console.warn('mux PUT failed', e);
          postToParent({ type:'mux.upload.error', data:{ message: String(e?.message||e) } });
        }
      };
      mediaRecorder.start(1000);
      console.log('[REC] started');
    }catch(e){ console.warn('[REC] cannot start', e); }
  }

  // ---------- Start ----------
  async function startInterview(){
    if (started || stopped) return;
    started = true;
    setAvatarState('speaking');
    setStatus(CTX.lang.startsWith('de')? 'Starte Interview…':'Starting interview…', 'Live');

    const token = await getRealtimeToken({
      uid: CTX.uid,
      companyId: CTX.companyId,
      lang: CTX.lang,
      voice: CTX.voice,
      debug: true,
      allowNoInvite: true     // Test-Bypass; in Produktion entfernen
    });

    if (!token?.ok && !token?.client_secret){
      const reason =
        (token && token.raw && (token.raw.error || token.raw.message)) ||
        (token && (token.error || token.message)) ||
        `status ${token?.status||'?'}`;
      console.warn('[Live] token missing/invalid. Raw:', token);
      setStatus(CTX.lang.startsWith('de')
        ? `Token-Fehler: ${String(reason)}`
        : `Token error: ${String(reason)}`, 'Error');
      setAvatarState('idle'); return;
    }

    muxUploadUrl = await getMuxUploadUrl({ uid: CTX.uid });

    await connectRealtime(token.client_secret, token.model || MODEL_DEFAULT);

    // Audio-Ausgabe versuchen / Gate falls blockiert
    const unlocked = await ensureAudioPlayback();
    if (!unlocked) showAudioGate();

    startRecording();

    // Session primen + erste Frage senden
    primeSession();
    sendGreeting(CTX);

    setAvatarState('speaking');
    setStatus(CTX.lang.startsWith('de')? 'Assistent spricht…':'Assistant speaking…','Live');
  }

  // ---------- Parent-Messaging ----------
  window.addEventListener('message', (ev)=>{
    const data = ev?.data || {};
    const t = data?.type || '';
    const pl = data?.data || {};

    if (t === 'clarity.live.context') {
      CTX = { ...CTX, ...pl };
      if (CTX.voice && !ALLOWED_VOICES.has(CTX.voice)) CTX.voice = 'verse';
    }
    if (t === 'clarity.live.start') {
      if (pl?.uid) CTX.uid = pl.uid;
      if (pl?.companyId) CTX.companyId = pl.companyId;
      if (pl?.lang) CTX.lang = (pl.lang||'de').toLowerCase();
      if (pl?.voice) CTX.voice = ALLOWED_VOICES.has(pl.voice) ? pl.voice : 'verse';
      (async () => { if (!mediaStream) await preflight(); else startInterview(); })();
    }
    if (t === 'clarity.live.hangup') {
      hangup();
    }
  });

  console.log('Token endpoint(s):', TOKEN_ENDPOINTS.join(', '), 'Mux endpoint(s):', MUX_ENDPOINTS.join(', '));
  setStatus('Bereit.', 'Idle'); setAvatarState('idle');
})();
</script>
</body>
</html>
Was sich konkret ändert / warum du jetzt Ton hörst:

PC.addTransceiver('audio', { direction:'recvonly' }) erzwingt einen Remote-Audiokanal.

ontrack erstellt sowohl Playback (nur Assistant zur Ausgabe → kein Echo) als auch einen Recording-Mix (Mic+Assistant) via WebAudio:

recordDest = audioCtx.createMediaStreamDestination()

micSource → micGain → recordDest

remoteSource → remoteGain → recordDest

remoteSource → (Gain) → audioCtx.destination (nur Assistant auf Lautsprecher)

mixedStream = new MediaStream([...videoTracks, recordDest.audio]) → der MediaRecorder nimmt beides auf.

Wenn der Browser dennoch Autoplay blockt, erscheint das Overlay „Audio einschalten“; ein Klick setzt play() durch.

Hinweis zum Mux-Fehler ERR_NAME_NOT_RESOLVED: das ist unabhängig vom Audio. Wenn dein Direct-Upload-Link echt ist, verschwindet das (es war bei dir schon „200 Object“ – vermutlich test-URL). Lass uns das nac
