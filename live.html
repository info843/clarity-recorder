<!-- public/live.html -->
<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <title>Clarity Live Interview</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    html,body{margin:0;padding:0;background:#0b0e12;color:#e5e7eb;font-family:system-ui,ui-sans-serif,Segoe UI,Roboto}
    .wrap{display:flex;flex-direction:column;gap:12px;max-width:920px;margin:0 auto;padding:16px}
    .row{display:flex;gap:12px;align-items:center}
    .videoBox{position:relative;width:100%;max-width:720px;aspect-ratio:16/9;background:#111827;border:1px solid #1f2937;border-radius:12px;overflow:hidden}
    video{width:100%;height:100%;object-fit:cover;background:#0f172a}
    .status{font-size:13px;opacity:.9}
    .meter{height:6px;background:#1f2937;border-radius:8px;overflow:hidden}
    .bar{height:100%;width:0%;background:#22c55e;transition:width .1s linear}
    .badge{font-size:12px;color:#9ca3af}
    .hidden{display:none}
  </style>
</head>
<body>
<div class="wrap">
  <div class="videoBox"><video id="local" autoplay playsinline muted></video></div>
  <div class="meter" aria-hidden="true"><div id="meterBar" class="bar"></div></div>
  <div class="row">
    <div id="status" class="status">Initialisiere…</div>
    <div id="badge" class="badge"></div>
  </div>
</div>

<script>
(() => {
  /* ------------------- Konfiguration ------------------- */
  const TOKEN_ENDPOINTS = [
    '/_functions/realtimeToken',      // bevorzugt: Wix http-function (Variante A)
    '/api/wix-token-proxy'           // Fallback, wenn vorhanden
  ];
  const MUX_ENDPOINTS = [
    '/_functions/muxDirectUpload',    // bevorzugt: Wix http-function
    '/api/mux-upload'                 // Fallback
  ];
  const MODEL_DEFAULT = 'gpt-4o-realtime-preview';
  const ALLOWED_VOICES = new Set(['alloy','ash','ballad','coral','echo','sage','shimmer','verse','marin','cedar']);
  const PREP_TRIGGER_TEXT_DE = 'ich bin bereit';
  const PREP_TRIGGER_TEXT_EN = 'i am ready';

  /* ------------------- State ------------------- */
  const qs = new URLSearchParams(location.search);
  const PARAMS = {
    uid: qs.get('uid') || '',
    companyId: qs.get('companyId') || '',
    lang: (qs.get('lang') || 'de').toLowerCase(),
    voice: (qs.get('voice') || 'verse').toLowerCase()
  };
  if (!ALLOWED_VOICES.has(PARAMS.voice)) PARAMS.voice = 'verse';

  let CTX = { companyName:'', position:'', lang:PARAMS.lang, uid:PARAMS.uid, companyId:PARAMS.companyId, voice:PARAMS.voice };
  let PC = null, DC = null, REMOTE_AUDIO = null;
  let mediaStream = null;          // Kamera+Mic
  let mediaRecorder = null;        // Aufnahme für Mux
  let recChunks = [];
  let muxUploadUrl = null;
  let started = false;
  let state = 'idle';
  let vadSpeaking = false;

  /* ------------------- UI ------------------- */
  const $video = document.getElementById('local');
  const $status = document.getElementById('status');
  const $badge = document.getElementById('badge');
  const $bar = document.getElementById('meterBar');

  function setStatus(text, badge='') {
    $status.textContent = text;
    $badge.textContent = badge ? `• ${badge}` : '';
  }
  function postToParent(obj){ try{ window.parent.postMessage(obj, '*'); }catch(_){} }
  function setAvatarState(st){
    postToParent({ type:'clarity.avatar', data:{ state: st } });
  }

  /* ------------------- Audio Meter / Beep ------------------- */
  let audioCtx, analyser, meterRAF;
  async function startLevelMeter(stream) {
    try{
      audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      const src = audioCtx.createMediaStreamSource(stream);
      analyser = audioCtx.createAnalyser();
      analyser.fftSize = 2048;
      src.connect(analyser);
      const data = new Uint8Array(analyser.frequencyBinCount);
      const tick = () => {
        analyser.getByteTimeDomainData(data);
        let maxDev = 0;
        for (let i=0;i<data.length;i++){
          const v = (data[i]-128)/128;
          maxDev = Math.max(maxDev, Math.abs(v));
        }
        const pct = Math.min(100, Math.round(maxDev * 180));
        $bar.style.width = pct + '%';
        meterRAF = requestAnimationFrame(tick);
      };
      meterRAF = requestAnimationFrame(tick);
    }catch(e){}
  }
  async function warmBeep(){
    try{
      const ctx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();
      const o = ctx.createOscillator(); const g = ctx.createGain();
      o.type = 'sine'; o.frequency.value = 880; g.gain.value = 0.001;
      o.connect(g).connect(ctx.destination);
      o.start(); setTimeout(()=>{ o.stop(); }, 150);
    }catch(_){}
  }

  /* ------------------- Preflight ------------------- */
  async function preflight(){
    setStatus(CTX.lang.startsWith('de') ? 'Gerätecheck…' : 'Running preflight…');
    try{
      mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: { echoCancellation:true, noiseSuppression:true, channelCount:1, sampleRate:48000 },
        video: CTX.mode === 'audio' ? false : { width: { ideal: 640 }, height: { ideal: 360 } }
      });
      $video.srcObject = mediaStream;
      await startLevelMeter(mediaStream);
      await warmBeep();
      setStatus(CTX.lang.startsWith('de') ? 'Sagen Sie „Ich bin bereit“.': 'Please say “I am ready”.', 'Preflight');
      state = 'preflight';

      // einfache Keyword-Detektion (lokal) über Web Speech API (Fallback: Timeout)
      try{
        const rec = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        rec.lang = CTX.lang.startsWith('de') ? 'de-DE' : 'en-US';
        rec.interimResults = true;
        rec.continuous = true;
        rec.onresult = (e) => {
          const t = Array.from(e.results).map(r=>r[0].transcript).join(' ').toLowerCase().trim();
          if (!t) return;
          // Triggerphrase
          if (t.includes(PREP_TRIGGER_TEXT_DE) || t.includes(PREP_TRIGGER_TEXT_EN)) {
            rec.stop();
            startInterview();
          }
        };
        rec.onerror = () => {}; // still
        rec.start();
        // Timeout als Safety (10s)
        setTimeout(()=>{ try{ rec.stop(); }catch(_){} startInterview(); }, 10000);
      }catch(_){
        // Kein SpeechRecognition verfügbar → Timeout
        setTimeout(startInterview, 3000);
      }
    }catch(e){
      setStatus(CTX.lang.startsWith('de') ? 'Kamera/Mikrofon fehlen.' : 'Camera/Microphone unavailable.', 'Error');
    }
  }

  /* ------------------- Token holen ------------------- */
  async function getRealtimeToken(payload){
    const body = JSON.stringify(payload);
    for (const url of TOKEN_ENDPOINTS){
      try{
        console.log('[Live] Fetching token…', url, body);
        const r = await fetch(url, { method:'POST', headers:{'Content-Type':'application/json'}, body });
        const txt = await r.text();
        let json=null; try{ json = txt ? JSON.parse(txt) : null; }catch(_){}
        console.log('[Live] Token response:', r.status, json||txt);
        if (json && json.ok && json.client_secret) return json;
      }catch(e){
        console.warn('[Live] token fetch failed', url, e);
      }
    }
    throw new Error('token_fetch_failed');
  }

  /* ------------------- Mux Direct Upload URL ------------------- */
  async function getMuxUploadUrl(payload){
    const body = JSON.stringify(payload);
    for (const url of MUX_ENDPOINTS){
      try{
        console.log('[Live] Request Mux upload…', url, body);
        const r = await fetch(url, { method:'POST', headers:{'Content-Type':'application/json'}, body });
        const txt = await r.text();
        let json=null; try{ json = txt ? JSON.parse(txt) : null; }catch(_){}
        console.log('[Live] Mux response:', r.status, json||txt);
        if (json && json.ok && json.upload && json.upload.url) return json.upload.url;
      }catch(e){
        console.warn('[Live] mux fetch failed', url, e);
      }
    }
    return null; // wir können ohne URL starten; Upload dann disabled
  }

  /* ------------------- WebRTC → OpenAI Realtime ------------------- */
  async function connectRealtime(clientSecret, model){
    if (!mediaStream) throw new Error('no_media');
    PC = new RTCPeerConnection();
    // Lokale Tracks anhängen
    mediaStream.getTracks().forEach(tr => PC.addTrack(tr, mediaStream));

    // DataChannel für Events
    DC = PC.createDataChannel('oai-events');
    DC.onopen = () => console.log('[RTC] DC open');
    DC.onmessage = onRealtimeEvent;

    // Remote-Audio empfangen
    PC.ontrack = (e) => {
      const [stream] = e.streams;
      if (!REMOTE_AUDIO) {
        REMOTE_AUDIO = new Audio();
        REMOTE_AUDIO.autoplay = true;
        REMOTE_AUDIO.playsInline = true;
      }
      REMOTE_AUDIO.srcObject = stream;
    };

    // Offer erzeugen
    const offer = await PC.createOffer({ offerToReceiveAudio: true, offerToReceiveVideo: false });
    await PC.setLocalDescription(offer);

    // SDP an OpenAI senden
    const url = `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model||MODEL_DEFAULT)}`;
    const ansResp = await fetch(url, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${clientSecret}`,
        'Content-Type': 'application/sdp'
      },
      body: offer.sdp
    });
    const answerSdp = await ansResp.text();
    await PC.setRemoteDescription({ type:'answer', sdp: answerSdp });

    console.log('[RTC] connected');
  }

  function onRealtimeEvent(ev){
    try{
      const msg = typeof ev.data === 'string' ? JSON.parse(ev.data) : ev.data;
      // Beispiele: conversation.item.delta / response.output_text.delta / ...
      const type = msg?.type || msg?.event || '';
      // einfache Transkript-Durchleitung:
      if (type.includes('delta') && msg?.delta) {
        postToParent({ type:'rt.transcript.partial', data:{ text: String(msg.delta) } });
      }
      if (type.includes('completed')) {
        postToParent({ type:'rt.transcript.final', data:{} });
      }
    }catch(e){
      // ignore
    }
  }

  /* ------------------- Aufnahme & Upload (Mux) ------------------- */
  function startRecording(){
    if (!mediaStream) return;
    try{
      recChunks = [];
      mediaRecorder = new MediaRecorder(mediaStream, { mimeType: 'video/webm;codecs=vp9,opus', videoBitsPerSecond: 800_000 });
      mediaRecorder.ondataavailable = (e)=>{ if (e.data?.size) recChunks.push(e.data); };
      mediaRecorder.onstop = async ()=>{
        if (!muxUploadUrl) { console.warn('[Mux] No upload URL, skipping.'); return; }
        try{
          const blob = new Blob(recChunks, { type: 'video/webm' });
          await fetch(muxUploadUrl, { method:'PUT', body: blob });
          postToParent({ type:'mux.upload.done', data:{ uploadId: muxUploadUrl.split('/').pop() || '' } });
        }catch(e){
          postToParent({ type:'mux.upload.error', data:{ message: String(e?.message||e) } });
        }
      };
      mediaRecorder.start(1000);
      console.log('[REC] started');
    }catch(e){
      console.warn('[REC] cannot start', e);
    }
  }
  function stopRecording(){
    try{ if (mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop(); }catch(_){}
  }

  /* ------------------- Interview Ablauf ------------------- */
  async function startInterview(){
    if (started) return;
    started = true;
    setAvatarState('speaking'); setStatus(CTX.lang.startsWith('de')? 'Starte Interview…':'Starting interview…', 'Live');

    // 1) Token holen
    const token = await getRealtimeToken({ uid: CTX.uid, companyId: CTX.companyId, lang: CTX.lang, voice: CTX.voice });
    if (!token || !token.ok || !token.client_secret) {
      setStatus(CTX.lang.startsWith('de')? 'Token-Fehler.':'Token error.', 'Error');
      setAvatarState('idle');
      return;
    }

    // 2) (Optional) Mux Direct Upload URL holen
    muxUploadUrl = await getMuxUploadUrl({ uid: CTX.uid });

    // 3) Realtime verbinden
    await connectRealtime(token.client_secret, token.model || MODEL_DEFAULT);

    // 4) Aufnahme starten (kein Pause, kein Resume)
    startRecording();

    // 5) minimaler Gesprächs-Flow (Server macht den Rest per Turn-Taking)
    setAvatarState('speaking');
    setStatus(CTX.lang.startsWith('de')? 'Assistent spricht…':'Assistant speaking…','Live');
    setTimeout(()=>{ setAvatarState('listening'); setStatus(CTX.lang.startsWith('de')? 'Bitte antworten.':'Please respond.','Live'); }, 1200);
  }

  /* ------------------- Parent Messaging ------------------- */
  window.addEventListener('message', (ev)=>{
    const data = ev?.data || {};
    const t = data?.type || '';
    const pl = data?.data || {};

    if (t === 'clarity.live.context') {
      CTX = { ...CTX, ...pl };
      if (CTX.voice && !ALLOWED_VOICES.has(CTX.voice)) CTX.voice = 'verse';
    }
    if (t === 'clarity.live.record') {
      // Nur „enabled: true/false“ wird akzeptiert; Pause nicht vorgesehen → ignorieren
    }
    if (t === 'clarity.live.start') {
      // UID/Company/lang/voice können hier nochmal übergeben werden
      if (pl?.uid) CTX.uid = pl.uid;
      if (pl?.companyId) CTX.companyId = pl.companyId;
      if (pl?.lang) CTX.lang = (pl.lang||'de').toLowerCase();
      if (pl?.voice) CTX.voice = ALLOWED_VOICES.has(pl.voice) ? pl.voice : 'verse';
      // Preflight, dann Start
      (async () => {
        if (!mediaStream) await preflight();  // preflight inkl. „Ich bin bereit“-Trigger
        else startInterview();
      })();
    }
    if (t === 'clarity.live.hangup') {
      stopRecording();
      setAvatarState('idle');
      setStatus(CTX.lang.startsWith('de')? 'Beendet.':'Ended.','');
      try { PC?.close?.(); } catch(_){}
    }
  });

  /* ------------------- Auto-Init (Badge / Konsolen-Info) ------------------- */
  console.log('Token endpoint(s):', TOKEN_ENDPOINTS.join(', '), 'Mux endpoint(s):', MUX_ENDPOINTS.join(', '));
  setStatus('Bereit.', 'Idle');
  setAvatarState('idle');
})();
</script>
</body>
</html>
