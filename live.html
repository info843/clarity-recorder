<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <title>Live Interview</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#0b0c10; --fg:#e5e7eb; --muted:#9ca3af; --ok:#10b981; --warn:#f59e0b; --err:#ef4444; --card:#111217; --line:#1f2330; }
    html,body{margin:0;padding:0;background:var(--bg);color:var(--fg);font:14px/1.45 system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif}
    .wrap{display:grid;grid-template-columns:1fr 320px;gap:16px;padding:14px;align-items:start}
    .card{background:var(--card);border:1px solid var(--line);border-radius:12px;padding:12px}
    .row{display:flex;gap:10px;align-items:center}
    .bar{height:6px;background:#0f172a;border-radius:999px;overflow:hidden}
    .bar > i{display:block;height:100%;width:0;background:#4f46e5;transition:width 50ms linear}
    .badge{display:inline-block;padding:2px 6px;border-radius:8px;background:#0f172a;border:1px solid #24314f;color:#cbd5e1;font-size:12px}
    .muted{color:var(--muted)}
    .btn{appearance:none;border:1px solid #2b3244;background:#141827;color:#f8fafc;border-radius:8px;padding:8px 10px;cursor:pointer}
    .btn:disabled{opacity:.5;cursor:not-allowed}
    video{width:100%;max-height:320px;background:#000;border-radius:8px}
    audio{width:100%}
    .grid2{display:grid;grid-template-columns:1fr 1fr;gap:10px}
    .log{font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px;color:#9ca3af;white-space:pre-wrap;max-height:220px;overflow:auto}
    .ghost{opacity:.65}
  </style>
</head>
<body>
  <div class="wrap">
    <section class="card">
      <div class="row" style="justify-content:space-between">
        <div>
          <div id="status" class="badge">Init…</div>
          <div id="audioBadge" class="badge" style="margin-left:6px">• (stumm?)</div>
        </div>
        <div class="row">
          <button id="btnStop" class="btn">Interview abbrechen</button>
        </div>
      </div>

      <div class="grid2" style="margin-top:12px">
        <div>
          <div class="muted" style="margin-bottom:6px">Kamera-Vorschau</div>
          <video id="localVideo" autoplay playsinline muted></video>
          <div class="muted" style="margin:8px 0 4px">Mic-Level</div>
          <div class="bar"><i id="micLevel"></i></div>
        </div>
        <div>
          <div class="muted" style="margin-bottom:6px">Assistent-Audio</div>
          <audio id="remoteAudio" controls></audio>
          <div class="muted" style="margin:8px 0 4px">System</div>
          <div class="bar"><i id="sysLevel"></i></div>
          <div class="muted" style="margin-top:10px">Hinweis: Bei blockiertem Autoplay ggf. in den Player klicken.</div>
        </div>
      </div>
    </section>

    <aside class="card">
      <div style="font-weight:600;margin-bottom:8px">Recorder / Upload</div>
      <div class="row">
        <div class="badge ghost" id="recState">idle</div>
        <div class="badge ghost" id="muxState" style="margin-left:6px">no upload</div>
      </div>
      <div class="muted" style="margin:8px 0 4px">Fortschritt</div>
      <div class="bar"><i id="recProgress"></i></div>
      <div class="muted" style="margin:10px 0 4px">Logs</div>
      <div id="log" class="log"></div>
    </aside>
  </div>

  <script>
  ;(()=>{"use strict";

  // ---- Single-Instance Guard (Wix lädt Scripts gern doppelt) ----
  if (window.__clarity_live_active) { console.warn('[LIVE] duplicate script ignored'); return; }
  window.__clarity_live_active = true;

  // -------------------- Query --------------------
  const Q = new URLSearchParams(location.search);
  const UID = (Q.get('uid')||'').trim();
  const COMPANY_ID = (Q.get('companyId')||'').trim();
  const LANG = (Q.get('lang')||'de').toLowerCase();
  const VOICE = (Q.get('voice')||'verse').toLowerCase();
  const AUTOSTART = Q.get('autostart') === '1';

  // -------------------- UI Els -------------------
  const $status = document.getElementById('status');
  const $badge  = document.getElementById('audioBadge');
  const $mic    = document.getElementById('micLevel');
  const $sys    = document.getElementById('sysLevel');
  const $log    = document.getElementById('log');
  const $localV = document.getElementById('localVideo');
  const $remoteA= document.getElementById('remoteAudio');
  const $btnStop= document.getElementById('btnStop');
  const $recState = document.getElementById('recState');
  const $muxState = document.getElementById('muxState');
  const $recProg  = document.getElementById('recProgress');

  function log(...a){
    try{
      console.log(...a);
      const s = a.map(x => typeof x==='object' ? JSON.stringify(x) : String(x)).join(' ');
      $log.textContent += s + "\n";
      $log.scrollTop = $log.scrollHeight;
    }catch(_){}
  }
  function setStatus(kind, text){
    $status.textContent = text || kind || '';
    $status.style.background = kind==='err' ? '#3b0d0d' : (kind==='ok' ? '#0d3b2a' : (kind==='warn' ? '#3b2f0d' : '#0f172a'));
    $status.style.borderColor = kind==='err' ? '#7f1d1d' : (kind==='ok' ? '#14532d' : (kind==='warn' ? '#7f6a1d' : '#24314f'));
  }

  // -------------------- Endpunkt -----------------
  const TOKEN_URL = 'https://www.clarity-nvl.com/_functions/realtimeToken';

  // -------------------- Global State -------------
  let pc, DC;
  let localStream, mixedStream, mediaRecorder;
  let remoteStream;
  let audioCtx, remoteSource;
  let destroyed = false;
  let started = false;

  // Parent-Handshake / Flags
  let PARENT_UPLOAD = false;   // ctx.uploadMode === 'parent'
  let RECORD_ENABLED = true;   // clarity.live.record.enabled
  let CTX = {};                // clarity.live.context payload
  let ENDED_SENT = false;      // einmalige ended-Message

  // Welcome/Start Gates
  const WELCOME_DELAY_MS = 300;
  const MIC_ENABLE_AFTER_WELCOME_MS = 2500;
  let waitingForWelcome = true;
  let FIRST_Q_SENT = false;

  // DC / readiness
  let dcIsOpen = false;
  let remotePlayable = false;
  let remoteTrackSeen = false;

  const dcQueue = [];
  function dcSend(obj){
    if (destroyed || !obj) return;
    try{
      if (dcIsOpen && DC && DC.readyState === 'open') {
        DC.send(JSON.stringify(obj));
      } else {
        dcQueue.push(obj);
        if (obj?.type) log('[DC] queued', obj.type);
      }
    }catch(e){ console.warn('[DC] send fail', e); log('[DC] send fail', e); }
  }
  function flushDC(){
    if (!dcIsOpen || !DC || DC.readyState!=='open') return;
    try{
      while (dcQueue.length){
        const msg = dcQueue.shift();
        DC.send(JSON.stringify(msg));
      }
      log('[DC] queue flushed');
    }catch(e){ console.warn('[DC] flush fail', e); }
  }
  function waitForDCOpen(timeout=8000){
    return new Promise(res=>{
      if (dcIsOpen) return res(true);
      const t0 = Date.now();
      const iv = setInterval(()=>{
        if (dcIsOpen) { clearInterval(iv); res(true); }
        else if (Date.now()-t0>timeout){ clearInterval(iv); res(false); }
      },50);
    });
  }
  function waitForRemotePlayable(timeout=8000){
    return new Promise(res=>{
      if (remotePlayable) return res(true);
      const t0 = Date.now();
      const iv = setInterval(()=>{
        if (remotePlayable) { clearInterval(iv); res(true); }
        else if (Date.now()-t0>timeout){ clearInterval(iv); res(false); }
      },50);
    });
  }

  // -------------------- Token --------------------
  async function getRealtimeToken(payload){
    log('[Live] Fetching token…', TOKEN_URL, JSON.stringify(payload));
    const r = await fetch(TOKEN_URL,{ method:'POST', headers:{'Content-Type':'application/json'}, body:JSON.stringify(payload) });
    if (!r.ok){
      const t = await r.text().catch(()=>r.statusText);
      throw new Error('token_http_'+r.status+': '+t);
    }
    const data = await r.json().catch(e=>{ throw new Error('token_json_parse: '+e); });
    const sec = extractClientSecret(data);
    if (!sec) throw new Error('client_secret_missing');
    log('[Live] effective', { lang:data?.reportLang||payload.lang, voice:data?.voice, model:data?.model });
    return { secret: sec, ctx: data };
  }
  function extractClientSecret(obj){
    if (!obj || typeof obj !== 'object') return null;
    if (typeof obj.client_secret === 'string') return obj.client_secret;
    if (obj.client_secret && typeof obj.client_secret.value === 'string') return obj.client_secret.value;
    if (obj.data && typeof obj.data.client_secret === 'string') return obj.data.client_secret;
    if (obj.token && typeof obj.token === 'string') return obj.token;
    if (obj.clientSecret && typeof obj.clientSecret === 'string') return obj.clientSecret;
    return null;
  }

  // -------------------- Realtime WebRTC ----------
  function onRealtimeEvent(ev){
    let data = ev && ev.data;
    try{ if (typeof data === 'string') data = JSON.parse(data); }catch(_){}
    if (!data) return;

    if (data.type === 'response.state' && data.state){
      parent?.postMessage?.({ type:'clarity.avatar', data:{ state:data.state } }, '*');
    }

    // Keyword-Start
    if (data.type === 'input_audio_transcription.completed'){
      const txt = (data?.transcription?.text || '').toLowerCase();
      if (/\bbereit\b|\bready\b|\bstart(en)?\b|\blas uns starten\b/.test(txt)) {
        maybeAskFirstQuestion();
      }
    }
  }

  async function connectRealtime(clientSecret){
    if (pc) return; // doppelte Handshakes verhindern
    pc = new RTCPeerConnection({ iceServers: [{ urls: ['stun:stun.l.google.com:19302'] }] });

    pc.onconnectionstatechange = () => {
      if (pc.connectionState === 'connected') log('[RTC] connected');
      else log('[RTC] connecting');
      if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected'){
        setStatus('err','Verbindung unterbrochen');
      }
    };

    pc.ontrack = async (ev) => {
      log('[RTC] ontrack kind=', ev.track.kind);
      if (!remoteStream) remoteStream = new MediaStream();
      remoteStream.addTrack(ev.track);
      $remoteA.srcObject = remoteStream;
      remoteTrackSeen = true;

      // Audio-Analyse
      try{
        audioCtx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();
        await audioCtx.resume().catch(()=>{});
        remoteSource = audioCtx.createMediaStreamSource(remoteStream);
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 512;
        const buf = new Uint8Array(analyser.frequencyBinCount);
        remoteSource.connect(analyser);
        (function tick(){
          if (destroyed) return;
          analyser.getByteTimeDomainData(buf);
          let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
          $sys.style.width = Math.min(100, dev*2) + '%';
          requestAnimationFrame(tick);
        })();
      }catch(_){}

      // Autoplay entsperren
      try {
        $remoteA.muted = false; $remoteA.volume = 1.0;
        $remoteA.setAttribute('autoplay',''); $remoteA.setAttribute('playsinline','');
        await $remoteA.play();
        remotePlayable = true;
        log('[AUDIO] remote audio playing (unlocked).');
      } catch(e) {
        remotePlayable = false;
        log('[AUDIO] autoplay blocked, waiting for user gesture.');
        setStatus('warn','Autoplay blockiert – bitte kurz in den Player klicken.');
      }

      // Badge „Audio in“
      try {
        const levelAnalyser = audioCtx.createAnalyser();
        levelAnalyser.fftSize = 1024;
        remoteSource.connect(levelAnalyser);
        const tbuf = new Uint8Array(levelAnalyser.frequencyBinCount);
        let lastNZ = 0;
        (function ping(){
          if (destroyed) return;
          levelAnalyser.getByteTimeDomainData(tbuf);
          let dev = 0;
          for (let i=0;i<tbuf.length;i++) dev = Math.max(dev, Math.abs(tbuf[i]-128));
          if (dev>2) lastNZ = performance.now();
          $badge.textContent = (performance.now()-lastNZ < 2000) ? '• Audio in' : '• (stumm?)';
          requestAnimationFrame(ping);
        })();
      }catch(_){}
    };

    // DataChannel
    DC = pc.createDataChannel('oai-events');
    DC.onopen = () => { dcIsOpen = true; log('[RTC] DC open'); flushDC(); };
    DC.onmessage = onRealtimeEvent;

    // Erst nur Empfang anbieten (Mic später)
    const offer = await pc.createOffer({ offerToReceiveAudio:true, offerToReceiveVideo:false });
    await pc.setLocalDescription(offer);

    const r = await fetch('https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview', {
      method: 'POST',
      headers: { 'Authorization': `Bearer ${clientSecret}`, 'Content-Type': 'application/sdp' },
      body: offer.sdp
    });
    if (!r.ok){
      const t = await r.text().catch(()=>r.statusText);
      throw new Error('realtime_handshake_failed: ' + t);
    }
    const answer = { type: 'answer', sdp: await r.text() };
    try{
      await pc.setRemoteDescription(answer);
    }catch(e){
      log('setRemoteDescription error', e);
      throw e;
    }
  }

  // -------------- System Prompt / Priming --------
  function buildSystemInstructions(ctx){
    const isDE = (String(ctx.reportLang||'de').toLowerCase().startsWith('de'));
    const langLine = isDE
      ? 'Sprich ausschließlich DEUTSCH. Wechsle zu keinem Zeitpunkt die Sprache.'
      : 'Speak strictly in ENGLISH. Do not switch languages at any time.';
    const structure = isDE
      ? 'Führe ein strukturiertes Job-Interview: Stelle genau EINE Frage zur Zeit, max. 1 Nachfrage, bleibe strikt beim Jobprofil.'
      : 'Conduct a structured job interview: ask exactly ONE question at a time, max 1 follow-up, stay strictly on the job profile.';
    const scope = isDE
      ? 'Antworte nur zu Rolle/Anzeige/Position. Für andere Themen verweise freundlich auf direkten Kontakt.'
      : 'Answer only about role/ad/position. For other topics, refer the candidate to direct contact.';
    return [langLine, structure, scope].join(' ');
  }
  function primeStrictLanguage(reportLang){
    const spoken = (reportLang === 'de') ? 'de' : 'en';
    // Sprache + Transkription + VAD=none (während Welcome)
    dcSend({
      type:'session.update',
      session:{
        voice: VOICE,
        instructions: buildSystemInstructions(CTX.reportLang ? CTX : { reportLang }),
        spoken_language: spoken,
        input_audio_transcription:{ model:'whisper-1', language: spoken },
        turn_detection:{ type:'none' },
        modalities:['audio'],
        input_audio_format:'pcm16',
        output_audio_format:'pcm16'
      }
    });
    log('[PRIME] session.update (lang+none VAD)', reportLang);
    clearConversation();
    createSystemMessage('Starte ein strukturiertes Interview. Stelle erst nach dem Signalwort „bereit“ die erste Frage.');
    cancelAllResponses();
  }
  function clearConversation(){ dcSend({ type:'conversation.clear' }); log('[CONV] cleared'); }
  function createSystemMessage(text){
    dcSend({ type:'conversation.item.create', item:{ type:'message', role:'system', content:[{type:'input_text', text}] }});
    log('[CONV] system item created');
  }
  function cancelAllResponses(){ dcSend({ type:'response.cancel' }); log('[CONV] responses cancelled'); }

  function speakPrepThenWelcome(){
    const isDE = String(CTX.reportLang||'de').toLowerCase().startsWith('de');
    const prep = isDE ? 'Einen Moment bitte, ich bereite das Interview vor …'
                      : 'One moment please, I am preparing the interview …';
    const welcome = isDE
      ? `Willkommen! Dieses Interview wird im Auftrag von ${CTX.companyName||'Clarity'} geführt. Sind Sie bereit zu starten? Bitte sagen Sie „bereit“, dann stelle ich die erste Frage.`
      : `Welcome! This interview is conducted on behalf of ${CTX.companyName||'Clarity'}. Are you ready to begin? Please say “ready”, then I will ask the first question.`;

    waitingForWelcome = true;

    dcSend({ type:'response.cancel' });
    log('[CONV] responses cancelled');

    // Vorbereitungs-Ansage
    dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: prep }});
    log('[WELCOME] prep spoken');

    // Welcome nach kurzer Pause
    setTimeout(()=>{
      dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: welcome }});
      log('[WELCOME] welcome spoken');

      // Nach dem Welcome: VAD aktivieren & Mic hochfahren
      setTimeout(()=>{
        dcSend({ type:'session.update', session:{ turn_detection:{ type:'server_vad', threshold:0.9, prefix_padding_ms:200, silence_duration_ms:800 } }});
        log('[PRIME] VAD enabled');
        enableMicUplink();
        waitingForWelcome = false;
        parent?.postMessage?.({ type:'clarity.live.prepared', data:{} }, '*');
      }, MIC_ENABLE_AFTER_WELCOME_MS);

    }, WELCOME_DELAY_MS);
  }

  function buildInterviewPlan(ctx){
    const isDE = String(ctx.reportLang||'de').toLowerCase().startsWith('de');
    const baseDE = [
      'Bitte stellen Sie sich kurz vor.',
      'Welche Erfahrung bringen Sie für die ausgeschriebene Position mit?',
      'Nennen Sie ein Projekt, auf das Sie besonders stolz sind – was war Ihre Rolle?',
      'Wie gehen Sie mit Zeitdruck und Prioritäten um?',
      'Warum möchten Sie bei uns arbeiten?'
    ];
    const baseEN = [
      'Please introduce yourself briefly.',
      'What experience do you bring for this position?',
      'Tell me about a project you are proud of – what was your role?',
      'How do you handle time pressure and priorities?',
      'Why do you want to work with us?'
    ];
    return isDE ? baseDE : baseEN;
  }

  function maybeAskFirstQuestion(){ if (!waitingForWelcome) askFirstQuestion(); }
  function askFirstQuestion(){
    if (FIRST_Q_SENT || waitingForWelcome) return;
    FIRST_Q_SENT = true;
    const plan = buildInterviewPlan(CTX);
    const isDE = String(CTX.reportLang||'de').toLowerCase().startsWith('de');
    const first = plan[0] || (isDE ? 'Bitte stellen Sie sich kurz vor.' : 'Please introduce yourself.');
    createSystemMessage('Starte jetzt die erste Interviewfrage. Kurze, präzise Fragen. Danach warten.');
    const q = (isDE ? `Erste Frage: ${first}` : `First question: ${first}`);
    dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: q }});
    log('[QUESTION] first sent');
  }

  // -------------------- Media / Recording --------
  async function setupLocalMedia(){
    try{
      localStream = await navigator.mediaDevices.getUserMedia({
        audio:{ echoCancellation:true, noiseSuppression:true, autoGainControl:true },
        video:true
      });
    } catch(e) {
      setStatus('err','Mic/Kamera blockiert. In Browser Freigabe erteilen.');
      log('[Media] getUserMedia error', e);
      // wir lassen den Flow weiterlaufen, damit Welcome trotzdem hörbar ist
    }
    if (localStream){
      $localV.srcObject = localStream;

      // Mic-Level
      try{
        const ctx = new (window.AudioContext || window.webkitAudioContext)();
        const src = ctx.createMediaStreamSource(localStream);
        const analyser = ctx.createAnalyser();
        analyser.fftSize = 512;
        const buf = new Uint8Array(analyser.frequencyBinCount);
        src.connect(analyser);
        (function tick(){
          if (destroyed) return;
          analyser.getByteTimeDomainData(buf);
          let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
          $mic.style.width = Math.min(100, dev*2) + '%';
          requestAnimationFrame(tick);
        })();
      }catch(_){}
    }
  }

  function enableMicUplink(){
    try{
      if (!pc || !localStream) return;
      // Nur jetzt den Audio-Track senden
      const a = localStream.getAudioTracks()[0];
      if (a) pc.addTrack(a, localStream);
      log('[MIC] uplink enabled');
    }catch(e){ log('[MIC] uplink error', e); }
  }

  async function ensureAudioUnlocked(){
    try{
      $remoteA.muted = false; $remoteA.volume = 1.0;
      $remoteA.setAttribute('autoplay',''); $remoteA.setAttribute('playsinline','');
      await $remoteA.play();
      remotePlayable = true;
      log('[AUDIO] remote audio playing (unlocked).');
      return true;
    }catch(_){
      remotePlayable = false;
      return false;
    }
  }

  async function blobToDataURL(blob){
    return new Promise((resolve,reject)=>{
      const reader = new FileReader();
      reader.onloadend = ()=> resolve(reader.result);
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  function startRecording(){
    if (!localStream){ log('[REC] cannot start – no localStream'); return; }

    // Mix remote + mic + video
    const ctx = new (window.AudioContext || window.webkitAudioContext)();
    const dest = ctx.createMediaStreamDestination();

    try{
      const micSrc = ctx.createMediaStreamSource(localStream);
      micSrc.connect(dest);
    }catch(e){ log('[REC] mic mix fail', e); }

    if (remoteStream){
      try{
        const remSrc = ctx.createMediaStreamSource(remoteStream);
        remSrc.connect(dest);
      }catch(e){ log('[REC] remote mix fail', e); }
    }

    mixedStream = new MediaStream();
    dest.stream.getAudioTracks().forEach(t => mixedStream.addTrack(t));
    const cam = localStream?.getVideoTracks?.()[0];
    if (cam) mixedStream.addTrack(cam);

    const hasVideo = !!mixedStream.getVideoTracks().length;
    mediaRecorder = new MediaRecorder(mixedStream, {
      mimeType: hasVideo ? 'video/webm;codecs=vp8,opus' : 'audio/webm;codecs=opus',
      videoBitsPerSecond: hasVideo ? 1_200_000 : undefined,
      audioBitsPerSecond: 128_000
    });
    const chunks = [];
    mediaRecorder.onstart = ()=>{ $recState.textContent='recording'; log('[REC] started'); };
    mediaRecorder.ondataavailable = (e)=>{ if (e.data && e.data.size>0) chunks.push(e.data); };
    mediaRecorder.onstop = async ()=>{
      $recState.textContent='stopped';
      const hasVid = !!mixedStream.getVideoTracks().length;
      const blob = new Blob(chunks, { type: hasVid ? 'video/webm' : 'audio/webm' });
      log('[REC] stop, size=', blob.size);

      try{
        $muxState.textContent='exporting';
        $recProg.style.width = '60%';
        const dataUrl = await blobToDataURL(blob);
        parent?.postMessage?.({ type:'recorder:export', data: { dataUrl, kind: (hasVid?'video':'audio'), bytes: blob.size } }, '*');
        $recProg.style.width = '100%';
        $muxState.textContent='export sent';
        sendEndedOnce();
      } catch(e){
        $muxState.textContent='export failed';
        log('[REC] export failed', e);
        parent?.postMessage?.({ type:'clarity.live.error', data:{ message: String(e?.message||e) } }, '*');
      } finally {
        chunks.length = 0;
      }
    };

    if (RECORD_ENABLED !== false) mediaRecorder.start();
  }

  function stopRecording(){
    try{ if (mediaRecorder && mediaRecorder.state!=='inactive') mediaRecorder.stop(); }catch(_){}
    try{ mixedStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
  }

  function sendEndedOnce(){
    if (ENDED_SENT) return;
    ENDED_SENT = true;
    parent?.postMessage?.({ type:'clarity.live.ended', data:{} }, '*');
  }

  // -------------------- Start / Stop Flow --------
  async function startInterview(){
    if (started) { log('[START] ignored (already started)'); return; }
    started = true;
    destroyed = false;
    ENDED_SENT = false;
    setStatus('info','Initialisiere…');

    await setupLocalMedia();

    const { secret, ctx } = await getRealtimeToken({ uid:UID, companyId:COMPANY_ID, lang:LANG, voice:VOICE, debug:true, allowNoInvite:true });

    await connectRealtime(secret);

    // Warten bis DC offen UND remote Audio spielbar → erst dann primen & Welcome sprechen
    await waitForDCOpen();
    await ensureAudioUnlocked();         // einmal aktiv versuchen
    await waitForRemotePlayable();       // falls Autoplay erst später freigegeben wird

    const usedLang = (CTX.reportLang || ctx.reportLang || LANG);
    CTX.reportLang = usedLang;
    CTX.companyName = ctx.companyName || CTX.companyName || 'Clarity';

    primeStrictLanguage(usedLang);
    speakPrepThenWelcome();

    setStatus('ok','Verbunden • Interview läuft');
  }

  async function hangup(){
    try{
      destroyed = true;
      started = false;
      FIRST_Q_SENT = false;
      waitingForWelcome = true;

      try{ DC?.close(); }catch(_){}
      try{ pc?.getSenders()?.forEach(s=>s.track && s.track.stop()); }catch(_){}
      try{ pc?.getReceivers()?.forEach(r=>r.track && r.track.stop()); }catch(_){}
      try{ pc?.close(); }catch(_){}
      pc = undefined; DC = undefined; dcIsOpen = false;

      try{ localStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
      try{ remoteStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
      localStream = undefined; remoteStream = undefined;
      if ($remoteA) $remoteA.srcObject = null;
      if ($localV) $localV.srcObject = null;

      stopRecording();

      // AudioContext schließen
      try{ audioCtx?.close?.(); }catch(_){}
      audioCtx = undefined; remoteSource = undefined;

      setStatus('warn','Beendet');
      log('[Live] hangup done');
      sendEndedOnce();
    }catch(e){
      log('[Live] hangup error', e);
    }
  }

  // -------------------- Parent-Messaging ---------
  window.addEventListener('message', (ev)=>{
    const msg = ev.data || {};
    const type = msg.type;

    if (type === 'clarity.live.context') {
      CTX = msg.data || {};
      PARENT_UPLOAD  = (CTX.uploadMode === 'parent');
      log('[CTX]', CTX);
    }
    else if (type === 'clarity.live.record') {
      RECORD_ENABLED = !!(msg.data && msg.data.enabled);
      log('[REC] record flag from parent:', RECORD_ENABLED);
    }
    else if (type === 'clarity.live.start') {
      window.__clarity_parent_started = true;
      startInterview().catch(e => {
        setStatus('err', 'Startfehler: ' + (e?.message||e));
        log('[Start error]', e);
        parent?.postMessage?.({ type:'clarity.live.error', data:{ message:String(e?.message||e) } }, '*');
      });
    }
    else if (type === 'clarity.live.stop' || type === 'clarity.live.hangup') {
      hangup();
    }
    else if (type === 'recorder:export') {
      stopRecording();
    }
    else if (type === 'clarity.live.prepared.confirm' || type === 'clarity.live.prepared') {
      if (RECORD_ENABLED) startRecording();
    }
  });

  // Autostart fallback (nur wenn Parent NICHT startet)
  setTimeout(()=>{
    if (!UID || !COMPANY_ID) return;
    if (!window.__clarity_parent_started && AUTOSTART && !started){
      log('[AUTO] no parent start → autostart');
      startInterview().catch(e=>{ setStatus('err','Autostart fehlgeschlagen'); log(e); });
    }
  }, 800);

  document.getElementById('btnStop').addEventListener('click', hangup);

  // Hinweis: iframe im Parent braucht allow="microphone; camera; autoplay"
  })();
  </script>
</body>
</html>
