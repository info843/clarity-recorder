<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <title>Live Interview</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#0b0c10; --fg:#e5e7eb; --muted:#9ca3af; --ok:#10b981; --warn:#f59e0b; --err:#ef4444; --card:#111217; --line:#1f2330; }
    html,body{margin:0;padding:0;background:var(--bg);color:var(--fg);font:14px/1.45 system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif}
    .wrap{display:grid;grid-template-columns:1fr 320px;gap:16px;padding:14px;align-items:start}
    .card{background:var(--card);border:1px solid var(--line);border-radius:12px;padding:12px}
    .row{display:flex;gap:10px;align-items:center}
    .bar{height:6px;background:#0f172a;border-radius:999px;overflow:hidden}
    .bar > i{display:block;height:100%;width:0;background:#4f46e5;transition:width 50ms linear}
    .badge{display:inline-block;padding:2px 6px;border-radius:8px;background:#0f172a;border:1px solid #24314f;color:#cbd5e1;font-size:12px}
    .muted{color:var(--muted)}
    .btn{appearance:none;border:1px solid #2b3244;background:#141827;color:#f8fafc;border-radius:8px;padding:8px 10px;cursor:pointer}
    .btn:disabled{opacity:.5;cursor:not-allowed}
    video{width:100%;max-height:320px;background:#000;border-radius:8px}
    audio{width:100%}
    .grid2{display:grid;grid-template-columns:1fr 1fr;gap:10px}
    .log{font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px;color:#9ca3af;white-space:pre-wrap;max-height:220px;overflow:auto}
    .ghost{opacity:.65}
  </style>
</head>
<body>
  <div class="wrap">
    <section class="card">
      <div class="row" style="justify-content:space-between">
        <div>
          <div id="status" class="badge">Init…</div>
          <div id="audioBadge" class="badge" style="margin-left:6px">• (stumm?)</div>
        </div>
        <div class="row">
          <button id="btnStop" class="btn">Interview abbrechen</button>
        </div>
      </div>

      <div class="grid2" style="margin-top:12px">
        <div>
          <div class="muted" style="margin-bottom:6px">Kamera-Vorschau</div>
          <video id="localVideo" autoplay playsinline muted></video>
          <div class="muted" style="margin:8px 0 4px">Mic-Level</div>
          <div class="bar"><i id="micLevel"></i></div>
        </div>
        <div>
          <div class="muted" style="margin-bottom:6px">Assistent-Audio</div>
          <audio id="remoteAudio" controls></audio>
          <div class="muted" style="margin:8px 0 4px">System</div>
          <div class="bar"><i id="sysLevel"></i></div>
          <div class="muted" style="margin-top:10px">Hinweis: Bei blockiertem Autoplay ggf. in den Player klicken.</div>
        </div>
      </div>
    </section>

    <aside class="card">
      <div style="font-weight:600;margin-bottom:8px">Recorder / Upload</div>
      <div class="row">
        <div class="badge ghost" id="recState">idle</div>
        <div class="badge ghost" id="muxState" style="margin-left:6px">no export</div>
      </div>
      <div class="muted" style="margin:8px 0 4px">Fortschritt</div>
      <div class="bar"><i id="recProgress"></i></div>
      <div class="muted" style="margin:10px 0 4px">Logs</div>
      <div id="log" class="log"></div>
    </aside>
  </div>

  <script>
  /* ===== Query / UI ===== */
  const Q = new URLSearchParams(location.search);
  const UID = (Q.get('uid')||'').trim();
  const COMPANY_ID = (Q.get('companyId')||'').trim();
  const LANG = (Q.get('lang')||'de').toLowerCase();
  const VOICE = (Q.get('voice')||'verse').toLowerCase();
  const AUTOSTART = Q.get('autostart') !== '0';

  const $status = document.getElementById('status');
  const $badge  = document.getElementById('audioBadge');
  const $mic    = document.getElementById('micLevel');
  const $sys    = document.getElementById('sysLevel');
  const $log    = document.getElementById('log');
  const $localV = document.getElementById('localVideo');
  const $remoteA= document.getElementById('remoteAudio');

  const $recState = document.getElementById('recState');
  const $muxState = document.getElementById('muxState');
  const $recProg  = document.getElementById('recProgress');

  function log(...a){
    console.log(...a);
    const s = a.map(x => typeof x==='object' ? JSON.stringify(x) : String(x)).join(' ');
    $log.textContent += s + "\n";
    $log.scrollTop = $log.scrollHeight;
    try{ parent?.postMessage?.({type:'clarity.live.log', data:s}, '*'); }catch(_){}
  }
  function setStatus(kind, text){
    $status.textContent = text || kind || '';
    $status.style.background = kind==='err' ? '#3b0d0d' : (kind==='ok' ? '#0d3b2a' : (kind==='warn' ? '#3b2f0d' : '#0f172a'));
    $status.style.borderColor = kind==='err' ? '#7f1d1d' : (kind==='ok' ? '#14532d' : (kind==='warn' ? '#7f6a1d' : '#24314f'));
  }

  /* ===== Endpunkt ===== */
  const TOKEN_URL = 'https://www.clarity-nvl.com/_functions/realtimeToken';

  /* ===== State (einmalig deklariert) ===== */
  let pc, DC;
  let dcIsOpen = false;
  const dcQueue = [];

  let localStream, remoteStream;
  let mixedStream, mediaRecorder;        // <— HIER EINMALIG
  let audioCtx, remoteSource;

  let destroyed = false;

  let PARENT_UPLOAD = true;
  let RECORD_ENABLED = false;
  let CTX = {};

  let FIRST_Q_SENT = false;
  let READY_DETECTED = false;

  let STARTED = false;
  let audioUnlocked = false;

  let remoteTrackReadyResolve;
  const remoteTrackReady = new Promise(r => remoteTrackReadyResolve = r);

  /* ===== DC Queue ===== */
  function dcSend(obj){
    try{
      if (dcIsOpen && DC?.readyState === 'open'){
        DC.send(JSON.stringify(obj));
      } else {
        dcQueue.push(obj);
        log('[DC] queued', (obj && (obj.type || Object.keys(obj)[0])) || '');
      }
    }catch(e){ log('[DC] send fail', e); }
  }
  function flushDCQueue(){
    if (!dcIsOpen || !DC) return;
    try{
      while (dcQueue.length){
        const m = dcQueue.shift();
        DC.send(JSON.stringify(m));
      }
      log('[DC] queue flushed');
    }catch(e){ log('[DC] flush error', e); }
  }

  /* ===== Token ===== */
  async function getRealtimeToken(payload){
    log('[Live] Fetching token…', TOKEN_URL, JSON.stringify(payload));
    const r = await fetch(TOKEN_URL,{ method:'POST', headers:{'Content-Type':'application/json'}, body:JSON.stringify(payload) });
    if (!r.ok){ const t=await r.text().catch(()=>r.statusText); throw new Error('token_http_'+r.status+': '+t); }
    const data = await r.json().catch(e=>{ throw new Error('token_json_parse: '+e); });
    if (!data?.ok) throw new Error('token_error: '+(data?.error||'unknown')+' '+(data?.message||'')); 
    const sec = data.client_secret || data?.clientSecret || data?.token || (data?.client_secret?.value);
    if (!sec) throw new Error('client_secret_missing');
    log('[Live] effective', { lang:data.reportLang||payload.lang, voice:data.voice, model:data.model });
    return { secret: sec, ctx: data };
  }

  /* ===== WebRTC ===== */
  async function connectRealtime(clientSecret){
    pc = new RTCPeerConnection({ iceServers: [{ urls: ['stun:stun.l.google.com:19302'] }] });

    pc.onconnectionstatechange = () => {
      log('[RTC]', pc.connectionState);
      if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected'){
        setStatus('err','Verbindung unterbrochen');
      }
    };

    pc.ontrack = async (ev) => {
      log('[RTC] ontrack kind=', ev.track.kind);
      if (!remoteStream) remoteStream = new MediaStream();
      remoteStream.addTrack(ev.track);
      $remoteA.srcObject = remoteStream;

      // System meter
      try{
        audioCtx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();
        remoteSource = audioCtx.createMediaStreamSource(remoteStream);
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 512;
        const buf = new Uint8Array(analyser.frequencyBinCount);
        remoteSource.connect(analyser);
        (function tick(){
          if (destroyed) return;
          analyser.getByteTimeDomainData(buf);
          let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
          $sys.style.width = Math.min(100, dev*2) + '%';
          requestAnimationFrame(tick);
        })();
      }catch(_){}

      // versuchen zu spielen (kann wegen Autoplay geblockt sein)
      try{ $remoteA.muted=false; $remoteA.volume=1.0; $remoteA.autoplay=true; $remoteA.playsInline=true; await $remoteA.play(); audioUnlocked=true; parent?.postMessage?.({type:'clarity.live.audio_ready'}, '*'); log('[AUDIO] remote audio playing (unlocked).'); }
      catch(e){ audioUnlocked=false; parent?.postMessage?.({type:'clarity.live.audio_wait'}, '*'); log('[AUDIO] autoplay blocked, waiting for user gesture.'); }

      remoteTrackReadyResolve?.();
    };

    // lokale Tracks (Mic erstmal stumm)
    if (localStream){
      localStream.getAudioTracks().forEach(t => { t.enabled=false; pc.addTrack(t, localStream); });
      localStream.getVideoTracks().forEach(t => pc.addTrack(t, localStream));
    }

    DC = pc.createDataChannel('oai-events');
    DC.onopen = () => { dcIsOpen = true; log('[RTC] DC open'); flushDCQueue(); };
    DC.onmessage = onRealtimeEvent;

    const offer = await pc.createOffer({ offerToReceiveAudio:true, offerToReceiveVideo:false });
    await pc.setLocalDescription(offer);
    const r = await fetch('https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview', {
      method: 'POST',
      headers: { 'Authorization': `Bearer ${clientSecret}`, 'Content-Type': 'application/sdp' },
      body: offer.sdp
    });
    if (!r.ok){ const t=await r.text().catch(()=>r.statusText); throw new Error('realtime_handshake_failed: '+t); }
    const answer = { type:'answer', sdp: await r.text() };
    await pc.setRemoteDescription(answer);
  }

  function onRealtimeEvent(ev){
    let data = ev && ev.data;
    try{ if (typeof data === 'string') data = JSON.parse(data); }catch(_){}
    if (!data) return;

    if (data.type === 'response.state' && data.state){
      parent?.postMessage?.({ type:'clarity.avatar', data:{ state:data.state } }, '*');
    }
    if (data.type === 'input_audio_buffer.speech_started'){
      maybeAskFirstQuestion();
    }
    if (data.type === 'input_audio_transcription.completed'){
      const txt = (data?.transcription?.text || '').toLowerCase();
      if (/(^|\b)(bereit|ready)(\b|$)/.test(txt)){ maybeAskFirstQuestion(); }
    }
  }

  /* ===== Audio Unlock Gate ===== */
  async function ensureAudioUnlocked(){
    if (audioUnlocked) return true;
    try{ await $remoteA.play(); audioUnlocked=true; parent?.postMessage?.({type:'clarity.live.audio_ready'}, '*'); log('[AUDIO] unlocked via play()'); return true; }catch(_){}
    parent?.postMessage?.({type:'clarity.live.audio_wait'}, '*');
    setStatus('warn','Bitte einmal hier klicken, um Audio zu aktivieren.');
    return new Promise((resolve)=>{
      const unlock = async ()=>{
        try{
          if (audioCtx && audioCtx.state!=='running'){ try{ await audioCtx.resume(); }catch(_){} }
          await $remoteA.play();
          audioUnlocked = true;
          parent?.postMessage?.({type:'clarity.live.audio_ready'}, '*');
          log('[AUDIO] unlocked by user gesture');
          window.removeEventListener('pointerdown', unlock);
          window.removeEventListener('keydown', unlock);
          resolve(true);
        }catch(_){}
      };
      window.addEventListener('pointerdown', unlock, { once:true });
      window.addEventListener('keydown', unlock, { once:true });
    });
  }

  /* ===== Priming & Welcome ===== */
  function buildSystemInstructions(ctx){
    const isDE = String(ctx.reportLang||'de').toLowerCase().startsWith('de');
    const langLine = isDE
      ? 'Sprich ausschließlich DEUTSCH. Wechsle nie die Sprache.'
      : 'Speak strictly in ENGLISH. Do not switch languages.';
    const structure = isDE
      ? 'Führe ein strukturiertes Job-Interview: nur EINE Frage zur Zeit, max. 1 Nachfrage, strikt am Jobprofil bleiben.'
      : 'Conduct a structured job interview: one question at a time, max one follow-up, stick to the job profile.';
    const waiting = isDE
      ? 'Nach jeder Frage warten, bis die Kandidatin/der Kandidat fertig ist.'
      : 'After each question, wait until the candidate is finished.';
    return [langLine, structure, waiting].join(' ');
  }

  function primeStrictLanguage(reportLang){
    const spoken = String(reportLang||'de').toLowerCase().startsWith('de') ? 'de' : 'en';
    const sys = buildSystemInstructions(CTX);

    dcSend({ type:'session.update', session:{
      voice: VOICE,
      instructions: sys,
      spoken_language: spoken,
      input_audio_format:'pcm16',
      output_audio_format:'pcm16',
      input_audio_transcription:{ model:'whisper-1', language: spoken },
      turn_detection:{ type:'none' },
      modalities:['audio']
    }});
    log('[PRIME] session.update (lang+none VAD)', spoken);

    dcSend({ type:'conversation.clear' }); log('[CONV] cleared');
    dcSend({ type:'conversation.item.create', item:{ type:'message', role:'system', content:[{type:'input_text', text:sys}] }});
    log('[CONV] system item created');
  }

  function speakPrepThenWelcome(){
    const isDE = String(CTX.reportLang||'de').toLowerCase().startsWith('de');
    const prep = isDE ? 'Einen Moment bitte, ich bereite das Interview vor …'
                      : 'One moment please, I am preparing the interview …';
    const welcome = isDE
      ? `Willkommen! Dieses Interview wird im Auftrag von ${CTX.companyName||'Clarity'} geführt. Sind Sie bereit zu starten? Bitte sagen Sie „bereit“, dann stelle ich die erste Frage.`
      : `Welcome! This interview is conducted on behalf of ${CTX.companyName||'Clarity'}. Are you ready to begin? Please say “ready”, then I will ask the first question.`;

    dcSend({ type:'response.cancel' });           log('[CONV] responses cancelled');
    dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: prep }});     log('[WELCOME] prep spoken');
    setTimeout(()=>{
      dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: welcome }});log('[WELCOME] welcome spoken');

      dcSend({ type:'session.update', session:{ turn_detection:{ type:'server_vad', threshold:0.9, prefix_padding_ms:200, silence_duration_ms:700 } }});
      log('[PRIME] VAD enabled');

      enableMicUplink();
      parent?.postMessage?.({ type:'clarity.live.prepared', data:{} }, '*');
    }, 900);
  }

  function buildInterviewPlan(ctx){
    const isDE = String(ctx.reportLang||'de').toLowerCase().startsWith('de');
    const baseDE = [
      'Bitte stellen Sie sich kurz vor.',
      'Warum passt diese Position zu Ihnen?',
      'Beschreiben Sie ein Projekt, auf das Sie stolz sind.',
      'Wie gehen Sie mit knappen Deadlines um?',
      'Welche Erwartungen haben Sie an die Rolle und das Team?'
    ];
    const baseEN = [
      'Please introduce yourself briefly.',
      'Why is this position a good fit for you?',
      'Describe a project you are proud of.',
      'How do you handle tight deadlines?',
      'What are your expectations for the role and the team?'
    ];
    const seeds = (isDE? baseDE : baseEN).slice();
    const text = (ctx.roleProfileText||'').trim() + '\n' + (ctx.docsText||'').trim();
    const lines = text.split(/\r?\n/).map(s=>s.trim()).filter(Boolean).slice(0,5);
    if (lines.length){ lines.forEach(l => seeds.push((isDE? 'Bezug zur Ausschreibung: ' : 'Regarding the job ad: ')+l)); }
    return seeds.slice(0, ctx.interviewConfig?.maxQuestions || 5);
  }

  function askFirstQuestion(){
    if (FIRST_Q_SENT) return;
    FIRST_Q_SENT = true;

    const plan = buildInterviewPlan(CTX);
    const isDE = String(CTX.reportLang||'de').toLowerCase().startsWith('de');
    const first = plan[0] || (isDE ? 'Bitte stellen Sie sich kurz vor.' : 'Please introduce yourself.');
    dcSend({ type:'conversation.item.create', item:{ type:'message', role:'system', content:[{type:'input_text', text:'Starte jetzt die erste Interviewfrage. Kurze, präzise Fragen. Danach warten.'}] }});
    log('[CONV] system item created');
    const q = (isDE ? `Erste Frage: ${first}` : `First question: ${first}`);
    dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: q }}); log('[QUESTION] first sent');
  }
  function maybeAskFirstQuestion(){ if (!FIRST_Q_SENT) askFirstQuestion(); }

  /* ===== Media / Recorder ===== */
  async function setupLocalMedia(){
    try{
      localStream = await navigator.mediaDevices.getUserMedia({
        audio:{ echoCancellation:true, noiseSuppression:true, autoGainControl:true },
        video:true
      });
    } catch(e) {
      setStatus('warn','Mic/Kamera blockiert (weiter ohne Mic).');
      log('[Media] getUserMedia error', e);
      try{ parent?.postMessage?.({ type:'clarity.live.permission_error', data:{ message:String(e?.name||e) } }, '*'); }catch(_){}
      localStream = null;
    }
    if (localStream){
      $localV.srcObject = localStream;
      try{
        const ctx = new (window.AudioContext || window.webkitAudioContext)();
        const src = ctx.createMediaStreamSource(localStream);
        const analyser = ctx.createAnalyser();
        analyser.fftSize = 512;
        const buf = new Uint8Array(analyser.frequencyBinCount);
        src.connect(analyser);
        (function tick(){
          if (destroyed) return;
          analyser.getByteTimeDomainData(buf);
          let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
          $mic.style.width = Math.min(100, dev*2) + '%';
          requestAnimationFrame(tick);
        })();
      }catch(_){}
      localStream.getAudioTracks().forEach(t => t.enabled=false);
    }
  }
  function enableMicUplink(){ try{ localStream?.getAudioTracks()?.forEach(t => t.enabled=true); log('[MIC] uplink enabled'); }catch(_){} }

  async function blobToDataURL(blob){
    return new Promise((resolve,reject)=>{ const r=new FileReader(); r.onloadend=()=>resolve(r.result); r.onerror=reject; r.readAsDataURL(blob); });
  }

  function prepareRecording(){
    const ctx = new (window.AudioContext || window.webkitAudioContext)();
    const dest = ctx.createMediaStreamDestination();
    if (localStream){ ctx.createMediaStreamSource(localStream).connect(dest); }
    if (remoteStream){ try{ ctx.createMediaStreamSource(remoteStream).connect(dest); }catch(e){ log('[REC] remote mix fail', e); } }
    mixedStream = new MediaStream();
    dest.stream.getAudioTracks().forEach(t => mixedStream.addTrack(t));
    const cam = localStream?.getVideoTracks?.()[0];
    if (cam) mixedStream.addTrack(cam);

    const hasVideo = !!mixedStream.getVideoTracks().length;
    mediaRecorder = new MediaRecorder(mixedStream, {
      mimeType: hasVideo ? 'video/webm;codecs=vp8,opus' : 'audio/webm;codecs=opus',
      videoBitsPerSecond: hasVideo ? 1_200_000 : undefined,
      audioBitsPerSecond: 128_000
    });
    const chunks = [];
    mediaRecorder.onstart = ()=>{ $recState.textContent='recording'; log('[REC] started'); };
    mediaRecorder.ondataavailable = (e)=>{ if (e.data && e.data.size>0) chunks.push(e.data); };
    mediaRecorder.onstop = async ()=>{
      $recState.textContent='stopped';
      const hasVid = !!mixedStream.getVideoTracks().length;
      const blob = new Blob(chunks, { type: hasVid ? 'video/webm' : 'audio/webm' });
      log('[REC] stop, size=', blob.size);
      try{
        $muxState.textContent='exporting'; $recProg.style.width='60%';
        const dataUrl = await blobToDataURL(blob);
        parent?.postMessage?.({ type:'recorder:export', data:{ dataUrl, kind:(hasVid?'video':'audio'), bytes: blob.size } }, '*');
        $recProg.style.width='100%'; $muxState.textContent='export sent';
      }catch(e){ $muxState.textContent='export failed'; log('[REC] export failed', e); }
      finally{ chunks.length=0; }
    };
    $recState.textContent='ready';
  }
  function startRecorder(){ try{ if (mediaRecorder && mediaRecorder.state==='inactive') mediaRecorder.start(); }catch(e){ log('[REC] start error', e); } }
  function stopRecording(){ try{ if (mediaRecorder && mediaRecorder.state!=='inactive') mediaRecorder.stop(); }catch(_){}
    try{ mixedStream?.getTracks()?.forEach(t=>t.stop()); }catch(_){} }

  /* ===== Start / Stop ===== */
  async function startInterview(){
    if (STARTED) return;
    STARTED = true;

    try{
      destroyed = false;
      setStatus('info','Initialisiere…');

      await setupLocalMedia();

      const { secret, ctx } = await getRealtimeToken({ uid:UID, companyId:COMPANY_ID, lang:LANG, voice:VOICE, debug:true, allowNoInvite:true });
      await connectRealtime(secret);
      prepareRecording();

      // Kontext
      CTX.reportLang = (CTX.reportLang || ctx.reportLang || LANG);
      CTX.companyName = ctx.companyName || CTX.companyName;

      // Welcome erst NACH Remote-Track & Audio-Unlock
      await remoteTrackReady;
      await ensureAudioUnlocked();

      primeStrictLanguage(CTX.reportLang);
      speakPrepThenWelcome();

      setStatus('ok','Verbunden • Interview läuft');
    }catch(e){
      setStatus('err','Startfehler: '+(e?.message||e));
      log('[Start error]', e);
      parent?.postMessage?.({ type:'clarity.live.error', data:{ message:String(e?.message||e) } }, '*');
    }
  }

  async function hangup(){
    destroyed = true;
    try{ DC?.close(); }catch(_){}
    try{ pc?.getSenders()?.forEach(s=>s.track && s.track.stop()); }catch(_){}
    try{ pc?.getReceivers()?.forEach(r=>r.track && r.track.stop()); }catch(_){}
    try{ pc?.close(); }catch(_){}
    try{ localStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
    try{ remoteStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
    stopRecording();
    setStatus('warn','Beendet');
    log('[Live] hangup done');
    parent?.postMessage?.({ type:'clarity.live.ended', data:{} }, '*');
  }

  /* ===== Parent Messaging ===== */
  window.addEventListener('message', (ev)=>{
    const msg = ev.data || {};
    const type = msg.type;

    if (type === 'clarity.live.context'){
      CTX = msg.data || {};
      PARENT_UPLOAD = (CTX.uploadMode === 'parent');
      log('[CTX]', CTX);
    }
    else if (type === 'clarity.live.record'){
      RECORD_ENABLED = !!(msg.data && msg.data.enabled);
      log('[REC] record flag from parent:', RECORD_ENABLED);
      if (RECORD_ENABLED) startRecorder();
    }
    else if (type === 'clarity.live.start'){
      startInterview();
    }
    else if (type === 'clarity.live.stop' || type === 'clarity.live.hangup'){
      hangup();
    }
    else if (type === 'recorder:export'){
      stopRecording();
    }
  });

  window.addEventListener('load', ()=>{
    try{ parent?.postMessage?.({ type:'clarity.live.hello', data:{ uid:UID, companyId:COMPANY_ID } }, '*'); }catch(_){}
    if (AUTOSTART){ setTimeout(()=>{ if (!STARTED) { log('[AUTO] no parent start → autostart'); startInterview(); } }, 1500); }
  });

  document.getElementById('btnStop').addEventListener('click', hangup);

  // WICHTIG im Parent-iframe: allow="microphone; camera; autoplay"
  </script>
</body>
</html>
