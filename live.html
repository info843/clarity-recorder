<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <title>Live Interview</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#0b0c10; --fg:#e5e7eb; --muted:#9ca3af; --ok:#10b981; --warn:#f59e0b; --err:#ef4444; --card:#111217; --line:#1f2330; }
    html,body{margin:0;padding:0;background:var(--bg);color:var(--fg);font:14px/1.45 system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif}
    .wrap{display:grid;grid-template-columns:1fr 320px;gap:16px;padding:14px;align-items:start}
    .card{background:var(--card);border:1px solid var(--line);border-radius:12px;padding:12px}
    .row{display:flex;gap:10px;align-items:center}
    .bar{height:6px;background:#0f172a;border-radius:999px;overflow:hidden}
    .bar > i{display:block;height:100%;width:0;background:#4f46e5;transition:width 50ms linear}
    .badge{display:inline-block;padding:2px 6px;border-radius:8px;background:#0f172a;border:1px solid #24314f;color:#cbd5e1;font-size:12px}
    .muted{color:var(--muted)}
    .btn{appearance:none;border:1px solid #2b3244;background:#141827;color:#f8fafc;border-radius:8px;padding:8px 10px;cursor:pointer}
    .btn:disabled{opacity:.5;cursor:not-allowed}
    video{width:100%;max-height:320px;background:#000;border-radius:8px}
    audio{width:100%}
    .grid2{display:grid;grid-template-columns:1fr 1fr;gap:10px}
    .log{font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px;color:#9ca3af;white-space:pre-wrap;max-height:220px;overflow:auto}
    .ghost{opacity:.65}
  </style>
</head>
<body>
  <div class="wrap">
    <section class="card">
      <div class="row" style="justify-content:space-between">
        <div>
          <div id="status" class="badge">Init…</div>
          <div id="audioBadge" class="badge" style="margin-left:6px">• (stumm?)</div>
        </div>
        <div class="row">
          <button id="btnStop" class="btn">Interview abbrechen</button>
        </div>
      </div>

      <div class="grid2" style="margin-top:12px">
        <div>
          <div class="muted" style="margin-bottom:6px">Kamera-Vorschau</div>
          <video id="localVideo" autoplay playsinline muted></video>
          <div class="muted" style="margin:8px 0 4px">Mic-Level</div>
          <div class="bar"><i id="micLevel"></i></div>
        </div>
        <div>
          <div class="muted" style="margin-bottom:6px">Assistent-Audio</div>
          <audio id="remoteAudio" controls></audio>
          <div class="muted" style="margin:8px 0 4px">System</div>
          <div class="bar"><i id="sysLevel"></i></div>
          <div class="muted" style="margin-top:10px">Hinweis: Bei blockiertem Autoplay ggf. in den Player klicken.</div>
        </div>
      </div>
    </section>

    <aside class="card">
      <div style="font-weight:600;margin-bottom:8px">Recorder / Upload</div>
      <div class="row">
        <div class="badge ghost" id="recState">idle</div>
        <div class="badge ghost" id="muxState" style="margin-left:6px">no export</div>
      </div>
      <div class="muted" style="margin:8px 0 4px">Fortschritt</div>
      <div class="bar"><i id="recProgress"></i></div>
      <div class="muted" style="margin:10px 0 4px">Logs</div>
      <div id="log" class="log"></div>
    </aside>
  </div>

  <script>
  /* ============ Query & UI ============ */
  const Q = new URLSearchParams(location.search);
  const UID = (Q.get('uid')||'').trim();
  const COMPANY_ID = (Q.get('companyId')||'').trim();
  const LANG = (Q.get('lang')||'de').toLowerCase();
  const VOICE = (Q.get('voice')||'verse').toLowerCase();
  const AUTOSTART = Q.get('autostart') !== '0';

  const $status = document.getElementById('status');
  const $badge  = document.getElementById('audioBadge');
  const $mic    = document.getElementById('micLevel');
  const $sys    = document.getElementById('sysLevel');
  const $log    = document.getElementById('log');
  const $localV = document.getElementById('localVideo');
  const $remoteA= document.getElementById('remoteAudio');
  const $recState = document.getElementById('recState');
  const $muxState = document.getElementById('muxState');
  const $recProg  = document.getElementById('recProgress');

  function log(...a){
    console.log(...a);
    const s = a.map(x => typeof x==='object' ? JSON.stringify(x) : String(x)).join(' ');
    $log.textContent += s + "\n";
    $log.scrollTop = $log.scrollHeight;
    try{ parent?.postMessage?.({type:'clarity.live.log', data:s}, '*'); }catch(_){}
  }
  function setStatus(kind, text){
    $status.textContent = text || kind || '';
    $status.style.background = kind==='err' ? '#3b0d0d' : (kind==='ok' ? '#0d3b2a' : (kind==='warn' ? '#3b2f0d' : '#0f172a'));
    $status.style.borderColor = kind==='err' ? '#7f1d1d' : (kind==='ok' ? '#14532d' : (kind==='warn' ? '#7f6a1d' : '#24314f'));
  }

  /* ============ Endpunkte ============ */
  const TOKEN_URL = 'https://www.clarity-nvl.com/_functions/realtimeToken';

  /* ============ State ============ */
  let pc, DC;
  let dcIsOpen = false;
  const dcQueue = [];
  let localStream, mixedStream, mediaRecorder;
  let remoteStream;
  let audioCtx, remoteSource;
  let destroyed = false;

  let PARENT_UPLOAD = true;
  let RECORD_ENABLED = false;
  let CTX = {};

  let FIRST_Q_SENT = false;
  let READY_DETECTED = false;
  let AUTO_FIRST_Q_TIMER = null;
  let STARTED = false;
  let PRIMED = false;
  let WELCOMED = false;

  /* ============ DC Queue ============ */
  function dcSend(obj){
    try{
      if (dcIsOpen && DC?.readyState === 'open'){
        DC.send(JSON.stringify(obj));
      } else {
        dcQueue.push(obj);
        log('[DC] queued', (obj && (obj.type || Object.keys(obj)[0])) || '');
      }
    }catch(e){ log('[DC] send fail', e); }
  }
  function flushDCQueue(){
    if (!dcIsOpen || !DC) return;
    try{
      while (dcQueue.length){
        const m = dcQueue.shift();
        DC.send(JSON.stringify(m));
      }
      log('[DC] queue flushed');
    }catch(e){ log('[DC] flush error', e); }
  }

  /* ============ Token ============ */
  async function getRealtimeToken(payload){
    log('[Live] Fetching token…', TOKEN_URL, JSON.stringify(payload));
    const r = await fetch(TOKEN_URL,{ method:'POST', headers:{'Content-Type':'application/json'}, body:JSON.stringify(payload) });
    if (!r.ok){ const t=await r.text().catch(()=>r.statusText); throw new Error('token_http_'+r.status+': '+t); }
    const data = await r.json().catch(e=>{ throw new Error('token_json_parse: '+e); });
    if (!data?.ok) throw new Error('token_error: '+(data?.error||'unknown')+' '+(data?.message||'')); 
    const sec = data.client_secret || data?.clientSecret || data?.token || (data?.client_secret?.value);
    if (!sec) throw new Error('client_secret_missing');
    log('[Live] effective', { lang:data.reportLang||payload.lang, voice:data.voice, model:data.model });
    return { secret: sec, ctx: data };
  }

  /* ============ WebRTC ============ */
  async function connectRealtime(clientSecret){
    pc = new RTCPeerConnection({ iceServers: [{ urls: ['stun:stun.l.google.com:19302'] }] });

    pc.onconnectionstatechange = () => {
      log('[RTC]', pc.connectionState);
      if (pc.connectionState === 'connected'){
        parent?.postMessage?.({ type:'clarity.live.ready', data:{} }, '*');
      }
      if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected'){
        setStatus('err','Verbindung unterbrochen');
      }
    };

    pc.ontrack = async (ev) => {
      log('[RTC] ontrack kind=', ev.track.kind);
      if (!remoteStream) remoteStream = new MediaStream();
      remoteStream.addTrack(ev.track);
      $remoteA.srcObject = remoteStream;

      // System meter
      try{
        audioCtx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();
        await audioCtx.resume().catch(()=>{});
        remoteSource = audioCtx.createMediaStreamSource(remoteStream);
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 512;
        const buf = new Uint8Array(analyser.frequencyBinCount);
        remoteSource.connect(analyser);
        (function tick(){
          if (destroyed) return;
          analyser.getByteTimeDomainData(buf);
          let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
          $sys.style.width = Math.min(100, dev*2) + '%';
          requestAnimationFrame(tick);
        })();
      }catch(_){}

      try{ $remoteA.muted=false; $remoteA.volume=1.0; $remoteA.autoplay=true; $remoteA.playsInline=true; await $remoteA.play(); log('[RTC] remote audio playing.'); }
      catch(e){ log('[RTC] play() blocked', e); setStatus('warn','Autoplay blockiert – bitte Player antippen.'); }
    };

    // Lokale Tracks (Mic erstmal stumm)
    if (localStream){
      localStream.getAudioTracks().forEach(t => { t.enabled=false; pc.addTrack(t, localStream); });
      localStream.getVideoTracks().forEach(t => pc.addTrack(t, localStream));
    }

    // DataChannel
    DC = pc.createDataChannel('oai-events');
    DC.onopen = () => {
      dcIsOpen = true;
      log('[RTC] DC open');
      parent?.postMessage?.({ type:'clarity.live.ready', data:{} }, '*');
      flushDCQueue();
      // Safety: falls Priming vorher gequeued nicht ankam, hier nochmal
      tryPrimeOnce();
      tryWelcomeOnce();
    };
    DC.onmessage = onRealtimeEvent;

    const offer = await pc.createOffer({ offerToReceiveAudio:true, offerToReceiveVideo:false });
    await pc.setLocalDescription(offer);

    const r = await fetch('https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview', {
      method: 'POST',
      headers: { 'Authorization': `Bearer ${clientSecret}`, 'Content-Type': 'application/sdp' },
      body: offer.sdp
    });
    if (!r.ok){ const t=await r.text().catch(()=>r.statusText); throw new Error('realtime_handshake_failed: '+t); }
    const answer = { type:'answer', sdp: await r.text() };
    await pc.setRemoteDescription(answer);
  }

  function onRealtimeEvent(ev){
    let data = ev && ev.data;
    try{ if (typeof data === 'string') data = JSON.parse(data); }catch(_){}
    if (!data) return;

    if (data.type === 'response.state' && data.state){
      parent?.postMessage?.({ type:'clarity.avatar', data:{ state:data.state } }, '*');
    }
    if (data.type === 'input_audio_buffer.speech_started'){
      READY_DETECTED = true; maybeAskFirstQuestion();
    }
    if (data.type === 'input_audio_transcription.completed'){
      try{
        const txt = (data.transcription?.text || '').toLowerCase();
        if (/(^|\b)(bereit|ready)(\b|$)/.test(txt)){ READY_DETECTED = true; maybeAskFirstQuestion(); }
      }catch(_){}
    }
  }

  /* ============ Interview Prime ============ */
  function buildSystemInstructions(ctx){
    const isDE = String(ctx.reportLang||'de').toLowerCase().startsWith('de');
    const langLine = isDE
      ? 'Sprich ausschließlich DEUTSCH. Wechsle nie die Sprache.'
      : 'Speak strictly in ENGLISH. Do not switch languages.';
    const structure = isDE
      ? 'Führe ein strukturiertes Job-Interview: nur EINE Frage zur Zeit, max. 1 Nachfrage, strikt am Jobprofil bleiben.'
      : 'Conduct a structured job interview: one question at a time, max one follow-up, stick to the job profile.';
    const waiting = isDE
      ? 'Warte nach jeder Frage, bis die Kandidatin/der Kandidat fertig ist.'
      : 'After each question, wait until the candidate is finished.';
    return [langLine, structure, waiting].join(' ');
  }

  function primeStrictLanguage(reportLang){
    const spoken = String(reportLang||'de').toLowerCase().startsWith('de') ? 'de' : 'en';
    const sys = buildSystemInstructions(CTX);

    // Session: HARTE Vorgaben inkl. instructions
    dcSend({ type:'session.update', session:{
      voice: VOICE,
      instructions: sys,
      spoken_language: spoken,
      input_audio_format:'pcm16',
      output_audio_format:'pcm16',
      input_audio_transcription:{ model:'whisper-1', language: spoken },
      turn_detection:{ type:'none' },
      modalities:['audio']
    }});
    log('[PRIME] session.update (lang+none VAD)', spoken);

    // Gesprächszustand zur Sicherheit resetten
    dcSend({ type:'conversation.clear' });
    log('[CONV] cleared');

    // doppelt als System-Item (zusätzliche Robustheit)
    dcSend({ type:'conversation.item.create', item:{ type:'message', role:'system', content:[{type:'input_text', text:sys}] }});
    log('[CONV] system item created');
  }

  function tryPrimeOnce(){
    if (PRIMED) return;
    PRIMED = true;
    primeStrictLanguage(CTX.reportLang || LANG);
  }

  function speakPrepThenWelcome(){
    if (WELCOMED) return;
    WELCOMED = true;

    const isDE = String(CTX.reportLang||'de').toLowerCase().startsWith('de');
    const prep = isDE ? 'Einen Moment bitte, ich bereite das Interview vor …'
                      : 'One moment please, I am preparing the interview …';
    const welcome = isDE
      ? `Willkommen! Dieses Interview wird im Auftrag von ${CTX.companyName||'Clarity'} geführt. Sind Sie bereit zu starten? Bitte sagen Sie „bereit“, dann stelle ich die erste Frage.`
      : `Welcome! This interview is conducted on behalf of ${CTX.companyName||'Clarity'}. Are you ready to begin? Please say “ready”, then I will ask the first question.`;

    dcSend({ type:'response.cancel' });
    log('[CONV] responses cancelled');

    dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: prep }});
    log('[WELCOME] prep spoken');

    setTimeout(()=>{
      dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: welcome }});
      log('[WELCOME] welcome spoken');

      dcSend({ type:'session.update', session:{ turn_detection:{ type:'server_vad', threshold:0.9, prefix_padding_ms:200, silence_duration_ms:700 } }});
      log('[PRIME] VAD enabled');

      enableMicUplink();

      parent?.postMessage?.({ type:'clarity.live.prepared', data:{} }, '*');
      log('[PREPARED] sent to parent');

      clearTimeout(AUTO_FIRST_Q_TIMER);
      AUTO_FIRST_Q_TIMER = setTimeout(()=>{ if (!FIRST_Q_SENT) maybeAskFirstQuestion(); }, 6000);
    }, 1000);
  }

  function tryWelcomeOnce(){ if (!WELCOMED) speakPrepThenWelcome(); }

  function buildInterviewPlan(ctx){
    const isDE = String(ctx.reportLang||'de').toLowerCase().startsWith('de');
    const baseDE = [
      'Bitte stellen Sie sich kurz vor.',
      'Warum passt diese Position zu Ihnen?',
      'Beschreiben Sie ein Projekt, auf das Sie stolz sind.',
      'Wie gehen Sie mit knappen Deadlines um?',
      'Welche Erwartungen haben Sie an die Rolle und das Team?'
    ];
    const baseEN = [
      'Please introduce yourself briefly.',
      'Why is this position a good fit for you?',
      'Describe a project you are proud of.',
      'How do you handle tight deadlines?',
      'What are your expectations for the role and the team?'
    ];
    const seeds = (isDE? baseDE : baseEN).slice();
    const text = (ctx.roleProfileText||'').trim() + '\n' + (ctx.docsText||'').trim();
    const lines = text.split(/\r?\n/).map(s=>s.trim()).filter(Boolean).slice(0,5);
    if (lines.length){ lines.forEach(l => seeds.push((isDE? 'Bezug zur Ausschreibung: ' : 'Regarding the job ad: ')+l)); }
    return seeds.slice(0, ctx.interviewConfig?.maxQuestions || 5);
  }

  function askFirstQuestion(){
    if (FIRST_Q_SENT) return;
    FIRST_Q_SENT = true;

    const plan = buildInterviewPlan(CTX);
    const isDE = String(CTX.reportLang||'de').toLowerCase().startsWith('de');
    const first = plan[0] || (isDE ? 'Bitte stellen Sie sich kurz vor.' : 'Please introduce yourself.');

    // kleine System-Notiz und dann Frage
    dcSend({ type:'conversation.item.create', item:{ type:'message', role:'system', content:[{type:'input_text', text:'Starte jetzt die erste Interviewfrage. Kurze, präzise Fragen. Danach warten.'}] }});
    log('[CONV] system item created');

    const q = (isDE ? `Erste Frage: ${first}` : `First question: ${first}`);
    dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: q }});
    log('[QUESTION] first sent');
  }
  function maybeAskFirstQuestion(){ if (!FIRST_Q_SENT) askFirstQuestion(); }

  /* ============ Media / Recorder ============ */
  async function setupLocalMedia(){
    try{
      localStream = await navigator.mediaDevices.getUserMedia({
        audio:{ echoCancellation:true, noiseSuppression:true, autoGainControl:true },
        video:true
      });
    } catch(e) {
      setStatus('err','Mic/Kamera blockiert. Bitte im Browser freigeben.');
      log('[Media] getUserMedia error', e);
      // Parent benachrichtigen (wichtig bei Permissions-Policy)
      try{ parent?.postMessage?.({ type:'clarity.live.permission_error', data:{ message: String(e?.name||e) } }, '*'); }catch(_){}
      // Ohne Mic weiter machen (nur Remote-Audio)
      localStream = null;
    }
    if (localStream){
      $localV.srcObject = localStream;
      try{
        const ctx = new (window.AudioContext || window.webkitAudioContext)();
        const src = ctx.createMediaStreamSource(localStream);
        const analyser = ctx.createAnalyser();
        analyser.fftSize = 512;
        const buf = new Uint8Array(analyser.frequencyBinCount);
        src.connect(analyser);
        (function tick(){
          if (destroyed) return;
          analyser.getByteTimeDomainData(buf);
          let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
          $mic.style.width = Math.min(100, dev*2) + '%';
          requestAnimationFrame(tick);
        })();
      }catch(_){}
      // Mic zunächst stumm
      localStream.getAudioTracks().forEach(t => t.enabled = false);
    }
  }

  function enableMicUplink(){ try{ localStream?.getAudioTracks()?.forEach(t => t.enabled=true); log('[MIC] uplink enabled'); }catch(_){} }

  async function blobToDataURL(blob){
    return new Promise((resolve,reject)=>{ const r=new FileReader(); r.onloadend=()=>resolve(r.result); r.onerror=reject; r.readAsDataURL(blob); });
  }

  function prepareRecording(){
    const ctx = new (window.AudioContext || window.webkitAudioContext)();
    const dest = ctx.createMediaStreamDestination();

    if (localStream){
      const micSrc = ctx.createMediaStreamSource(localStream); micSrc.connect(dest);
    }
    if (remoteStream){ try{ ctx.createMediaStreamSource(remoteStream).connect(dest); }catch(e){ log('[REC] remote mix fail', e); } }

    mixedStream = new MediaStream();
    dest.stream.getAudioTracks().forEach(t => mixedStream.addTrack(t));
    const cam = localStream?.getVideoTracks?.()[0];
    if (cam) mixedStream.addTrack(cam);

    const hasVideo = !!mixedStream.getVideoTracks().length;
    mediaRecorder = new MediaRecorder(mixedStream, {
      mimeType: hasVideo ? 'video/webm;codecs=vp8,opus' : 'audio/webm;codecs=opus',
      videoBitsPerSecond: hasVideo ? 1_200_000 : undefined,
      audioBitsPerSecond: 128_000
    });

    const chunks = [];
    mediaRecorder.onstart = ()=>{ $recState.textContent='recording'; log('[REC] started'); };
    mediaRecorder.ondataavailable = (e)=>{ if (e.data && e.data.size>0) chunks.push(e.data); };
    mediaRecorder.onstop = async ()=>{
      $recState.textContent='stopped';
      const hasVid = !!mixedStream.getVideoTracks().length;
      const blob = new Blob(chunks, { type: hasVid ? 'video/webm' : 'audio/webm' });
      log('[REC] stop, size=', blob.size);

      try{
        $muxState.textContent='exporting'; $recProg.style.width='60%';
        const dataUrl = await blobToDataURL(blob);
        parent?.postMessage?.({ type:'recorder:export', data:{ dataUrl, kind:(hasVid?'video':'audio'), bytes: blob.size } }, '*');
        $recProg.style.width='100%'; $muxState.textContent='export sent';
      }catch(e){ $muxState.textContent='export failed'; log('[REC] export failed', e); }
      finally{ chunks.length=0; }
    };

    $recState.textContent='ready';
  }
  function startRecorder(){ try{ if (mediaRecorder && mediaRecorder.state==='inactive') mediaRecorder.start(); }catch(e){ log('[REC] start error', e); } }
  function stopRecording(){ try{ if (mediaRecorder && mediaRecorder.state!=='inactive') mediaRecorder.stop(); }catch(_){}
    try{ mixedStream?.getTracks()?.forEach(t=>t.stop()); }catch(_){} }

  /* ============ Start / Stop ============ */
  async function startInterview(){
    if (STARTED) return;
    STARTED = true;

    try{
      destroyed = false;
      setStatus('info','Initialisiere…');

      await setupLocalMedia();
      const { secret, ctx } = await getRealtimeToken({ uid:UID, companyId:COMPANY_ID, lang:LANG, voice:VOICE, debug:true, allowNoInvite:true });
      await connectRealtime(secret);
      prepareRecording();

      CTX.reportLang = (CTX.reportLang || ctx.reportLang || LANG);
      CTX.companyName = ctx.companyName || CTX.companyName;

      tryPrimeOnce();        // wird gequeued, falls DC noch nicht offen
      tryWelcomeOnce();      // dito

      setStatus('ok','Verbunden • Interview läuft');
    }catch(e){
      setStatus('err','Startfehler: '+(e?.message||e));
      log('[Start error]', e);
      parent?.postMessage?.({ type:'clarity.live.error', data:{ message:String(e?.message||e) } }, '*');
    }
  }

  async function hangup(){
    destroyed = true;
    try{ DC?.close(); }catch(_){}
    try{ pc?.getSenders()?.forEach(s=>s.track && s.track.stop()); }catch(_){}
    try{ pc?.getReceivers()?.forEach(r=>r.track && r.track.stop()); }catch(_){}
    try{ pc?.close(); }catch(_){}
    try{ localStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
    try{ remoteStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
    stopRecording();
    setStatus('warn','Beendet');
    log('[Live] hangup done');
    parent?.postMessage?.({ type:'clarity.live.ended', data:{} }, '*');
  }

  /* ============ Parent Messaging ============ */
  window.addEventListener('message', (ev)=>{
    const msg = ev.data || {};
    const type = msg.type;

    if (type === 'clarity.live.context'){
      CTX = msg.data || {};
      PARENT_UPLOAD = (CTX.uploadMode === 'parent');
      log('[CTX]', CTX);
    }
    else if (type === 'clarity.live.record'){
      RECORD_ENABLED = !!(msg.data && msg.data.enabled);
      log('[REC] record flag from parent:', RECORD_ENABLED);
      if (RECORD_ENABLED) startRecorder();
    }
    else if (type === 'clarity.live.start'){
      startInterview();
    }
    else if (type === 'clarity.live.stop' || type === 'clarity.live.hangup'){
      hangup();
    }
    else if (type === 'recorder:export'){
      stopRecording();
    }
    else if (type === 'clarity.live.ping'){
      parent?.postMessage?.({ type:'clarity.live.pong', data:{} }, '*');
    }
  });

  function helloParent(){
    try{ parent?.postMessage?.({ type:'clarity.live.hello', data:{ uid:UID, companyId:COMPANY_ID } }, '*'); }catch(_){}
  }

  window.addEventListener('load', ()=>{
    helloParent();
    if (AUTOSTART){
      setTimeout(()=>{ if (!STARTED) { log('[AUTO] no parent start → autostart'); startInterview(); } }, 1800);
    }
  });

  document.getElementById('btnStop').addEventListener('click', hangup);

  // WICHTIG im Parent-Iframe:
  // <iframe ... allow="microphone; camera; autoplay" ...>
  </script>
</body>
</html>
