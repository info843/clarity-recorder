<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Live</title>
  <style>
    html,body { margin:0; padding:0; background:#0b0f14; color:#e5e7eb; font-family:system-ui,Segoe UI,Roboto,Arial,sans-serif; }
    .sr { position:absolute; left:-9999px; width:1px; height:1px; overflow:hidden; }
    #log { position:fixed; left:8px; bottom:8px; right:8px; max-height:40vh; overflow:auto; font:12px ui-monospace; color:#9ca3af; }
    #controls { position:fixed; top:8px; right:8px; display:flex; gap:8px; }
    button { background:#111827; color:#e5e7eb; border:1px solid #374151; border-radius:8px; padding:8px 12px; cursor:pointer; }
    button:hover { background:#1f2937; }
    video { max-width:40vw; height:auto; background:#111827; border:1px solid #374151; border-radius:12px; }
    audio { display:none; } /* braucht kein UI */
  </style>
</head>
<body>
  <!-- Remote-Audio (muss existieren!) -->
  <audio id="remote" autoplay playsinline></audio>

  <div id="controls">
    <button id="btnPlay">Audio entsperren</button>
  </div>

  <div id="log"></div>

  <script>
    // ===== Konfiguration =====
    const TOKEN_ENDPOINT = '/api/wix-token-proxy'; // dein Proxy in interview.clarity-nvl.com
    const DEFAULT_VOICE = 'verse';

    // ===== State =====
    let pc, dc;
    let localStream;
    let recordEnabled = false;
    let started = false;

    const logEl = document.getElementById('log');
    function log(...a){ const t=a.map(x=>typeof x==='object'?JSON.stringify(x):String(x)).join(' ');
      console.log('[LIVE]',...a); if(logEl){ logEl.innerHTML += `<div>${t}</div>`; logEl.scrollTop=logEl.scrollHeight; } }

    // ===== Remote-Audio Autoplay/Policy Fix =====
    const remoteEl = document.getElementById('remote');
    remoteEl.autoplay = true;
    remoteEl.playsInline = true;
    remoteEl.muted = false;
    remoteEl.volume = 1.0;

    async function ensureAudioUnlocked() {
      try { await remoteEl.play(); log('remote.play ok'); }
      catch(e){ log('remote.play blocked:', e?.message || e); }
    }
    // Button für den Fall, dass Autoplay blockiert wurde
    document.getElementById('btnPlay').addEventListener('click', ensureAudioUnlocked);
    // Zusätzlich jeden Pointer als „Gesture“ nutzen
    window.addEventListener('pointerdown', ensureAudioUnlocked, { once:true });

    // ===== Parent → iFrame Messaging =====
    window.addEventListener('message', async (ev) => {
      const msg = typeof ev.data === 'string' ? parseMaybeJSON(ev.data) : ev.data || {};
      if (!msg || !msg.type) return;
      if (msg.type === 'clarity.live.context'){
        // falls du Kontext mitschicken willst (Company, Position etc.)
        log('context', msg.data);
      }
      if (msg.type === 'clarity.live.record'){
        recordEnabled = !!(msg.data && msg.data.enabled);
        log('recordEnabled =', recordEnabled);
      }
      if (msg.type === 'clarity.live.start'){
        if (started) return;
        started = true;
        const { uid, companyId, lang } = msg.data || {};
        try{
          await startLive({ uid, companyId, lang: lang || 'de' });
        }catch(e){
          log('startLive error', e);
          postParent({ type:'mux.upload.error', data:{ message: e?.message || String(e) }});
        }
      }
    });

    function postParent(obj){
      try { window.parent.postMessage(obj, '*'); } catch {}
    }
    function parseMaybeJSON(s){ try{ return JSON.parse(s); }catch{ return null; } }

    // ===== Live Start =====
    async function startLive({ uid, companyId, lang='de' }){
      log('Token endpoint:', TOKEN_ENDPOINT);
      log('Fetching token…', JSON.stringify({ uid, companyId, lang, voice: DEFAULT_VOICE }));

      // 1) Token holen
      const tr = await fetch(TOKEN_ENDPOINT, {
        method: 'POST',
        headers: { 'Content-Type':'application/json' },
        body: JSON.stringify({ uid, companyId, lang, voice: DEFAULT_VOICE })
      });
      const tjson = await tr.json().catch(()=>({}));
      log('Token response status:', tr.status, 'body:', JSON.stringify(tjson));
      if (!tr.ok || !tjson?.ok || !tjson?.token) throw new Error('token error');

      // 2) Medien anfordern (Video+Audio, aber Audio ist entscheidend)
      await ensureAudioUnlocked(); // vorab versuchen
      localStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: true });
      log('Cam+Mic ready');

      // 3) PeerConnection
      pc = new RTCPeerConnection();
      pc.onconnectionstatechange = () => log('pc:', pc.connectionState);
      pc.oniceconnectionstatechange = () => log('ice:', pc.iceConnectionState);

      // **WICHTIG**: Audio sendrecv, damit Remote-Audio reinkommt
      pc.addTransceiver('audio', { direction: 'sendrecv' });
      // Video typischerweise sendonly (falls du Remote-Video nicht brauchst)
      pc.addTransceiver('video', { direction: 'sendonly' });

      // Lokale Spuren an die passenden Sender hängen
      for (const track of localStream.getTracks()) {
        const kind = track.kind; // 'audio' | 'video'
        const sender = pc.getSenders().find(s => s.track == null && s.transport == null && s?.track?.kind === kind) // older
                      || pc.getSenders().find(s => s?.track == null && s?.transport == null)
                      || null;
        // fallback – einfacher: addTrack
        pc.addTrack(track, localStream);
      }

      // Remote-Track → Audio-Element
      pc.ontrack = (ev) => {
        const [stream] = ev.streams;
        if (stream && remoteEl.srcObject !== stream) {
          remoteEl.srcObject = stream;
          remoteEl.play().catch(e => log('remote.play error', e?.message || e));
          log('Remote stream set');
        }
      };

      // 4) DataChannel (Steuerung/Text)
      dc = pc.createDataChannel('oai-events');
      dc.onopen = () => {
        log('DataChannel open');

        // Session-Update (optional: VAD aktivieren)
        safeSend({
          type: 'session.update',
            session: {
              turn_detection: { type: 'server_vad' } // Modell hört auf Sprechpausen
            }
        });

        // --- MINI SMOKE TEST: sofort hörbare Ausgabe erzeugen ---
        safeSend({
          type: 'response.create',
          response: {
            modalities: ['audio','text'],
            instructions: "Sprich auf Deutsch: 'Test. Ich bin verbunden.' Stelle anschließend eine einfache Einstiegsfrage."
          }
        });
      };
      dc.onmessage = (ev) => {
        // hier kannst du Model-Events loggen
        log('DC message:', typeof ev.data === 'string' ? ev.data : '[binary]');
      };

      // 5) Offer/Answer (OpenAI Realtime)
      const offer = await pc.createOffer({ offerToReceiveAudio: true, offerToReceiveVideo: false });
      await pc.setLocalDescription(offer);

      const baseUrl = 'https://api.openai.com/v1/realtime';
      const model = tjson.model || 'gpt-4o-realtime-preview-2025-09-12';
      const sdpResp = await fetch(`${baseUrl}?model=${encodeURIComponent(model)}`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${tjson.token}`,
          'Content-Type': 'application/sdp'
        },
        body: offer.sdp
      });

      const answerSdp = await sdpResp.text();
      await pc.setRemoteDescription({ type: 'answer', sdp: answerSdp });
      log('Realtime connected');

      // Aufzeichnen? (deine Recorder-/Mux-Logik bleibt unberührt)
      if (recordEnabled) {
        // Hier deine bestehende MediaRecorder→Mux Logik weiterverwenden
        log('MediaRecorder started (video+audio)');
        // ...
      }
    }

    function safeSend(obj){
      try { dc?.send?.(JSON.stringify(obj)); }
      catch(e){ log('dc send error:', e?.message || e); }
    }
  </script>
</body>
</html>
