<!doctype html>
<html lang="de">
<head>
  <meta charset="utf-8" />
  <title>Live Interview</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { --bg:#0b0c10; --fg:#e5e7eb; --muted:#9ca3af; --ok:#10b981; --warn:#f59e0b; --err:#ef4444; --card:#111217; --line:#1f2330; }
    html,body{margin:0;padding:0;background:var(--bg);color:var(--fg);font:14px/1.45 system-ui,Segoe UI,Roboto,Helvetica,Arial,sans-serif}
    .wrap{display:grid;grid-template-columns:1fr 320px;gap:16px;padding:14px;align-items:start}
    .card{background:var(--card);border:1px solid var(--line);border-radius:12px;padding:12px}
    .row{display:flex;gap:10px;align-items:center}
    .bar{height:6px;background:#0f172a;border-radius:999px;overflow:hidden}
    .bar > i{display:block;height:100%;width:0;background:#4f46e5;transition:width 50ms linear}
    .badge{display:inline-block;padding:2px 6px;border-radius:8px;background:#0f172a;border:1px solid #24314f;color:#cbd5e1;font-size:12px}
    .muted{color:var(--muted)}
    .btn{appearance:none;border:1px solid #2b3244;background:#141827;color:#f8fafc;border-radius:8px;padding:8px 10px;cursor:pointer}
    .btn:disabled{opacity:.5;cursor:not-allowed}
    video{width:100%;max-height:320px;background:#000;border-radius:8px}
    audio{width:100%}
    .grid2{display:grid;grid-template-columns:1fr 1fr;gap:10px}
    .log{font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px;color:#9ca3af;white-space:pre-wrap;max-height:220px;overflow:auto}
    .ghost{opacity:.65}
  </style>
</head>
<body>
  <div class="wrap">
    <section class="card">
      <div class="row" style="justify-content:space-between">
        <div>
          <div id="status" class="badge">Init…</div>
          <div id="audioBadge" class="badge" style="margin-left:6px">• (stumm?)</div>
        </div>
        <div class="row">
          <button id="btnStop" class="btn">Interview abbrechen</button>
        </div>
      </div>

      <div class="grid2" style="margin-top:12px">
        <div>
          <div class="muted" style="margin-bottom:6px">Kamera-Vorschau</div>
          <video id="localVideo" autoplay playsinline muted></video>
          <div class="muted" style="margin:8px 0 4px">Mic-Level</div>
          <div class="bar"><i id="micLevel"></i></div>
        </div>
        <div>
          <div class="muted" style="margin-bottom:6px">Assistent-Audio</div>
          <audio id="remoteAudio" controls></audio>
          <div class="muted" style="margin:8px 0 4px">System</div>
          <div class="bar"><i id="sysLevel"></i></div>
          <div class="muted" style="margin-top:10px">Hinweis: Bei blockiertem Autoplay ggf. in den Player klicken.</div>
        </div>
      </div>
    </section>

    <aside class="card">
      <div style="font-weight:600;margin-bottom:8px">Recorder / Upload</div>
      <div class="row">
        <div class="badge ghost" id="recState">idle</div>
        <div class="badge ghost" id="muxState" style="margin-left:6px">no export</div>
      </div>
      <div class="muted" style="margin:8px 0 4px">Fortschritt</div>
      <div class="bar"><i id="recProgress"></i></div>
      <div class="muted" style="margin:10px 0 4px">Logs</div>
      <div id="log" class="log"></div>
    </aside>
  </div>

<script>
;(()=>{"use strict";

// ---- Guard: doppelte Ausführung verhindern (Wix lädt manchmal doppelt) ----
if (window.__clarity_live_active) { console.warn('[LIVE] duplicate script ignored'); return; }
window.__clarity_live_active = true;

// -------------------- Query --------------------
const Q = new URLSearchParams(location.search);
const UID = (Q.get('uid')||'').trim();
const COMPANY_ID = (Q.get('companyId')||'').trim();
const LANG = (Q.get('lang')||'de').toLowerCase();
const VOICE = (Q.get('voice')||'verse').toLowerCase();
// Autostart standardmäßig AUS – Parent steuert!
const AUTOSTART = Q.get('autostart') === '1';

// -------------------- UI Els -------------------
const $status = document.getElementById('status');
const $badge  = document.getElementById('audioBadge');
const $mic    = document.getElementById('micLevel');
const $sys    = document.getElementById('sysLevel');
const $log    = document.getElementById('log');
const $localV = document.getElementById('localVideo');
const $remoteA= document.getElementById('remoteAudio');
const $btnStop= document.getElementById('btnStop');
const $recState = document.getElementById('recState');
const $muxState = document.getElementById('muxState');
const $recProg  = document.getElementById('recProgress');

function log(...a){
  try{
    console.log(...a);
    const s = a.map(x => typeof x==='object' ? JSON.stringify(x) : String(x)).join(' ');
    $log.textContent += s + "\n";
    $log.scrollTop = $log.scrollHeight;
    parent?.postMessage?.({type:'clarity.live.log', data:s}, '*');
  }catch(_){}
}
function setStatus(kind, text){
  $status.textContent = text || kind || '';
  $status.style.background = kind==='err' ? '#3b0d0d' : (kind==='ok' ? '#0d3b2a' : (kind==='warn' ? '#3b2f0d' : '#0f172a'));
  $status.style.borderColor = kind==='err' ? '#7f1d1d' : (kind==='ok' ? '#14532d' : (kind==='warn' ? '#7f6a1d' : '#24314f'));
}

// -------------------- Endpunkt -----------------
const TOKEN_URL = 'https://www.clarity-nvl.com/_functions/realtimeToken';

// -------------------- Global State -------------
let pc, DC;
let localStream = null, mixedStream = null, mediaRecorder = null;
let remoteStream = null;
let audioCtx = null, remoteSource = null;
let destroyed = false;
let started = false;

// Parent flags / Kontext
let PARENT_UPLOAD = true;
let RECORD_ENABLED = false;
let CTX = {};
let ENDED_SENT = false;

// Flow-Gates
let PRIMED = false;
let WELCOMED = false;
let FIRST_Q_SENT = false;
let dcIsOpen = false;

// DC-Queue
const dcQueue = [];
function dcSend(obj){
  if (!obj) return;
  if (dcIsOpen && DC?.readyState === 'open') {
    try { DC.send(JSON.stringify(obj)); } catch(e){ log('[DC] send fail', e); }
  } else {
    dcQueue.push(obj);
    if (obj?.type) log('[DC] queued', obj.type);
  }
}
function flushDC(){
  if (!dcIsOpen || !DC || DC.readyState!=='open') return;
  while (dcQueue.length){
    const msg = dcQueue.shift();
    try { DC.send(JSON.stringify(msg)); } catch(e){ log('[DC] flush fail', e); }
  }
  log('[DC] queue flushed');
}

// -------------------- Token --------------------
async function getRealtimeToken(payload){
  log('[Live] Fetching token…', TOKEN_URL, JSON.stringify(payload));
  const r = await fetch(TOKEN_URL,{ method:'POST', headers:{'Content-Type':'application/json'}, body:JSON.stringify(payload) });
  if (!r.ok){
    const t = await r.text().catch(()=>r.statusText);
    throw new Error('token_http_'+r.status+': '+t);
  }
  const data = await r.json().catch(e=>{ throw new Error('token_json_parse: '+e); });
  if (!data?.ok) throw new Error(data?.error || 'token_error');
  const sec = (data.client_secret?.value) || data.client_secret || data.clientSecret || data.token;
  if (!sec) throw new Error('client_secret_missing');
  log('[Live] effective', { lang:data.reportLang||payload.lang, voice:data.voice, model:data.model });
  return { secret: sec, ctx: data };
}

// -------------------- WebRTC -------------------
function onRealtimeEvent(ev){
  let data = ev && ev.data;
  try{ if (typeof data === 'string') data = JSON.parse(data); }catch(_){}
  if (!data) return;

  if (data.type === 'response.state' && data.state){
    parent?.postMessage?.({ type:'clarity.avatar', data:{ state:data.state } }, '*');
  }

  if (data.type === 'input_audio_transcription.completed'){
    const txt = (data?.transcription?.text || '').toLowerCase();
    if (/\bbereit\b|\bready\b|\bstart(en)?\b/.test(txt)) {
      maybeAskFirstQuestion();
    }
  }
}

async function connectRealtime(clientSecret){
  if (pc) return;
  pc = new RTCPeerConnection({ iceServers: [{ urls: ['stun:stun.l.google.com:19302'] }] });

  pc.onconnectionstatechange = () => {
    if (pc.connectionState === 'connected') log('[RTC] connected');
    else log('[RTC] connecting');
    if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected'){
      setStatus('err','Verbindung unterbrochen');
    }
  };

  pc.ontrack = async (ev) => {
    log('[RTC] ontrack kind=', ev.track.kind);
    if (!remoteStream) remoteStream = new MediaStream();
    remoteStream.addTrack(ev.track);
    $remoteA.srcObject = remoteStream;

    // Audio-Analyse
    try{
      audioCtx = audioCtx || new (window.AudioContext || window.webkitAudioContext)();
      await audioCtx.resume().catch(()=>{});
      remoteSource = audioCtx.createMediaStreamSource(remoteStream);
      const analyser = audioCtx.createAnalyser();
      analyser.fftSize = 512;
      const buf = new Uint8Array(analyser.frequencyBinCount);
      remoteSource.connect(analyser);
      (function tick(){
        if (destroyed) return;
        analyser.getByteTimeDomainData(buf);
        let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
        $sys.style.width = Math.min(100, dev*2) + '%';
        requestAnimationFrame(tick);
      })();
    }catch(_){}

    // Playback – hier nochmals versuchen (Fallback falls Autoplay blockiert war)
    try {
      $remoteA.muted = false; $remoteA.volume = 1.0;
      $remoteA.setAttribute('autoplay',''); $remoteA.setAttribute('playsinline','');
      await $remoteA.play();
      log('[AUDIO] remote audio playing (unlocked).');
    } catch(e) {
      log('[AUDIO] autoplay blocked, waiting for user gesture.');
      setStatus('warn','Autoplay blockiert – bitte in den Player klicken.');
      const once = ()=>{$remoteA.play().catch(()=>{}); document.removeEventListener('pointerdown', once);};
      document.addEventListener('pointerdown', once, { once:true });
    }

    // Eingangspegel-Badge
    try {
      const levelAnalyser = audioCtx.createAnalyser();
      levelAnalyser.fftSize = 1024;
      remoteSource.connect(levelAnalyser);
      const tbuf = new Uint8Array(levelAnalyser.frequencyBinCount);
      let lastNZ = 0;
      (function ping(){
        if (destroyed) return;
        levelAnalyser.getByteTimeDomainData(tbuf);
        let dev = 0;
        for (let i=0;i<tbuf.length;i++) dev = Math.max(dev, Math.abs(tbuf[i]-128));
        if (dev>2) lastNZ = performance.now();
        $badge.textContent = (performance.now()-lastNZ < 2000) ? '• Audio in' : '• (stumm?)';
        requestAnimationFrame(ping);
      })();
    }catch(_){}
  };

  // DataChannel
  DC = pc.createDataChannel('oai-events');
  DC.onopen = () => { dcIsOpen = true; log('[RTC] DC open'); flushDC(); };
  DC.onmessage = onRealtimeEvent;

  // Nur Empfang aushandeln (Mic kommt später)
  const offer = await pc.createOffer({ offerToReceiveAudio:true, offerToReceiveVideo:false });
  await pc.setLocalDescription(offer);

  const r = await fetch('https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview', {
    method: 'POST',
    headers: { 'Authorization': `Bearer ${clientSecret}`, 'Content-Type': 'application/sdp' },
    body: offer.sdp
  });
  if (!r.ok){
    const t = await r.text().catch(()=>r.statusText);
    throw new Error('realtime_handshake_failed: ' + t);
  }
  const answer = { type:'answer', sdp: await r.text() };
  await pc.setRemoteDescription(answer);

  parent?.postMessage?.({ type:'clarity.live.ready', data:{} }, '*');
}

// -------------------- Interview Prime -------------------
function buildSystemInstructions(ctx){
  const isDE = String(ctx.reportLang||'de').toLowerCase().startsWith('de');
  const langLine = isDE
    ? 'Sprich ausschließlich DEUTSCH. Wechsle nie die Sprache.'
    : 'Speak strictly in ENGLISH. Do not switch languages.';
  const structure = isDE
    ? 'Führe ein strukturiertes Job-Interview: nur EINE Frage zur Zeit, max. 1 Nachfrage, strikt am Jobprofil bleiben.'
    : 'Conduct a structured job interview: one question at a time, max one follow-up, stick to the job profile.';
  const waiting = isDE
    ? 'Warte nach jeder Frage, bis die Kandidatin/der Kandidat fertig ist.'
    : 'After each question, wait until the candidate is finished.';
  return [langLine, structure, waiting].join(' ');
}

function primeStrictLanguage(reportLang){
  const spoken = String(reportLang||'de').toLowerCase().startsWith('de') ? 'de' : 'en';
  const sys = buildSystemInstructions(CTX);

  dcSend({ type:'session.update', session:{
    voice: VOICE,
    instructions: sys,
    spoken_language: spoken,
    input_audio_format:'pcm16',
    output_audio_format:'pcm16',
    input_audio_transcription:{ model:'whisper-1', language: spoken },
    turn_detection:{ type:'none' },
    modalities:['audio']
  }});
  log('[PRIME] session.update (lang+none VAD)', spoken);

  dcSend({ type:'conversation.clear' });
  log('[CONV] cleared');

  dcSend({ type:'conversation.item.create', item:{ type:'message', role:'system', content:[{type:'input_text', text:sys}] }});
  log('[CONV] system item created');
}

function tryPrimeOnce(){
  if (PRIMED) return;
  PRIMED = true;
  primeStrictLanguage(CTX.reportLang || LANG);
}

function speakPrepThenWelcome(){
  if (WELCOMED) return;
  WELCOMED = true;

  const isDE = String(CTX.reportLang||'de').toLowerCase().startsWith('de');
  const prep = isDE ? 'Einen Moment bitte, ich bereite das Interview vor …'
                    : 'One moment please, I am preparing the interview …';
  const welcome = isDE
    ? `Willkommen! Dieses Interview wird im Auftrag von ${CTX.companyName||'Clarity'} geführt. Sind Sie bereit zu starten? Bitte sagen Sie „bereit“, dann stelle ich die erste Frage.`
    : `Welcome! This interview is conducted on behalf of ${CTX.companyName||'Clarity'}. Are you ready to begin? Please say “ready”, then I will ask the first question.`;

  dcSend({ type:'response.cancel' });
  log('[CONV] responses cancelled');

  // 1) Vorbereitung
  dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: prep }});
  log('[WELCOME] prep spoken');

  // 2) Willkommen
  setTimeout(()=>{
    dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: welcome }});
    log('[WELCOME] welcome spoken');

    // 3) Jetzt VAD aktivieren
    dcSend({ type:'session.update', session:{ turn_detection:{ type:'server_vad', threshold:0.9, prefix_padding_ms:200, silence_duration_ms:700 } }});
    log('[PRIME] VAD enabled');

    // 4) Mic aktivieren (Permissions-Prompt NACH Welcome)
    enableMicUplink();

    // 5) Parent informieren
    parent?.postMessage?.({ type:'clarity.live.prepared', data:{} }, '*');
    log('[PREPARED] sent to parent');

    // 6) Fallback: erste Frage nach 6s, falls niemand „bereit“ sagt
    setTimeout(()=>{ if (!FIRST_Q_SENT) maybeAskFirstQuestion(); }, 6000);
  }, 900);
}

function tryWelcomeOnce(){ if (!WELCOMED) speakPrepThenWelcome(); }

function buildInterviewPlan(ctx){
  const isDE = String(ctx.reportLang||'de').toLowerCase().startsWith('de');
  const baseDE = [
    'Bitte stellen Sie sich kurz vor.',
    'Warum passt diese Position zu Ihnen?',
    'Beschreiben Sie ein Projekt, auf das Sie stolz sind.',
    'Wie gehen Sie mit knappen Deadlines um?',
    'Welche Erwartungen haben Sie an die Rolle und das Team?'
  ];
  const baseEN = [
    'Please introduce yourself briefly.',
    'Why is this position a good fit for you?',
    'Describe a project you are proud of.',
    'How do you handle tight deadlines?',
    'What are your expectations for the role and the team?'
  ];
  const seeds = (isDE? baseDE : baseEN).slice();
  const text = (ctx.roleProfileText||'').trim() + '\n' + (ctx.docsText||'').trim();
  const lines = text.split(/\r?\n/).map(s=>s.trim()).filter(Boolean).slice(0,5);
  if (lines.length){ lines.forEach(l => seeds.push((isDE? 'Bezug zur Ausschreibung: ' : 'Regarding the job ad: ')+l)); }
  return seeds.slice(0, ctx.interviewConfig?.maxQuestions || 5);
}

function askFirstQuestion(){
  if (FIRST_Q_SENT) return;
  FIRST_Q_SENT = true;

  const plan = buildInterviewPlan(CTX);
  const isDE = String(CTX.reportLang||'de').toLowerCase().startsWith('de');
  const first = plan[0] || (isDE ? 'Bitte stellen Sie sich kurz vor.' : 'Please introduce yourself.');

  dcSend({ type:'conversation.item.create', item:{ type:'message', role:'system', content:[{type:'input_text', text:'Starte jetzt die erste Interviewfrage. Kurze, präzise Fragen. Danach warten.'}] }});
  log('[CONV] system item created');

  const q = (isDE ? `Erste Frage: ${first}` : `First question: ${first}`);
  dcSend({ type:'response.create', response:{ conversation:'default', modalities:['audio'], instructions: q }});
  log('[QUESTION] first sent');
}
function maybeAskFirstQuestion(){ if (!FIRST_Q_SENT) askFirstQuestion(); }

// -------------------- Media / Recorder -------------------
async function setupLocalMedia(){
  try{
    localStream = await navigator.mediaDevices.getUserMedia({
      audio:{ echoCancellation:true, noiseSuppression:true, autoGainControl:true },
      video:true
    });
  } catch(e) {
    setStatus('err','Mic/Kamera blockiert. Bitte im Browser freigeben.');
    log('[Media] getUserMedia error', e);
    parent?.postMessage?.({ type:'clarity.live.permission_error', data:{ message: String(e?.name||e) } }, '*');
    localStream = null;
  }
  if (localStream){
    $localV.srcObject = localStream;
    try{
      const ctx = new (window.AudioContext || window.webkitAudioContext)();
      const src = ctx.createMediaStreamSource(localStream);
      const analyser = ctx.createAnalyser();
      analyser.fftSize = 512;
      const buf = new Uint8Array(analyser.frequencyBinCount);
      src.connect(analyser);
      (function tick(){
        if (destroyed) return;
        analyser.getByteTimeDomainData(buf);
        let dev=0; for(let i=0;i<buf.length;i++) dev=Math.max(dev, Math.abs(buf[i]-128));
        $mic.style.width = Math.min(100, dev*2) + '%';
        requestAnimationFrame(tick);
      })();
    }catch(_){}
    // Mic zunächst stumm
    localStream.getAudioTracks().forEach(t => t.enabled = false);
  }
}

async function enableMicUplink(){
  // Falls noch keine Permission: erst jetzt anfragen
  if (!localStream){
    try{
      await setupLocalMedia();
    }catch(_){}
  }
  try{
    localStream?.getAudioTracks()?.forEach(t => t.enabled=true);
    // Sender hinzufügen
    if (pc && localStream){
      localStream.getTracks().forEach(tr => pc.addTrack(tr, localStream));
    }
    log('[MIC] uplink enabled');
  }catch(_){}
}

// -------------------- Recorder (Mix) -------------------
async function blobToDataURL(blob){
  return new Promise((resolve,reject)=>{ const r=new FileReader(); r.onloadend=()=>resolve(r.result); r.onerror=reject; r.readAsDataURL(blob); });
}

function prepareRecording(){
  const ctx = new (window.AudioContext || window.webkitAudioContext)();
  const dest = ctx.createMediaStreamDestination();

  if (localStream){
    const micSrc = ctx.createMediaStreamSource(localStream); micSrc.connect(dest);
  }
  if (remoteStream){ try{ ctx.createMediaStreamSource(remoteStream).connect(dest); }catch(e){ log('[REC] remote mix fail', e); } }

  mixedStream = new MediaStream();
  dest.stream.getAudioTracks().forEach(t => mixedStream.addTrack(t));
  const cam = localStream?.getVideoTracks?.()[0];
  if (cam) mixedStream.addTrack(cam);

  const hasVideo = !!mixedStream.getVideoTracks().length;
  mediaRecorder = new MediaRecorder(mixedStream, {
    mimeType: hasVideo ? 'video/webm;codecs=vp8,opus' : 'audio/webm;codecs=opus',
    videoBitsPerSecond: hasVideo ? 1_200_000 : undefined,
    audioBitsPerSecond: 128_000
  });

  const chunks = [];
  mediaRecorder.onstart = ()=>{ $recState.textContent='recording'; log('[REC] started'); };
  mediaRecorder.ondataavailable = (e)=>{ if (e.data && e.data.size>0) chunks.push(e.data); };
  mediaRecorder.onstop = async ()=>{
    $recState.textContent='stopped';
    const hasVid = !!mixedStream.getVideoTracks().length;
    const blob = new Blob(chunks, { type: hasVid ? 'video/webm' : 'audio/webm' });
    log('[REC] stop, size=', blob.size);
    try{
      $muxState.textContent='exporting'; $recProg.style.width='60%';
      const dataUrl = await blobToDataURL(blob);
      parent?.postMessage?.({ type:'recorder:export', data:{ dataUrl, kind:(hasVid?'video':'audio'), bytes: blob.size } }, '*');
      $recProg.style.width='100%'; $muxState.textContent='export sent';
    }catch(e){ $muxState.textContent='export failed'; log('[REC] export failed', e); }
    finally{ chunks.length=0; }
  };

  $recState.textContent='ready';
}
function startRecorder(){ try{ if (mediaRecorder && mediaRecorder.state==='inactive') mediaRecorder.start(); }catch(e){ log('[REC] start error', e); } }
function stopRecording(){ try{ if (mediaRecorder && mediaRecorder.state!=='inactive') mediaRecorder.stop(); }catch(_){}
  try{ mixedStream?.getTracks()?.forEach(t=>t.stop()); }catch(_){} }

// -------------------- Start / Stop -------------------
async function startInterview(){
  if (started) return;
  started = true;
  try{
    destroyed = false;
    setStatus('info','Initialisiere…');

    // Wichtig: Audio im User-Gesture entsperren
    try { $remoteA.play().catch(()=>{}); } catch(_){}

    const { secret, ctx } = await getRealtimeToken({ uid:UID, companyId:COMPANY_ID, lang:LANG, voice:VOICE, debug:true, allowNoInvite:true });
    // Kontext merken
    CTX.reportLang = (CTX.reportLang || ctx.reportLang || LANG);
    CTX.companyName = ctx.companyName || CTX.companyName;

    // Verbinden (nur Empfang)
    await connectRealtime(secret);

    // Recorder vorbereiten (Tracks werden nach Welcome gemixt)
    prepareRecording();

    // Parent informieren (falls der Parent erst danach primen will)
    parent?.postMessage?.({ type:'clarity.live.ready', data:{} }, '*');

  }catch(e){
    setStatus('err','Startfehler: '+(e?.message||e));
    log('[Start error]', e);
    parent?.postMessage?.({ type:'clarity.live.error', data:{ message:String(e?.message||e) } }, '*');
  }
}

async function hangup(){
  destroyed = true;
  try{ DC?.close(); }catch(_){}
  try{ pc?.getSenders()?.forEach(s=> s.track && s.track.stop()); }catch(_){}
  try{ pc?.getReceivers()?.forEach(r=> r.track && r.track.stop()); }catch(_){}
  try{ pc?.close(); }catch(_){}
  try{ localStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
  try{ remoteStream?.getTracks()?.forEach(t => t.stop()); }catch(_){}
  stopRecording();
  setStatus('warn','Beendet');
  log('[Live] hangup done');
  if (!ENDED_SENT){ ENDED_SENT = true; parent?.postMessage?.({ type:'clarity.live.ended', data:{} }, '*'); }
}

// -------------------- Parent Messaging -------------------
window.addEventListener('message', (ev)=>{
  const msg = ev.data || {};
  const type = msg.type;

  if (type === 'clarity.live.context'){
    CTX = msg.data || {};
    PARENT_UPLOAD = (CTX.uploadMode === 'parent');
    log('[CTX]', CTX);
    return;
  }

  if (type === 'clarity.live.start'){
    // Start kommt aus User-Click im Parent -> AudioContext/Autoplay ok
    startInterview();
    return;
  }

  // Parent primt HART NACH ready
  if (type === 'clarity.live.prime'){
    const rl = (msg.data && msg.data.reportLang) || CTX.reportLang || LANG;
    CTX.reportLang = rl;
    tryPrimeOnce();
    tryWelcomeOnce();  // sagt Prep + Welcome, schaltet danach VAD + Mic
    return;
  }

  if (type === 'clarity.live.record'){
    RECORD_ENABLED = !!(msg.data && msg.data.enabled);
    log('[REC] record flag from parent:', RECORD_ENABLED);
    if (RECORD_ENABLED) startRecorder();
    return;
  }

  if (type === 'recorder:export'){ stopRecording(); return; }
  if (type === 'clarity.live.stop' || type === 'clarity.live.hangup'){ hangup(); return; }
  if (type === 'clarity.live.ping'){ parent?.postMessage?.({ type:'clarity.live.pong', data:{} }, '*'); return; }
});

function helloParent(){
  try{ parent?.postMessage?.({ type:'clarity.live.hello', data:{ uid:UID, companyId:COMPANY_ID } }, '*'); }catch(_){}
}

window.addEventListener('load', ()=>{
  helloParent();
  // Autostart nur wenn ausdrücklich gewünscht
  if (AUTOSTART){
    setTimeout(()=>{ if (!started) { log('[AUTO] no parent start → autostart'); startInterview(); } }, 1500);
  }
});
document.getElementById('btnStop').addEventListener('click', hangup);

// Hinweis: Das Parent-Iframe braucht allow="microphone; camera; autoplay; encrypted-media"
})();</script>
</body>
</html>
